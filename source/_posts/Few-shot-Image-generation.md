---
title: Few-shot Image generation
date: 2019-04-04 13:44:00
tags: few-shot learning
categories: Meta-learning
---

## Meta learning

### Definition
* "learn to learn", intends to design models that can learn *__new__* skills or adapt to *__new__* environment *__rapidly__* with *__a few__* traing samples, like *__human learning way__*. The detail can be posted in [Meta-Learning: Learning to Learn Fast](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)

<!-- *  Optimization aims:
$\theta^* = \arg\min_\theta \mathbb{E}_{\mathcal{D}\sim p(\mathcal{D})} [\mathcal{L}_\theta(\mathcal{D})]$ -->

where $\mathcal{D}=\langle S, B\rangle$, Support set and Batch set.

* Training steps
1. sample a subset of labels $L\subset\mathcal{L}$.
2. samples a support set $S^L \subset \mathcal{D}$ and a training batch $B^L \subset \mathcal{D}$. Both of them belong to the sampled label set $L$, $y \in L, \forall (x, y) \in S^L, B^L$.
3. support set is the input of model.
4. the update of model parameters is based on the loss in backpropagation calculated from the mini-batch $B^L$.

each pair of sampled dataset $(S^L, B^L)$ is regarded as one data point, such that trained models can generalize to other datasets. Symbols in red are added for meta-learning in addition to the general supervised learning objective.
\begin{aligned}
\theta = \arg\max_\theta \color{red}{E_{L\subset\mathcal{L}}[} E_{\color{red}{S^L \subset\mathcal{D}, }B^L \subset\mathcal{D}} [\sum_{(x, y)\in B^L} P_\theta(x, y\color{red}{, S^L})] \color{red}{]}
\end{aligned}

* Traning stages
1. meta-learner: a optimizer $g_\phi$ learns how to update the learner model’s parameters via the support set $S$, $\theta' = g_\phi(\theta, S)$
2. learner: A classifier $f_\theta$ is the “learner” model, trained for operating a given task.
<!-- final learning objective is $\mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]]$ -->


 

### Common methods


#### model-based: $f_\theta(\mathbf{x}, S)$
* use *__recurrent__* network with *__internal (or external)__* memory.

#### metric-based: $\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i$
* learn *__efficient__* distance metric.
* similar to *__nearnest neighbors algorithm__* (KNN,k-means) and *__kernel density estimation__*.
* the predicted possibility of labeled samples is from is a *__weighted sum of support set samples__*, and the weight is generated by a kernel function $k_\theta$, which can measure the *__similarity__* of twp data samples:
$$
P_\theta(y \vert \mathbf{x}, S) = \sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_{i}
$$
* crucial keys: *__good kernel__*
* solution: learning *__embedding vector__* of input data explicitly and use them to design *__proper kernel__* functions.
* Siamese networks: 
    - assumption: the *__learned embedding__* can be *__generalized__* to be useful for measuring the distance between images of unknown categories.
    - *__verification__*, images pairs.
    - final prediction is the class of the support image with highest probability.
* Matching networks:
	- $g_{\theta}$ with $k$ classifiers for k classes, while $f_{\theta}$ for testing images.
	- attention kernel depends on two embedding functions $g$ and $f$, in simple version where $f=g$. in complex version where integrating full contextual embedding (FCE), it achieves improvement on hard tasks, not for simple tasks.
* Relation network
    - image pairs, feature concatenation.
    - relation modeled: mse loss function.
* Prototypical
    - images of each class are embedded into $M$ dimensional feature vector, each class has _prototype_ feature vector from the mean vector of the embedded support data samples in each class.
    - squared euclidean distance.



#### optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$
* optimize the *__model parameter__* explicitly for *__fast__* learning
