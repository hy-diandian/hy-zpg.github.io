<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="default">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hongyan's Notes, NexT">





  <link rel="alternate" href="/atom.xml" title="Hongyan's Notes" type="application/atom+xml">






<meta name="description" content="New Rhythm">
<meta name="keywords" content="Coding &amp;&amp; Learning">
<meta property="og:type" content="website">
<meta property="og:title" content="Hongyan&#39;s Notes">
<meta property="og:url" content="https://www.yanhong.website/index.html">
<meta property="og:site_name" content="Hongyan&#39;s Notes">
<meta property="og:description" content="New Rhythm">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hongyan&#39;s Notes">
<meta name="twitter:description" content="New Rhythm">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.yanhong.website/">





  <title>Hongyan's Notes</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hongyan's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Cheatsheet</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            Guestbook
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2021/04/12/Generation/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/04/12/Generation/" itemprop="url">Image Generation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-04-12T19:59:19+08:00">
                2021-04-12
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Generative-Model/" itemprop="url" rel="index">
                    <span itemprop="name">Generative Model</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Generative-Model-Conlusion"><a href="#Generative-Model-Conlusion" class="headerlink" title="Generative Model Conlusion"></a>Generative Model Conlusion</h2><ul>
<li><p>categories: VAE-based, GAN-based, autogressive model based, flow-based.</p>
</li>
<li><p>advantage and disadvantage: vae-based (ad: model each training samples, ideal vae-based model without mode collapse; disad: suffer from posterior collapse); gan-based (ad: clear and realistic images; disad: suffering from mode collapse); autogressive model based: ad: effective probability estimation; disad: sampling speed is slow.</p>
</li>
<li><p>improvement methods: combinations, CVAE-GAN (VAE, GAN), VQ-VAE (AE, autogressive model), VQ-GAN (AE, autogressive model by combinating cnn with transformer)</p>
</li>
</ul>
<h3 id="Taming-Transformers-for-High-Resolution-Image-Synthesis"><a href="#Taming-Transformers-for-High-Resolution-Image-Synthesis" class="headerlink" title="Taming Transformers for High-Resolution Image Synthesis"></a>Taming Transformers for High-Resolution Image Synthesis</h3><ul>
<li><p><a href="https://arxiv.org/pdf/2012.09841.pdf" target="_blank" rel="noopener">CVPR 2021 PAPER</a></p>
</li>
<li><p><a href="https://blog.csdn.net/qq_40723205/article/details/113181463" target="_blank" rel="noopener">blog</a></p>
</li>
<li><p>target: demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images.</p>
</li>
<li><p>core idea: use CNNs to learn a contextrich vocabulary of image constituents by a learning a codebook; utilize transformers to efficiently model their composition within high-resolution images.</p>
</li>
<li><p>method: using VQGAN to learn a codebook, different from VQ-VAE.</p>
</li>
<li><p><font color="red"> difference from VQ-VAE</font>: using transforer to deal with higher resolution image (VQ-VAE:256 <em> 256 images, dimension reduction 32 </em> 32 in discrete codebook space, VQGAN: 1280 <em> 460 images, dimension reduction 16 </em> 16 in discrete codebook space). VQ procedure: using adversarial loss to replace reconstruction loss. autogressive precedure: using transformer  to replace pixelCNN.</p>
</li>
<li><p><font color="red"> similar idea with VQ-VAE</font> : Using discrete representation can discraite natual modality better (introducing VQ, or other discrete methods). Using autogressive model can conduct effective probability estimation, but which should be implemented in low-dimensiton space rather than large pixel space (introducing autogressive models, such as pixelCNN, transformer).</p>
</li>
</ul>
<h4 id="Related-paper-NIPS-2017-Neural-Discrete-Representation-Learning"><a href="#Related-paper-NIPS-2017-Neural-Discrete-Representation-Learning" class="headerlink" title="Related paper NIPS 2017: Neural Discrete Representation Learning"></a><a href="https://arxiv.org/pdf/1711.00937.pdf" target="_blank" rel="noopener">Related paper NIPS 2017: Neural Discrete Representation Learning</a></h4><ul>
<li><p>based on VAE, but different from VAE: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static.</p>
</li>
<li><p>the reason of VQ: discrete representations which are potentially a more natural fit for many of the modalities we are interested in. Furthermore, discrete representations are a natural fit for complex reasoning, planning and predictive learning.</p>
</li>
<li><p>learning discrete representation: vector quantization. </p>
</li>
<li><p>methods: testing by removing encoder and using autogressive model pixelCNN to build relation aming the dimension of sampling Z. </p>
</li>
<li><p><a href="https://kexue.fm/archives/6760" target="_blank" rel="noopener">blog</a></p>
</li>
<li><p>my understanding: discrete representations are more natural fit for many of modalities (langauge is discrete, images can be desscibed by discrete language). VAE has <font color="blue"> posterior  collapse </font>  which ignores the learnt latent code, as the powerful decoder, which can generate/reconstruct image with noise generated from <font color="blue">reparameterize </font>.  Inspired from pixelCNN with autogressive model by discribe a image with discrete pixel, VQ-VAE face two problems: recursive order designs, speeding up sampling (because sampling in autogressive model is slow). The shortcoming of autogressive model in pixel space is ignoring the relationship among continous pixels. VQ-VAE is designed to dimenstion reduction in latent code space, and then using autogressive model to model discrete code. In detail, using <font color="blue">neighbor reconstruction</font> to generate m * m matrix to achieve discrete coding, using <font color="blue">straight-thought estimator</font> to optimize the function without gradient. </p>
<ul>
<li><p>related concepts: AE: x-z-x, using compressed latent code to describe a image, VAE: x - z’ -&gt; x, enforce the latent code z’ to obey normal distribution and the sampling new latent code from the normal distribution. VAE: advantage is that ideal VAE can model each training samples without mode collapse, disadvantage is that VAE suffer from posterior collapse which ignores the latent code due to the powerful decoder, which can generate/reconstruct image with the noise introduced from reparamerization.</p>
</li>
<li><p>related concepts: autogressive model, such as pixelCNN using conditional probability distribution to generte image in pixel space. the advantage is the effective probability estimation, the disacvantage is the slow sampling speed.</p>
</li>
</ul>
</li>
</ul>
<h4 id="Related-paper-NIPS-2019-Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2"><a href="#Related-paper-NIPS-2019-Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2" class="headerlink" title="Related paper NIPS 2019: Generating Diverse High-Fidelity Images with VQ-VAE-2"></a><a href="https://arxiv.org/pdf/1906.00446.pdf" target="_blank" rel="noopener">Related paper NIPS 2019: Generating Diverse High-Fidelity Images with VQ-VAE-2</a></h4><ul>
<li><p>improvement: scaling and enhancing the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before.</p>
</li>
<li><p>using hierarchical VQ-VAE: top latent code (32 <em> 32) to model global information, while bottom latent code (64 </em> 64) conditioned on top latent code to represent local detail.</p>
</li>
</ul>
<h4 id="Related-paper-CVPR-2020-Learning-Representations-by-Predicting-Bags-of-Visual-Words"><a href="#Related-paper-CVPR-2020-Learning-Representations-by-Predicting-Bags-of-Visual-Words" class="headerlink" title="Related paper CVPR 2020: Learning Representations by Predicting Bags of Visual Words"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Gidaris_Learning_Representations_by_Predicting_Bags_of_Visual_Words_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Related paper CVPR 2020: Learning Representations by Predicting Bags of Visual Words</a></h4><h4 id="Related-paper-ICML-2020-Predictive-Sampling-with-Forecasting-Autoregressive-Models"><a href="#Related-paper-ICML-2020-Predictive-Sampling-with-Forecasting-Autoregressive-Models" class="headerlink" title="Related paper ICML 2020:Predictive Sampling with Forecasting Autoregressive Models"></a><a href="http://proceedings.mlr.press/v119/wiggers20a/wiggers20a.pdf" target="_blank" rel="noopener">Related paper ICML 2020:Predictive Sampling with Forecasting Autoregressive Models</a></h4><h4 id="Related-paper-ICML-2020-Latent-Bernoulli-Autoencoder"><a href="#Related-paper-ICML-2020-Latent-Bernoulli-Autoencoder" class="headerlink" title="Related paper ICML 2020:Latent Bernoulli Autoencoder"></a><a href="http://proceedings.mlr.press/v119/fajtl20a/fajtl20a.pdf" target="_blank" rel="noopener">Related paper ICML 2020:Latent Bernoulli Autoencoder</a></h4><h4 id="Related-paper-2021-Multimodal-Controller-for-Generative-Models"><a href="#Related-paper-2021-Multimodal-Controller-for-Generative-Models" class="headerlink" title="Related paper 2021:Multimodal Controller for Generative Models"></a><a href="https://arxiv.org/pdf/2002.02572.pdf" target="_blank" rel="noopener">Related paper 2021:Multimodal Controller for Generative Models</a></h4><h3 id="Generating-Diverse-Structure-for-Image-Inpainting-With-Hierarchical-VQ-VAE"><a href="#Generating-Diverse-Structure-for-Image-Inpainting-With-Hierarchical-VQ-VAE" class="headerlink" title="Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE"></a>Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE</h3><ul>
<li><p><a href="https://arxiv.org/pdf/2103.10022.pdf" target="_blank" rel="noopener">2021 PAPER</a></p>
</li>
<li><p>Application of VQ-VAE(VQ-VAE-2)</p>
</li>
<li><p>similar idea to VQ-VAE-2: using hirearchical VQ-VAE to genrate top(tructual) latent code and bottom(texture) latent code. using pixelCNN to generate structural top latent code, which can aviod mode collapse occured in GAN and ensure the diversity of structure.</p>
</li>
</ul>
<h2 id="Inspiration"><a href="#Inspiration" class="headerlink" title="Inspiration"></a>Inspiration</h2><ul>
<li><font color="red"> using idea of VQ-GAN to achieve specific application, such as high-resolution few-shot image generation. </font> 
</li>
<li><font color="red"> using idea of VQ-VAE (VQ-VAE-2: hirearchical VQ-vae) or VQ-GAN to achieve fuion-based few-shot image generation, which can also using top latent code to ensure the diversity of structures and using bottom latent code to model local detail. </font> 



</li>
</ul>
<h3 id="GIQA-Generated-Image-Quality-Assessment"><a href="#GIQA-Generated-Image-Quality-Assessment" class="headerlink" title="GIQA: Generated Image Quality Assessment"></a>GIQA: Generated Image Quality Assessment</h3><ul>
<li><a href="https://arxiv.org/pdf/2003.08932.pdf" target="_blank" rel="noopener">ECCV 2020 PAPER</a></li>
</ul>
<h3 id="Using-VQGAN"><a href="#Using-VQGAN" class="headerlink" title="Using VQGAN"></a>Using VQGAN</h3><ul>
<li><p>background knowledge</p>
<ul>
<li>word embedding<ul>
<li><a href="https://www.zhihu.com/question/32275069" target="_blank" rel="noopener">blog</a></li>
<li>procedure: creating a group of features for each word, for examples using 1024d vector to represent each indice in codebook, and then conducting distribution representation for the group of features.</li>
<li>how to learn the word embedding: using neural network with supervision.</li>
<li>effect: leart the context information among those words, for example the relation among the indices of codebook.</li>
</ul>
</li>
<li>transformer<ul>
<li>sequence2sequence + selfattention</li>
<li><a href="https://zhuanlan.zhihu.com/p/82391768" target="_blank" rel="noopener">blog1</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">blog2</a></li>
<li><a href="https://www.zhihu.com/search?type=content&amp;q=transformer实现" target="_blank" rel="noopener">blog3</a></li>
</ul>
</li>
</ul>
</li>
<li><p>detailed implmentation</p>
<ul>
<li><p>VQ</p>
<ul>
<li>implementation：  x(b,w’,h’,c) -&gt; z(b, w, h, c<em>{z}) <font color="red"> lenght = w * h, which need to be inferred from the transformer, representing the numbers of indices of codebook.</font> -&gt; embedding f</em>{z}(1024, n_z), each dimension of c_z is embedded with a 1024d feature -&gt; mini-encoding(b <em> w </em> h, 1024), mini-encoding-indices(b <em> w </em> h, 1) used to search of quantized z in codebook.</li>
<li>understanding of VG: discrete representation, model much training samples in this discrete feature space, each local component (w * h) represents different meaning, combing them to reconstruct different image. The relationship among this component is potential and is to be excavated.</li>
</ul>
</li>
<li><p>transformer</p>
<ul>
<li>training phase: indices<em>{x}(b, w * h) is indices of codebook of input x, indices</em>{c}(b, w <em> h) is indices of codebook of condition c, cating (indicese<em>{c}, indices</em>{x}) to obtain indices_{cat}(b, w </em> h <em> 2), using transformer(indices<em>{cat}[b,-1]) to regress the indices</em>{cat}[-1], supervised by the real indices_{cat}(b, w </em> h * 2), in this way, the autogressive model is built.</li>
<li>tesing phase: giving indices<em>{c} (or indices</em>{c} + part indices<em>{x}) to predict the indices</em>{x} with needed steps.</li>
<li>understanding of transformer<ul>
<li>unconditional senario: using same depth image downsample to obtain indices and upsample to obtain reconstruction image, build complex relationship among discrete codebook of input image.</li>
<li>conditional: using different conditional image to obtain different discrete indices, build complex relationship among discrete codebook of input and discreate codebook of conditional image.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>few-shot transforer</p>
<ul>
<li><p>transformation-based: given a input image from a category, selecting another image from the same category as conditional image, using VQGAN to learn same codebook of input image and conditional image. In transformer learning, using indices of conditional image as conditional information to learn relationship among those codebook (including input codebook and conditional codebook)</p>
<ul>
<li>problem: pairs are not unique, don’t like  input image and its corresponding depth image</li>
</ul>
</li>
<li><p>fusion-based: using the indices of several conditional images as conditional information, using transformer to learn the relationship. </p>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2021/04/10/few-shot-image-generation-methods/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/04/10/few-shot-image-generation-methods/" itemprop="url">Few-shot Image Generation Methods and Applications</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-04-10T09:43:48+08:00">
                2021-04-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Meta-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Meta-learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Few-shot-Image-Generation"><a href="#Few-shot-Image-Generation" class="headerlink" title="Few-shot Image Generation"></a>Few-shot Image Generation</h2><h3 id="Few-shot-Image-Generation-with-Elastic-Weight-Consolidation"><a href="#Few-shot-Image-Generation-with-Elastic-Weight-Consolidation" class="headerlink" title="Few-shot Image Generation with Elastic Weight Consolidation"></a>Few-shot Image Generation with Elastic Weight Consolidation</h3><ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/file/b6d767d2f8ed5d21a44b0e5886680cb9-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p>
</li>
<li><p>core idea: preserving the diversity of source domain, adapting the information of source domain to the target domain with a few sampels from target domain with adapting the pretrained model’s weights.</p>
</li>
<li><p>storyline:</p>
<ul>
<li><p>70,000 images for just a specific domain (aligned faces) or 1.3M images across different classes -&gt; in the artistic domain, it is at best cumbersome, and at times prohibitive, to hire artists to make thousands of creations -&gt; our goal is to generalize from a few, new examples.  <font color="blue">our setting need much data belonging to same domain such as flowers, animal faces, then using a few samples from different category like funit regarding category as domain</font>.</p>
</li>
<li><p>training an algorithm to generate more data of a target domain, given only a few examples. An underlying assumption with this setup is that the source and target domains share some latent factors, with some differences related to their distinct difference in appearance. <font color="blue">finding shared some latent code, how near or how far</font>.</p>
</li>
<li><p>propose a straightforward and effective adaptation technique, adapting the pretrained model’s weights, without introducing additional parameters. Fixing the architecture implies that tedious manual designs on new parameters are not necessary.</p>
</li>
<li><p>A key property to note is that weights have different levels of importance; thus, each parameter should not be treated equally in the adaptation, or tuning process.  we consider there will always be an inherent trade-off between preserving information from the source and adapting to the target domain. <font color="blue">the number of target examples and the dissimilarity between the source and target domain</font>.</p>
</li>
</ul>
</li>
<li><p>method: </p>
<ul>
<li><p>we do the adaptation with a few examples, some weights in the last layer are more important and should be better preserved than those in other layers.</p>
</li>
<li><p>we could directly use Fisher information as an importance measure for weights and add a regularization loss to penalize the weight change.</p>
</li>
</ul>
</li>
<li><p>setting: <font color="green">big domain without considering the category of source domain and target domain. real faces vs. emoji faces.</font></p>
</li>
</ul>
<ul>
<li><p>experiment:</p>
<ul>
<li><p>datasets: real faces (200k) vs. emoji faces (80k), animal faces (10 <em> 5000) vs. portrait paintings (16 </em> 10). landscape photos (5000) vs. pencil landscape drawings(10). <font color="blue">data size 10-80k</font></p>
</li>
<li><p>evaluation metrics: FID(user study) for more taget images (less target images) and LPIPS.</p>
</li>
<li><p>visualization.</p>
</li>
</ul>
</li>
</ul>
<h3 id="Few-shot-Adaptation-of-Generative-Adversarial-Networks"><a href="#Few-shot-Adaptation-of-Generative-Adversarial-Networks" class="headerlink" title="Few-shot Adaptation of Generative Adversarial Networks"></a>Few-shot Adaptation of Generative Adversarial Networks</h3><ul>
<li><p><a href="https://openreview.net/pdf?id=6R51jA4fOB" target="_blank" rel="noopener">ICLR 2021 paper</a></p>
</li>
<li><p>core idea: proposesing a simple and effective method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than 100 images) by repurposing component analysis techniques and learns to adapt the singular values of the pre-trained weights while freezing the corresponding singular vectors.</p>
</li>
<li><p>storyline:</p>
<ul>
<li><p>Training these models, however, typically requires large, diverse datasets in a target visual domain -&gt; low-data regime (e.g., less than 1,000 samples), GANs frequently suffer from memorization or instability, leading to a lack of diversity or poor visual quality -&gt; transfer learning, images generated by GAN-based methods are blurry and lack details, flow-based models require compute- and memory-intensive architectures.</p>
</li>
<li><p>we restrict the space of trainable parameters to a small number of highly-expressive parameters that modulate orthogonal features of the pre-trained weight space.</p>
</li>
<li><p>Our method first applies singular value decomposition (SVD) to the network weights of a pretrained GAN (generator + discriminator). We then adapts the singular values using GAN optimization on the target few-shot domain, with fixed left/right singular vectors. We show that varying singular values in the weight space corresponds to semantically meaningful changes of the synthesized image while preserving natural structure. proposed method achieves higher image quality after adaptation. </p>
</li>
</ul>
</li>
<li><p>setting: big domain without considering the category of source domain and target domain. real faces vs. emoji faces.</p>
</li>
<li><p>experiments:</p>
<ul>
<li><p>datasets: LSUN Churches→ Van Gogh paintings(30), FFHQ→Art portraits(5-100), FFHQ→Anime Rem ID(25).  <font color="blue">data size 5-100</font></p>
</li>
<li><p>metrics: FID, sharpness, and face quality index (FQI).</p>
</li>
</ul>
</li>
</ul>
<h3 id="Augmentation-Interpolative-AutoEncoders-for-Unsupervised-Few-Shot-Image-Generation"><a href="#Augmentation-Interpolative-AutoEncoders-for-Unsupervised-Few-Shot-Image-Generation" class="headerlink" title="Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation"></a>Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation</h3><ul>
<li><p><a href="https://arxiv.org/pdf/2011.13026.pdf" target="_blank" rel="noopener">paper</a></p>
</li>
<li><p>core idea: autoencoders generalize extremely well to new domains, even when trained on highly constrained data.</p>
</li>
<li><p>storyline: </p>
<ul>
<li><p>generating high-quality, diverse, high-resolution images requires a large dataset -&gt; need generative models that can train on one set of image classes, and then generalize to a new class using only a small quantity of new images: few-shot image generation.</p>
</li>
<li><p>previous methods need large labeled datasets of hundreds of classes, substantial computation at test time, highly domain-specific, generalizing only across very similar classes.</p>
</li>
<li><p>We leverage the finding that although the latent spaces of powerful generative models, such as VAEs and GANs, do not generalize to new classes, the representations learned by autoencoders (AEs) generalize extremely well -&gt; Interpolative AutoEncoders -&gt; Augmentation-Interpolative Autoencoder (AugIntAE) achieves simple, robust, highly general, and completely unsupervised few-shot image generation.</p>
</li>
</ul>
</li>
<li><p>setting: <font color="green">given  a large, unlabelled collection of images depicting objects from a set of seen classes and  a very small set of images - as few as two - belonging to a novel class, Our goal is to train a network on seen classes and generates images clearly belonging to the novel class. interpolating a pair of unseen images to generate new images belonging to the novel class.</font></p>
</li>
</ul>
<ul>
<li><p>experiments:</p>
<ul>
<li><p>datasets:MNIST → EMNIST , Omniglot (train → test), CelebA (male → female) , CIFAR-10 → CIFAR-100</p>
</li>
<li><p>metrics: FID score and train/test classification error.</p>
</li>
</ul>
</li>
</ul>
<h3 id="MineGAN-effective-knowledge-transfer-from-GANs-to-target-domains-with-few-images"><a href="#MineGAN-effective-knowledge-transfer-from-GANs-to-target-domains-with-few-images" class="headerlink" title="MineGAN: effective knowledge transfer from GANs to target domains with few images"></a>MineGAN: effective knowledge transfer from GANs to target domains with few images</h3><ul>
<li><p><a href="https://arxiv.org/pdf/1912.05270.pdf" target="_blank" rel="noopener">CVPR 2020 paper</a></p>
</li>
<li><p>core idea: using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain.</p>
</li>
<li><p>storyline: high-quality pretrained models -&gt; combining high-quality pretrained models with other models and adjust them to a target distribution is a desirable objective -&gt; knowledge transfer for generative models has received significantly less attention, possibly due to its great difficulty, especially when transferring to target domains with few images -&gt; previous works severely limits the flexibility of the knowledge transfer, mode collapse -&gt; a miner network that transforms a multivariate normal distribution into a distribution on the input space of the pretrained GAN in such a way that the generated images resemble those of the target domain.</p>
</li>
</ul>
<ul>
<li><p>setting: big domain without considering the category of source domain and target domain. human faces vs. children faces.</p>
</li>
<li><p>experiment:</p>
<ul>
<li><p>dataset: CelebA→FFHQ children, <font color="blue">data size 100-1000</font> </p>
</li>
<li><p>evaluation: MV, KMMD, FID</p>
</li>
</ul>
</li>
</ul>
<h3 id="CharacterGAN-Few-Shot-Keypoint-Character-Animation-and-Reposing"><a href="#CharacterGAN-Few-Shot-Keypoint-Character-Animation-and-Reposing" class="headerlink" title="CharacterGAN: Few-Shot Keypoint Character Animation and Reposing"></a>CharacterGAN: Few-Shot Keypoint Character Animation and Reposing</h3><ul>
<li><a href="https://arxiv.org/pdf/2102.03141.pdf" target="_blank" rel="noopener">paper</a></li>
</ul>
<h3 id="GAN-Memory-with-No-Forgetting"><a href="#GAN-Memory-with-No-Forgetting" class="headerlink" title="GAN Memory with No Forgetting"></a>GAN Memory with No Forgetting</h3><ul>
<li><a href="https://papers.nips.cc/paper/2020/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></li>
</ul>
<h3 id="On-Leveraging-Pretrained-GANs-for-Generation-with-Limited-Data"><a href="#On-Leveraging-Pretrained-GANs-for-Generation-with-Limited-Data" class="headerlink" title="On Leveraging Pretrained GANs for Generation with Limited Data"></a>On Leveraging Pretrained GANs for Generation with Limited Data</h3><ul>
<li><a href="http://proceedings.mlr.press/v119/zhao20a/zhao20a.pdf" target="_blank" rel="noopener">ICML 2020 paper</a></li>
</ul>
<h3 id="Differentiable-Augmentation-for-Data-Efficient-GAN-Training"><a href="#Differentiable-Augmentation-for-Data-Efficient-GAN-Training" class="headerlink" title="Differentiable Augmentation for Data-Efficient GAN Training"></a>Differentiable Augmentation for Data-Efficient GAN Training</h3><ul>
<li><p><a href="https://proceedings.neurips.cc//paper/2020/file/55479c55ebd1efd3ff125f1337100388-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p>
</li>
<li><p>method:  improving the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples.</p>
</li>
<li><p>storyline:</p>
<ul>
<li><p>The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set.</p>
</li>
<li><p>it is of critical importance to eliminate the need of immense datasets for GAN training.</p>
</li>
<li><p>suggesting that the discriminator is simply memorizing the entire training set. This severe over-fitting problem disrupts the training dynamics and leads to degraded image quality.</p>
</li>
<li><p>augmentations (cropping, flipping, scaling, color jittering, and region masking) in real images, the generator would be encouraged to match the distribution of the augmented images.</p>
</li>
<li><p>augmentation need to be done in real images and generate images, which breaks the subtle balance between the generator and discriminator, leading to poor convergence as they are optimizing completely different objectives.</p>
</li>
<li><p>DiffAugment, which applies the same differentiable augmentation to both real and fake images for both generator and discriminator training. It enables the gradients to be propagated through the augmentation back to the generator, regularizes the discriminator without manipulating the target distribution, and maintains the balance of training dynamics.</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>setting: training set  <font color="blue">data size 25% ~ 100% </font> </li>
</ul>
<h3 id="Training-Generative-Adversarial-Networks-with-Limited-Data"><a href="#Training-Generative-Adversarial-Networks-with-Limited-Data" class="headerlink" title="Training Generative Adversarial Networks with Limited Data"></a>Training Generative Adversarial Networks with Limited Data</h3><ul>
<li><p><a href="https://papers.nips.cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p>
</li>
<li><p>methods: augmentation strtegies in discriminator.</p>
</li>
<li><p>setting: training set <font color="blue">data size 1k ~ 200k </font> </p>
</li>
</ul>
<h3 id="Image-Generation-From-Small-Datasets-via-Batch-Statistics-Adaptation"><a href="#Image-Generation-From-Small-Datasets-via-Batch-Statistics-Adaptation" class="headerlink" title="Image Generation From Small Datasets via Batch Statistics Adaptation"></a>Image Generation From Small Datasets via Batch Statistics Adaptation</h3><ul>
<li><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">2019 ICCV paper</a></p>
</li>
<li><p>core idea: Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small datase.</p>
</li>
<li><p>method: we propose a novel method focusing on the pa- rameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small (<font color="blue">data size 25 ~ 500 </font> ).</p>
</li>
</ul>
<ul>
<li><p>setting: <font color="green">selecting pretrained model SNGAN and BigGAN. All experiments in this paper are not class conditional, but our method can be easily extended to be class-conditional by learning BatchNorm statistics for each class independently.</font></p>
</li>
<li><p>experiment:</p>
<ul>
<li><p>dataset: FFHQ dataset -&gt; anime face datase, imagenet -&gt; oxford flowers. </p>
</li>
<li><p>metrics: FID, KMMD</p>
</li>
</ul>
</li>
</ul>
<h3 id="DVG-Face-Dual-Variational-Generation-for-Heterogeneous-Face-Recognition"><a href="#DVG-Face-Dual-Variational-Generation-for-Heterogeneous-Face-Recognition" class="headerlink" title="DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition"></a>DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition</h3><ul>
<li><a href="https://arxiv.org/pdf/2009.09399.pdf" target="_blank" rel="noopener">2021 PAMI paper</a></li>
</ul>
<h3 id="Implicit-Semantic-Data-Augmentation-for-Deep-Networks"><a href="#Implicit-Semantic-Data-Augmentation-for-Deep-Networks" class="headerlink" title="Implicit Semantic Data Augmentation for Deep Networks"></a>Implicit Semantic Data Augmentation for Deep Networks</h3><ul>
<li><p><a href="https://arxiv.org/pdf/1909.12220.pdf" target="_blank" rel="noopener">NIPS 2019 PAPER</a></p>
</li>
<li><p>inspireation: based on KL-version delta, using normal distribution with learnt covariance from conditional images and zero means, sampling from the learnt normal distribution to generate new images.</p>
</li>
</ul>
<h2 id="Inspiration"><a href="#Inspiration" class="headerlink" title="Inspiration"></a>Inspiration</h2><ul>
<li><font color="red"> few-shot image generation - source domain: category information, conditional GAN -> targte domain: category information, presevring diversity information and distinctive feature among different categories. Given a few samples from different category of target domain </font> 




</li>
</ul>
<h2 id="Few-shot-Font-Generation"><a href="#Few-shot-Font-Generation" class="headerlink" title="Few-shot Font Generation"></a>Few-shot Font Generation</h2><h3 id="GANwriting-Content-Conditioned-Generation-of-Styled-Handwritten-Word-Images"><a href="#GANwriting-Content-Conditioned-Generation-of-Styled-Handwritten-Word-Images" class="headerlink" title="GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images"></a>GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images</h3><ul>
<li><p><a href="https://arxiv.org/pdf/2003.02567.pdf" target="_blank" rel="noopener">2020 ECCV paper</a></p>
</li>
<li><p>storyline:</p>
<ul>
<li><p>the classes had to be predefined beforehand during the training stage in conditional gan,  it was impossible to produce images from other unseen classes during inference.</p>
</li>
<li><p>sequence -&gt; generate raw images. the produced results by previous works are not realistic, still exhibiting a poor quality, sometimes producing barely legible word images.</p>
</li>
<li><p>we present a non-recurrent generative architecture conditioned to textual content sequences, that is specially tailored to produce realistic handwritten word images, indis- tinguishable to humans. our approach1 is able to artificially render realistic handwritten word images that match a certain textual content and that mimic some style features (text skew, slant, roundness, stroke width, ligatures, etc.) from an exemplar writer.</p>
</li>
</ul>
</li>
</ul>
<h3 id="Few-Shot-Text-Style-Transfer-via-Deep-Feature-Similarity"><a href="#Few-Shot-Text-Style-Transfer-via-Deep-Feature-Similarity" class="headerlink" title="Few-Shot Text Style Transfer via Deep Feature Similarity"></a>Few-Shot Text Style Transfer via Deep Feature Similarity</h3><ul>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9098082" target="_blank" rel="noopener">TIP 2020 paper</a></li>
</ul>
<h3 id="JointFontGAN-Joint-Geometry-Content-GAN-for-Font-Generation-via-Few-Shot-Learning"><a href="#JointFontGAN-Joint-Geometry-Content-GAN-for-Font-Generation-via-Few-Shot-Learning" class="headerlink" title="JointFontGAN: Joint Geometry-Content GAN for Font Generation via Few-Shot Learning"></a>JointFontGAN: Joint Geometry-Content GAN for Font Generation via Few-Shot Learning</h3><ul>
<li><a href="https://par.nsf.gov/servlets/purl/10199594" target="_blank" rel="noopener">ACM MM 2020</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/11/21/Shadow-Generation/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/21/Shadow-Generation/" itemprop="url">Shadow Generation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-21T11:14:40+08:00">
                2020-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Shadow-Detection"><a href="#Shadow-Detection" class="headerlink" title="Shadow Detection"></a>Shadow Detection</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><ul>
<li>SBU</li>
<li>UCF</li>
<li>CUHK-Shadow <a href="https://arxiv.org/pdf/1911.06998.pdf" target="_blank" rel="noopener">Revisiting Shadow Detection: A New Benchmark Dataset for Complex World, NOT ACCEPTED</a></li>
</ul>
<h4 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h4><ul>
<li><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Distraction-Aware_Shadow_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Distraction-aware Shadow Detection,CVPR2019</a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Bidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection</a></p>
</li>
</ul>
<h3 id="Shadow-Removal"><a href="#Shadow-Removal" class="headerlink" title="Shadow Removal"></a>Shadow Removal</h3><h4 id="Datasets-1"><a href="#Datasets-1" class="headerlink" title="Datasets"></a>Datasets</h4><ul>
<li>LRSS</li>
<li>UIUC</li>
<li>LRSS</li>
<li>ISTD <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal, CVPR2018</a></li>
</ul>
<h4 id="Papers-1"><a href="#Papers-1" class="headerlink" title="Papers"></a>Papers</h4><ul>
<li><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.pdf" target="_blank" rel="noopener">DeshadowNet: A Multi-context Embedding Deep Network for Shadow Removal, CVPR2017</a></li>
</ul>
<p><strong> dataset: UIUC(76 pairs,  {shadow, shadow-free}), LRSS (37 pairs,  {shadow, shadow-free}), New constructed SRD (3088 pairs {shadow, shadow-free})
</strong> input-output: shadow — shadow-free </p>
<ul>
<li><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal, CVPR2018</a></li>
</ul>
<p><strong> dataset: SRD, UIUC, LRSS, SBU(4727, pairs{shadow, shadow mask}), UCF (245, {shadow, shadow mask}), New constructed ISTD (1870 tuples {shadow, shadow mask, shadow-free})
</strong> input-output: shadow — shadow-free (using shadow detector to obtain shadow mask)</p>
<ul>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data, ICCV2019</a></li>
</ul>
<p><strong> dataset: USR, SRD, ISTD
</strong> input-output: {shadow,  shadow mask} — shadow-free </p>
<ul>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ARGAN_Attentive_Recurrent_Generative_Adversarial_Network_for_Shadow_Detection_and_ICCV_2019_paper.pdf" target="_blank" rel="noopener">ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal, ICCV2019</a></li>
</ul>
<p><strong> dataset: SBU, SRD, ISTD
</strong> input-output: shadow — shadow-free </p>
<ul>
<li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Shadow_Removal_via_Shadow_Image_Decomposition_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Shadow Removal via Shadow Image Decomposition, ICCV2019</a></li>
</ul>
<p><strong> dataset: ISTD
</strong> input-output: {shadow, shadow mask} — shadow-free</p>
<ul>
<li><a href="http://www.chengjianglong.com/publications/RISGAN_AAAI.pdf" target="_blank" rel="noopener">RIS-GAN: Explore Residual and Illumination with Generative Adversarial Networks for Shadow Removal, AAAI 2020</a></li>
</ul>
<p><strong> dataset: SRD, ISTD
</strong> input-output: shadow — shadow-free </p>
<ul>
<li><a href="https://www3.cs.stonybrook.edu/~cvl/content/papers/2020/Hieu_ECCV2020.pdf" target="_blank" rel="noopener">From Shadow Segmentation to Shadow Removal, ECCV2020</a></li>
</ul>
<p><strong> dataset: ISTD
</strong> input-output: {shadow, shadow mask} — shadow-free </p>
<ul>
<li><a href="https://arxiv.org/pdf/1911.08718.pdf" target="_blank" rel="noopener">Towards Ghost-free Shadow Removal via<br>Dual Hierarchical Aggregation Network and Shadow Matting GAN, AAAI2020</a></li>
</ul>
<p><strong> dataset: ISTD
</strong> input-output: {shadow, shadow mask} — shadow-free </p>
<h3 id="Instance-Shadow"><a href="#Instance-Shadow" class="headerlink" title="Instance Shadow"></a>Instance Shadow</h3><h4 id="Datasets-2"><a href="#Datasets-2" class="headerlink" title="Datasets"></a>Datasets</h4><ul>
<li>SOBA <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Shadow_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Instance Shadow Detection, CVPR2020</a></li>
</ul>
<h4 id="Papers-2"><a href="#Papers-2" class="headerlink" title="Papers"></a>Papers</h4><ul>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Shadow_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Instance Shadow Detection, CVPR2020</a></li>
</ul>
<h3 id="Shadow-Generation"><a href="#Shadow-Generation" class="headerlink" title="Shadow Generation"></a>Shadow Generation</h3><h4 id="Datasets-3"><a href="#Datasets-3" class="headerlink" title="Datasets"></a>Datasets</h4><ul>
<li>3D rendering generated <a href="https://link.springer.com/article/10.1007/s41095-019-0136-1" target="_blank" rel="noopener">ShadowGAN: Shadow synthesis for virtual objects with conditional adversarial networks</a></li>
</ul>
<h4 id="Papers-3"><a href="#Papers-3" class="headerlink" title="Papers"></a>Papers</h4><ul>
<li><p><a href="https://link.springer.com/article/10.1007/s41095-019-0136-1" target="_blank" rel="noopener">ShadowGAN: Shadow synthesis for virtual objects with conditional adversarial networks</a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2009.08255.pdf" target="_blank" rel="noopener">Adversarial Image Composition with Auxiliary Illumination</a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_ARShadowGAN_Shadow_Generative_Adversarial_Network_for_Augmented_Reality_in_Single_CVPR_2020_paper.pdf" target="_blank" rel="noopener">ARShadowGAN: Shadow Generative Adversarial Network for Augmented Reality in Single Light Scenes, CVPR2020</a></p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/11/20/Detection/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/20/Detection/" itemprop="url">Abnormal Generation for Abnormal Detection</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-20T09:42:42+08:00">
                2020-11-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://github.com/yzhao062/anomaly-detection-resources" target="_blank" rel="noopener">Updated Links1 Related General AD</a><br><a href="https://github.com/hoya012/awesome-anomaly-detection" target="_blank" rel="noopener">Updated Links2 Related Image AD</a></p>
<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><ul>
<li>not a binary classification because of the limited defected images</li>
</ul>
<h2 id="Coarse-Categories"><a href="#Coarse-Categories" class="headerlink" title="Coarse Categories"></a>Coarse Categories</h2><p><a href="https://zhuanlan.zhihu.com/p/116235115" target="_blank" rel="noopener">refer</a></p>
<ul>
<li>open-set recognition: labeled data (category information) + unknown data </li>
<li>unlabeled data: clean (training samples are all positive), polluted(there are a few defected images in training set)<br><img src="figures/AD_category.jpg" alt=""></li>
</ul>
<h2 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h2><ul>
<li><p>open-set recognition (labeled data): classifier -&gt; (predicted category + confidence, {confidence &lt; threshold} is abnormal sample)</p>
</li>
<li><p>unlabeled data: training data (normal samples) + testing data(normal + abnormal)</p>
</li>
</ul>
<h3 id="One-Class-Anomaly-Classification-target"><a href="#One-Class-Anomaly-Classification-target" class="headerlink" title="One Class (Anomaly) Classification target"></a>One Class (Anomaly) Classification target</h3><h3 id="Out-of-Distribution-OOD-Detection-target"><a href="#Out-of-Distribution-OOD-Detection-target" class="headerlink" title="Out-of-Distribution(OOD) Detection target"></a>Out-of-Distribution(OOD) Detection target</h3><h3 id="Unsupervised-Anomaly-Segmentation-target"><a href="#Unsupervised-Anomaly-Segmentation-target" class="headerlink" title="Unsupervised Anomaly Segmentation target"></a>Unsupervised Anomaly Segmentation target</h3><h2 id="Defect-Generation"><a href="#Defect-Generation" class="headerlink" title="Defect Generation"></a>Defect Generation</h2><p><a href="https://ieeexplore.ieee.org/document/9000806" target="_blank" rel="noopener">Defect Image Sample Generation With GAN for Improving Defect Recognition</a></p>
<ul>
<li>using cycleGAN</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/09/28/Disentangle-GAN/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/28/Disentangle-GAN/" itemprop="url">Disentangle_GAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-28T09:06:26+08:00">
                2020-09-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Generative-Model/" itemprop="url" rel="index">
                    <span itemprop="name">Generative Model</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Deformation-aware-Unpaired-Image-Translation-for-Pose-Estimation-on-Laboratory-Animals"><a href="#Deformation-aware-Unpaired-Image-Translation-for-Pose-Estimation-on-Laboratory-Animals" class="headerlink" title="Deformation-aware Unpaired Image Translation for Pose Estimation on Laboratory Animals"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Deformation-Aware_Unpaired_Image_Translation_for_Pose_Estimation_on_Laboratory_Animals_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Deformation-aware Unpaired Image Translation for Pose Estimation on Laboratory Animals</a></h4><ul>
<li>Deformation-aware</li>
</ul>
<h4 id="Reusing-Discriminators-for-Encoding-Towards-Unsupervised-Image-to-Image-Translation"><a href="#Reusing-Discriminators-for-Encoding-Towards-Unsupervised-Image-to-Image-Translation" class="headerlink" title="Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Reusing_Discriminators_for_Encoding_Towards_Unsupervised_Image-to-Image_Translation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation</a></h4><ul>
<li>reusing part parameters of discriminator in generator</li>
</ul>
<h4 id="Semi-supervised-Learning-for-Few-shot-Image-to-Image-Translation"><a href="#Semi-supervised-Learning-for-Few-shot-Image-to-Image-Translation" class="headerlink" title="Semi-supervised Learning for Few-shot Image-to-Image Translation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Semi-Supervised_Learning_for_Few-Shot_Image-to-Image_Translation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Semi-supervised Learning for Few-shot Image-to-Image Translation</a></h4><ul>
<li>noisy label</li>
</ul>
<h4 id="Unsupervised-Multi-Modal-Image-Registration-via-Geometry-Preserving-Image-to-Image-Translation"><a href="#Unsupervised-Multi-Modal-Image-Registration-via-Geometry-Preserving-Image-to-Image-Translation" class="headerlink" title="Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Arar_Unsupervised_Multi-Modal_Image_Registration_via_Geometry_Preserving_Image-to-Image_Translation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation</a></h4><ul>
<li>spatial transformation network and a translation network</li>
</ul>
<h4 id="One-Shot-Domain-Adaptation-For-Face-Generation"><a href="#One-Shot-Domain-Adaptation-For-Face-Generation" class="headerlink" title="One-Shot Domain Adaptation For Face Generation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_One-Shot_Domain_Adaptation_for_Face_Generation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">One-Shot Domain Adaptation For Face Generation</a></h4><ul>
<li><p>how to mimic a complete distribution of the target image given just one example. </p>
</li>
<li><p>our method aims to mimic a complete distribution of the target domain given just one example.</p>
</li>
<li><p>we directly manipulate the distribution in the image space.</p>
</li>
<li><p>a face manip- ulation detector.</p>
</li>
<li><p>For each random style vector s that we sampled with the mapping network, we replace the final layers of s with those of sI before giving it as input to the generator so that the generated random image g(s) inherits the low-level color and textures from I.</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/09/25/Re-identification/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/25/Re-identification/" itemprop="url">video-based Re-identification</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-25T15:20:06+08:00">
                2020-09-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Person-Re-identification/" itemprop="url" rel="index">
                    <span itemprop="name">Person Re-identification</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="Video-to-video"><a href="#Video-to-video" class="headerlink" title="Video-to-video"></a>Video-to-video</h5><h6 id="Co-segmentation-Inspired-Attention-Networks-for-Video-based-Person-Re-identification"><a href="#Co-segmentation-Inspired-Attention-Networks-for-Video-based-Person-Re-identification" class="headerlink" title="Co-segmentation Inspired Attention Networks for Video-based Person Re-identification"></a><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identification_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Co-segmentation Inspired Attention Networks for Video-based Person Re-identification</a></h6><ul>
<li>setting: v-t-v</li>
<li>issue: severe occlusions, background clutter, viewpoint change, etc. noisy background features from irrelevant non-salient regions may get misinterpreted as the person’s features and get aggregated in the video descriptor.  the subject alignment and scene variation aggravate the prolem and result in a drastic drop in Re-ID accuracy. misses out the salient accessories associated with the subject (e.g., backpack, bag, hat and coat) that are also important cues for Re-ID.</li>
<li>problem: background nosiy need to be ignored, associated with the subject (e.g., backpack, bag, hat and coat) need to be paid attention.</li>
<li>previous method: Co-attention by leveraging inter-video (probe vs. gallery video snippets) co-attention(computationally expensive).</li>
<li>method:discovered a set of distinctive body parts using diverse spatial attentions and discriminative frames by a temporal attention model.  effectively captures the attention between frames of a video.</li>
<li>effects:  interpretable.</li>
</ul>
<h6 id="Global-Local-Temporal-Representations-For-Video-Person-Re-Identification"><a href="#Global-Local-Temporal-Representations-For-Video-Person-Re-Identification" class="headerlink" title="Global-Local Temporal Representations For Video Person Re-Identification"></a><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Global-Local Temporal Representations For Video Person Re-Identification</a></h6><ul>
<li>setting: v-t-v</li>
<li>issue: however they still show certain limitations in the aspects of either efficiency or the capability of temporal cues modeling.</li>
<li>motivation: the short-term temporal cue among adjacent frames helps to distinguish visually similar pedestrians. The long-term temporal cue helps to alleviate the occlusions and noises in video sequences.</li>
<li>method: exploit the multi-scale temporal cues in video sequences. short-term temporal cues among adjacent frames, then capturing the long-term relations among inconsecutive frames. the short-term temporal cues are modeled by parallel dilated convolutions with different temporal dilation rates to represent the motion and appearance of pedestriain. The long-term relations are captured by a temporal self-attention model to alleviate the occlusions and noises in video sequences.</li>
</ul>
<h6 id="Spatial-Temporal-Graph-Convolutional-Network-for-Video-based-Person-Re-identification"><a href="#Spatial-Temporal-Graph-Convolutional-Network-for-Video-based-Person-Re-identification" class="headerlink" title="Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification</a></h6><ul>
<li>setting: v-t-v</li>
<li>issue: the occlusion problem and the visual ambiguity problem for visually similar negative samples.</li>
<li>motivation: modeling the temporal relations of different frames and the spatial relations within a frame has the potential for solving the above problems.</li>
<li>method: two GCN branches, a spatial one and a temporal one. The spatial branch extracts structural information of a human body. The temporal branch mines discriminative cues from adjacent frames.</li>
</ul>
<h6 id="Learning-Multi-Granular-Hypergraphs-for-Video-Based-Person-Re-Identification"><a href="#Learning-Multi-Granular-Hypergraphs-for-Video-Based-Person-Re-Identification" class="headerlink" title="Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Learning_Multi-Granular_Hypergraphs_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</a></h6><ul>
<li>setting: v-t-v</li>
<li>issue: misalignment and occlusion</li>
</ul>
<h6 id="Appearance-Preserving-3D-Convolution-for-Video-based-Person-Re-identification"><a href="#Appearance-Preserving-3D-Convolution-for-Video-based-Person-Re-identification" class="headerlink" title="Appearance-Preserving 3D Convolution for Video-based Person Re-identification"></a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470222.pdf" target="_blank" rel="noopener">Appearance-Preserving 3D Convolution for Video-based Person Re-identification</a></h6><ul>
<li>setting: v-to-v</li>
<li>issue:  temporal coherence </li>
<li>method:  we disentangle the video representation into the temporal coherence and motion parts and randomly change the scale of the temporal motion features as the adversarial noise.</li>
</ul>
<h6 id="Temporal-Complementary-Learning-for-Video-Person-Re-Identification"><a href="#Temporal-Complementary-Learning-for-Video-Person-Re-Identification" class="headerlink" title="Temporal Complementary Learning for Video Person Re-Identification"></a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700392.pdf" target="_blank" rel="noopener">Temporal Complementary Learning for Video Person Re-Identification</a></h6><ul>
<li>setting: v-to-v</li>
<li>issue:  do not take full advantage of the rich spatial-temporal clues in the video. The redundant features typically attend to the same local salient parts, which are difficult to distinguish the persons with similar appearance.  </li>
<li>method:  discover diverse visual cues for different frames of a video to form a full characteristic of each identify.  erases the most salient part of the second and subsequent frames of the input video sequence.</li>
</ul>
<h6 id="Temporal-Coherence-or-Temporal-Motion-Which-is-More-Critical-for-Video-based-Person-Re-identification"><a href="#Temporal-Coherence-or-Temporal-Motion-Which-is-More-Critical-for-Video-based-Person-Re-identification" class="headerlink" title="Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?"></a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530647.pdf" target="_blank" rel="noopener">Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</a></h6><ul>
<li>setting: v-to-v</li>
<li>issue: temporal appearance misalignment </li>
<li>method:  Appearance-Preserving 3D Convolution</li>
</ul>
<h5 id="Image-to-image"><a href="#Image-to-image" class="headerlink" title="Image-to-image"></a>Image-to-image</h5><h5 id="Image-to-video"><a href="#Image-to-video" class="headerlink" title="Image-to-video"></a>Image-to-video</h5><h4 id="Paper1"><a href="#Paper1" class="headerlink" title="Paper1"></a>Paper1</h4><ul>
<li><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720256.pdf" target="_blank" rel="noopener">Exploiting Temporal Coherence for Self-Supervised One-shot Video Re-identification</a></p>
</li>
<li><p>one-shot: one tracklet</p>
</li>
<li><p>combining pseudo-labels and representation learning from temperoal cohenrence</p>
</li>
<li><p>novel: representation learning.</p>
</li>
</ul>
<h5 id="Paper2"><a href="#Paper2" class="headerlink" title="Paper2"></a>Paper2</h5><ul>
<li><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</a></p>
</li>
<li><p>one-shot: one tracklet</p>
</li>
</ul>
<h4 id="Paper3"><a href="#Paper3" class="headerlink" title="Paper3"></a>Paper3</h4><ul>
<li><p><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Bak_One-Shot_Metric_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">One-Shot Metric Learning for Person Re-identification</a></p>
</li>
<li><p>one-shot: single-pair</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/09/22/Network/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/22/Network/" itemprop="url">Incremental/Real-time Generative Adversarial Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-22T11:03:10+08:00">
                2020-09-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Few-shot-Generation/" itemprop="url" rel="index">
                    <span itemprop="name">Few-shot Generation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li>Paper1: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Incremental Learning Using Conditional Adversarial Networks</a></li>
</ul>
<ul>
<li>Paper2: <a href="https://arxiv.org/pdf/1910.01568.pdf" target="_blank" rel="noopener">Incremental learning for the detection and classification of GAN-generated images</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/09/20/Few-shot-Learning-Baselines/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/20/Few-shot-Learning-Baselines/" itemprop="url">Few-shot Learning Baselines</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-20T17:37:27+08:00">
                2020-09-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Meta-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Meta-learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Few-shot-image-Classification"><a href="#Few-shot-image-Classification" class="headerlink" title="Few-shot image Classification"></a>Few-shot image Classification</h4><h5 id="Related-baselines"><a href="#Related-baselines" class="headerlink" title="Related baselines"></a>Related baselines</h5><ul>
<li><p>Paper1: <a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="noopener">Matching Networks for One Shot Learning</a>, <a href="https://github.com/AntreasAntoniou/MatchingNetworks" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper2: <a href="https://arxiv.org/pdf/1703.03400.pdf" target="_blank" rel="noopener">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>, <a href="https://github.com/cbfinn/maml" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper3: <a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="noopener">On First-Order Meta-Learning Algorithms</a>, <a href="https://github.com/openai/supervised-reptile" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper4: <a href="https://arxiv.org/pdf/1711.06025.pdf" target="_blank" rel="noopener">Learning to Compare: Relation Network for Few-Shot Learning
</a>, <a href="https://github.com/floodsung/LearningToCompare_FSL" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper5: <a href="https://arxiv.org/pdf/2003.14247.pdf" target="_blank" rel="noopener">DPGN: Distribution Propagation Graph Network for Few-shot Learning</a>, <a href="https://github.com/megvii-research/DPGN" target="_blank" rel="noopener">Code links</a></p>
</li>
</ul>
<ul>
<li><p>Paper6: <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Meta-Transfer Learning for Few-Shot Learning</a>, <a href="https://github.com/y2l/meta-transfer-learning-tensorflow" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper7: <a href="https://arxiv.org/pdf/2001.08735.pdf" target="_blank" rel="noopener">Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation</a>, <a href="https://github.com/hytseng0509/CrossDomainFewShot" target="_blank" rel="noopener">Code links</a></p>
</li>
</ul>
<h5 id="Related-datasets"><a href="#Related-datasets" class="headerlink" title="Related datasets"></a>Related datasets</h5><ul>
<li><p>Dataset1: <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="noopener">Omniglot</a></p>
</li>
<li><p>Dataset2: <a href="https://www.nist.gov/itl/products-and-services/emnist-dataset" target="_blank" rel="noopener">EMNIST</a></p>
</li>
<li><p>Dataset3: <a href="https://mtl.yyliu.net/download/" target="_blank" rel="noopener">miniImagenet</a></p>
</li>
<li><p>Dataset4: <a href="https://github.com/ElementAI/TADAM" target="_blank" rel="noopener">Fewshot-CIFAR100</a></p>
</li>
<li><p>Dataset5: <a href="https://mtl.yyliu.net/download/" target="_blank" rel="noopener">𝒕𝒊𝒆𝒓𝒆𝒅ImageNet</a></p>
</li>
</ul>
<h4 id="Few-shot-image-generation"><a href="#Few-shot-image-generation" class="headerlink" title="Few-shot image generation"></a>Few-shot image generation</h4><h5 id="Related-baselines-1"><a href="#Related-baselines-1" class="headerlink" title="Related baselines"></a>Related baselines</h5><ul>
<li><p>Paper1: <a href="http://proceedings.mlr.press/v84/bartunov18a/bartunov18a.pdf" target="_blank" rel="noopener">Few-shot Generative Modelling with Generative Matching Networks</a>, <a href="hhttps://github.com/sbos/gmn" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper2: <a href="https://arxiv.org/pdf/1901.02199.pdf" target="_blank" rel="noopener">FIGR: Few-shot Image Generation with Reptile</a>, <a href="https://github.com/LuEE-C/FIGR" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper3: <a href="https://arxiv.org/pdf/2001.00576" target="_blank" rel="noopener">DAWSON: A do- main adaptive few shot generation framework.</a>, <a href="https://github.com/LC1905/musegan/" target="_blank" rel="noopener">Code links</a></p>
</li>
<li><p>Paper4: <a href="https://arxiv.org/pdf/1711.04340" target="_blank" rel="noopener">Data Augmentation Generative Adversarial Networks</a>, <a href="https://github.com/AntreasAntoniou/DAGAN" target="_blank" rel="noopener">Code links</a></p>
</li>
</ul>
<h5 id="Related-datasets-1"><a href="#Related-datasets-1" class="headerlink" title="Related datasets"></a>Related datasets</h5><ul>
<li><p>Dataset1: <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="noopener">Omniglot</a></p>
</li>
<li><p>Dataset2: <a href="https://www.nist.gov/itl/products-and-services/emnist-dataset" target="_blank" rel="noopener">EMNIST</a></p>
</li>
<li><p>Dataset3: <a href="https://drive.google.com/drive/folders/15x2C11OrNeKLMzBDHrv8NPOwyre6H3O5" target="_blank" rel="noopener">VGGFace</a></p>
</li>
<li><p>Dataset4: <a href="https://github.com/NVlabs/FUNIT" target="_blank" rel="noopener">Animal Faces</a></p>
</li>
<li><p>Dataset5: <a href="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/" target="_blank" rel="noopener">Flowers</a></p>
</li>
<li><p>Dataset6: <a href="http://dl.allaboutbirds.org/nabirds" target="_blank" rel="noopener">NABbirds</a></p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/09/08/Learning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/08/Learning/" itemprop="url">Normalization in Deep Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-08T09:30:39+08:00">
                2020-09-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Related-links"><a href="#Related-links" class="headerlink" title="Related links"></a>Related links</h4><ul>
<li><a href="https://zhuanlan.zhihu.com/p/43200897" target="_blank" rel="noopener">1</a></li>
<li><a href="http://xiaofengshi.com/2019/03/06/深度学习-Normalization/" target="_blank" rel="noopener">2</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/75920414" target="_blank" rel="noopener">3</a></li>
</ul>
<h4 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h4><ul>
<li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">BN</a></li>
<li>applicable in cnn and mlp, not applicable in rnn.</li>
<li>statistical data of mini-batch</li>
<li>sensitve to the batchsize</li>
<li>widely used on classification task, generally avoid being used in generative tasks.</li>
<li>training phase: mu and sigma are calculated from feature map of all mini-batch, scale and shift are learned from data.</li>
<li>testing phase: using the global mu and sigma as the inputs of bn layer, because single sample is tested in inference phase instead of mini-batch.</li>
</ul>
<h4 id="InstanceNorm"><a href="#InstanceNorm" class="headerlink" title="InstanceNorm"></a>InstanceNorm</h4><ul>
<li><a href="https://arxiv.org/pdf/1607.08022.pdf" target="_blank" rel="noopener">IN</a></li>
<li>mu and sigma are calculated from feature map of single sample.</li>
<li>scale and shift are different at each channel.</li>
<li>not applicable for 2-D feature map, like mlp and rnn.</li>
<li>applicable for generative tasks.</li>
</ul>
<h4 id="LayerNorm"><a href="#LayerNorm" class="headerlink" title="LayerNorm"></a>LayerNorm</h4><ul>
<li><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">LN paper</a></li>
<li>applicable in rnn.</li>
</ul>
<h4 id="GroupNorm"><a href="#GroupNorm" class="headerlink" title="GroupNorm"></a>GroupNorm</h4><ul>
<li><a href="https://arxiv.org/pdf/1803.08494.pdf" target="_blank" rel="noopener">GN paper</a></li>
<li>tradeoff between LN and IN</li>
</ul>
<h4 id="WeightNorm"><a href="#WeightNorm" class="headerlink" title="WeightNorm"></a>WeightNorm</h4><ul>
<li><a href="https://zhuanlan.zhihu.com/p/55102378" target="_blank" rel="noopener">relationship between bn and wn</a></li>
<li><a href="https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf" target="_blank" rel="noopener">WN paper</a></li>
<li>applicable in rnn and gan.</li>
<li>normalization in weights of model parameters.</li>
</ul>
<h4 id="CosineNorm"><a href="#CosineNorm" class="headerlink" title="CosineNorm"></a>CosineNorm</h4><ul>
<li><a href="https://arxiv.org/pdf/1702.05870.pdf" target="_blank" rel="noopener">CN paper</a></li>
</ul>
<h4 id="AdaptiveInstanceNorm"><a href="#AdaptiveInstanceNorm" class="headerlink" title="AdaptiveInstanceNorm"></a>AdaptiveInstanceNorm</h4><ul>
<li><a href="https://zhuanlan.zhihu.com/p/57875010" target="_blank" rel="noopener">from bn to in, from in to cin, from cin to adain</a></li>
<li><a href="https://arxiv.org/pdf/1703.06868.pdf" target="_blank" rel="noopener">AdaIN paper</a></li>
<li>learning shift and scale from other samples.</li>
</ul>
<h4 id="AttentiveNorm"><a href="#AttentiveNorm" class="headerlink" title="AttentiveNorm"></a>AttentiveNorm</h4><ul>
<li><a href="https://arxiv.org/pdf/2004.03828.pdf" target="_blank" rel="noopener">AN paper</a></li>
<li>model long-range dependency in class-conditional image generation. It introduces a self-attention module in the convolution-based generator, which is helpful for capturing the relation of distant regions.</li>
<li>based on IN, preserving semantics spatially. </li>
<li><a href="https://github.com/shepnerd/AttenNorm/blob/466d727d27fc17dbccd1a5e2090fe91491a26483/inpaint-attnorm/net/network.py#L8" target="_blank" rel="noopener">tf code</a></li>
<li>can be used in F2GAN(smilar to Class-conditional Image Generation, using the interpolation coefficients as class information)</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2020/09/01/Fewshot-Feature-Generation/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/01/Fewshot-Feature-Generation/" itemprop="url">Fewshot Feature Generation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-01T08:50:54+08:00">
                2020-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/FewShot/" itemprop="url" rel="index">
                    <span itemprop="name">FewShot</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Paper1"><a href="#Paper1" class="headerlink" title="Paper1"></a>Paper1</h4><ul>
<li><p>paper links: <a href="https://arxiv.org/pdf/1806.04734.pdf" target="_blank" rel="noopener">∆-encoder: an effective sample synthesis method for few-shot object recognition
</a></p>
</li>
<li><p>feature augmentation.</p>
</li>
</ul>
<h4 id="Paper2"><a href="#Paper2" class="headerlink" title="Paper2"></a>Paper2</h4><ul>
<li>paper links: <a href="https://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks.pdf" target="_blank" rel="noopener">Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks</a></li>
<li><p>feature augmentation.</p>
</li>
<li><p>addressing the problem of <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Hariharan_Low-Shot_Visual_Recognition_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Low-shot Visual Recognition by Shrinking and Hallucinating Features</a>: learn a finite set of transformation mappings between examples in each base category and directly apply them to seed novel points for extra data. However, since mappings are<br>enumerable (even in large amount), this model suffers from poor generalization. </p>
</li>
<li><p>intra-class variances of base classes are shareable with any novel classes.</p>
</li>
<li><p>building upon the assumption that related classes should have similar intra-class variance, we introduce a new loss term for preserving covariance during the translation process.</p>
</li>
<li><p>incorporating extra noise from a mixture of Gaussian distributions could result in more<br>diverse results.</p>
</li>
<li><p>preserving covariance information from relevant base classes to a novel class can improve low-shot generation quality.</p>
</li>
<li><p>weighted loss: similarity, weights assigned to each training samples (the distance between noval class and base class). No reconstruction method, 2way-2shot.</p>
</li>
</ul>
<h4 id="Paper3"><a href="#Paper3" class="headerlink" title="Paper3"></a>Paper3</h4><ul>
<li><p>paper links: <a href="https://arxiv.org/pdf/1803.09014.pdf" target="_blank" rel="noopener">Feature Transfer Learning for Face Recognition with Under-Represented Data</a></p>
</li>
<li><p>A Gaussian prior of the variance is assumed across all subjects and the variance from regular ones are transferred to the under-represented ones.</p>
</li>
<li><p>generating vague images: translatede results.</p>
</li>
</ul>
<h4 id="Paper4"><a href="#Paper4" class="headerlink" title="Paper4"></a>Paper4</h4><ul>
<li><p>paper links: <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Dixit_AGA_Attribute-Guided_Augmentation_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Attribute-Guided Augmentation</a></p>
</li>
<li><p>feature augmentation.</p>
</li>
<li><p>learns a mapping that allows syn- thesis of data such that an attribute of a synthesized sample is at a desired value or strength.</p>
</li>
<li><p>This is particularly inter- esting in situations where little data with no attribute an- notation is available for learning, but we have access to an external corpus of heavily annotated samples.</p>
</li>
</ul>
<h4 id="Paper5"><a href="#Paper5" class="headerlink" title="Paper5"></a>Paper5</h4><ul>
<li><p>paper links: <a href="https://arxiv.org/pdf/2002.10826.pdf" target="_blank" rel="noopener">Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective</a></p>
</li>
<li><p>Constructing the feature cloud for long-tailed data.</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Hongyan</p>
              <p class="site-description motion-element" itemprop="description">New Rhythm</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">55</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hy-zpg" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yanhong.sjtu@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hongyan</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === '') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
