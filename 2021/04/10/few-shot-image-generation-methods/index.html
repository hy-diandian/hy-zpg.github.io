<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="default">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="few-shot learning,">





  <link rel="alternate" href="/atom.xml" title="Hongyan's Notes" type="application/atom+xml">






<meta name="description" content="Few-shot Image GenerationFew-shot Image Generation with Elastic Weight Consolidation NIPS 2020 paper  core idea: preserving the diversity of source domain, adapting the information of source domain to">
<meta name="keywords" content="few-shot learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Few-shot Image Generation Methods and Applications">
<meta property="og:url" content="https://www.yanhong.website/2021/04/10/few-shot-image-generation-methods/index.html">
<meta property="og:site_name" content="Hongyan&#39;s Notes">
<meta property="og:description" content="Few-shot Image GenerationFew-shot Image Generation with Elastic Weight Consolidation NIPS 2020 paper  core idea: preserving the diversity of source domain, adapting the information of source domain to">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2021-04-12T06:59:43.719Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Few-shot Image Generation Methods and Applications">
<meta name="twitter:description" content="Few-shot Image GenerationFew-shot Image Generation with Elastic Weight Consolidation NIPS 2020 paper  core idea: preserving the diversity of source domain, adapting the information of source domain to">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.yanhong.website/2021/04/10/few-shot-image-generation-methods/">





  <title>Few-shot Image Generation Methods and Applications | Hongyan's Notes</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hongyan's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Cheatsheet</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            Guestbook
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.yanhong.website/2021/04/10/few-shot-image-generation-methods/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hongyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hongyan's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Few-shot Image Generation Methods and Applications</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-04-10T09:43:48+08:00">
                2021-04-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Meta-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Meta-learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Few-shot-Image-Generation"><a href="#Few-shot-Image-Generation" class="headerlink" title="Few-shot Image Generation"></a>Few-shot Image Generation</h2><h3 id="Few-shot-Image-Generation-with-Elastic-Weight-Consolidation"><a href="#Few-shot-Image-Generation-with-Elastic-Weight-Consolidation" class="headerlink" title="Few-shot Image Generation with Elastic Weight Consolidation"></a>Few-shot Image Generation with Elastic Weight Consolidation</h3><ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2020/file/b6d767d2f8ed5d21a44b0e5886680cb9-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p>
</li>
<li><p>core idea: preserving the diversity of source domain, adapting the information of source domain to the target domain with a few sampels from target domain with adapting the pretrained model’s weights.</p>
</li>
<li><p>storyline:</p>
<p>** 70,000 images for just a specific domain (aligned faces) or 1.3M images across different classes -&gt; in the artistic domain, it is at best cumbersome, and at times prohibitive, to hire artists to make thousands of creations -&gt; our goal is to generalize from a few, new examples.  <font color="blue">our setting need much data belonging to same domain such as flowers, animal faces, then using a few samples from different category like funit regarding category as domain</font>.</p>
<p>** training an algorithm to generate more data of a target domain, given only a few examples. An underlying assumption with this setup is that the source and target domains share some latent factors, with some differences related to their distinct difference in appearance. <font color="blue">finding shared some latent code, how near or how far</font>.</p>
<p>** propose a straightforward and effective adaptation technique, adapting the pretrained model’s weights, without introducing additional parameters. Fixing the architecture implies that tedious manual designs on new parameters are not necessary.</p>
<p>** A key property to note is that weights have different levels of importance; thus, each parameter should not be treated equally in the adaptation, or tuning process.  we consider there will always be an inherent trade-off between preserving information from the source and adapting to the target domain. <font color="blue">the number of target examples and the dissimilarity between the source and target domain</font>.</p>
</li>
<li><p>method: </p>
<p>** we do the adaptation with a few examples, some weights in the last layer are more important and should be better preserved than those in other layers.</p>
<p>** we could directly use Fisher information as an importance measure for weights and add a regularization loss to penalize the weight change.</p>
</li>
<li><p>setting: <font color="red">big domain without considering the category of source domain and target domain. real faces vs. emoji faces.</font></p>
</li>
</ul>
<ul>
<li><p>experiment:</p>
<p><em>* datasets: real faces (200k) vs. emoji faces (80k), animal faces (10 </em> 5000) vs. portrait paintings (16 * 10). landscape photos (5000) vs. pencil landscape drawings(10). <font color="blue">data size 10-80k</font></p>
<p>** evaluation metrics: FID(user study) for more taget images (less target images) and LPIPS.</p>
<p>** visualization.</p>
</li>
</ul>
<h3 id="Few-shot-adaptation-of-generative-adversarial-networks"><a href="#Few-shot-adaptation-of-generative-adversarial-networks" class="headerlink" title="Few-shot adaptation of generative adversarial networks"></a>Few-shot adaptation of generative adversarial networks</h3><ul>
<li><p><a href="https://openreview.net/pdf?id=6R51jA4fOB" target="_blank" rel="noopener">ICLR 2021 paper</a></p>
</li>
<li><p>core idea: proposesing a simple and effective method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than 100 images) by repurposing component analysis techniques and learns to adapt the singular values of the pre-trained weights while freezing the corresponding singular vectors.</p>
</li>
<li><p>storyline:</p>
<p>**  Training these models, however, typically requires large, diverse datasets in a target visual domain -&gt; low-data regime (e.g., less than 1,000 samples), GANs frequently suffer from memorization or instability, leading to a lack of diversity or poor visual quality -&gt; transfer learning, images generated by GAN-based methods are blurry and lack details, flow-based models require compute- and memory-intensive architectures.</p>
<p>** we restrict the space of trainable parameters to a small number of highly-expressive parameters that modulate orthogonal features of the pre-trained weight space.</p>
<p>** Our method first applies singular value decomposition (SVD) to the network weights of a pretrained GAN (generator + discriminator). We then adapts the singular values using GAN optimization on the target few-shot domain, with fixed left/right singular vectors. We show that varying singular values in the weight space corresponds to semantically meaningful changes of the synthesized image while preserving natural structure. proposed method achieves higher image quality after adaptation. </p>
</li>
<li><p>setting: big domain without considering the category of source domain and target domain. real faces vs. emoji faces.</p>
</li>
<li><p>experiments:</p>
<p>** datasets: LSUN Churches→ Van Gogh paintings(30), FFHQ→Art portraits(5-100), FFHQ→Anime Rem ID(25).  <font color="blue">data size 5-100</font></p>
<p>** metrics: FID, sharpness, and face quality index (FQI).</p>
</li>
</ul>
<h3 id="Augmentation-Interpolative-AutoEncoders-for-Unsupervised-Few-Shot-Image-Generation"><a href="#Augmentation-Interpolative-AutoEncoders-for-Unsupervised-Few-Shot-Image-Generation" class="headerlink" title="Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation"></a>Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation</h3><ul>
<li><p><a href="https://arxiv.org/pdf/2011.13026.pdf" target="_blank" rel="noopener">paper</a></p>
</li>
<li><p>core idea: autoencoders generalize extremely well to new domains, even when trained on highly constrained data.</p>
</li>
<li><p>storyline: </p>
<p>** generating high-quality, diverse, high-resolution images requires a large dataset -&gt; need generative models that can train on one set of image classes, and then generalize to a new class using only a small quantity of new images: few-shot image generation.</p>
<p>** previous methods need large labeled datasets of hundreds of classes, substantial computation at test time, highly domain-specific, generalizing only across very similar classes.</p>
<p>** We leverage the finding that although the latent spaces of powerful generative models, such as VAEs and GANs, do not generalize to new classes, the representations learned by autoencoders (AEs) generalize extremely well -&gt; Interpolative AutoEncoders -&gt; Augmentation-Interpolative Autoencoder (AugIntAE) achieves simple, robust, highly general, and completely unsupervised few-shot image generation.</p>
</li>
<li><p>setting: given  a large, unlabelled collection of images depicting objects from a set of seen classes and  a very small set of images - as few as two - belonging to a novel class, Our goal is to train a network on seen classes and generates images clearly belonging to the novel class. interpolating a pair of unseen images to generate new images belonging to the novel class.</p>
</li>
<li><p>experiments:</p>
<p>** datasets:MNIST → EMNIST , Omniglot (train → test), CelebA (male → female) , CIFAR-10 → CIFAR-100</p>
<p>** metrics: FID score and train/test classification error.</p>
</li>
</ul>
<h3 id="MineGAN-effective-knowledge-transfer-from-GANs-to-target-domains-with-few-images"><a href="#MineGAN-effective-knowledge-transfer-from-GANs-to-target-domains-with-few-images" class="headerlink" title="MineGAN: effective knowledge transfer from GANs to target domains with few images"></a>MineGAN: effective knowledge transfer from GANs to target domains with few images</h3><ul>
<li><p><a href="https://arxiv.org/pdf/1912.05270.pdf" target="_blank" rel="noopener">CVPR 2020 paper</a></p>
</li>
<li><p>core idea: using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain.</p>
</li>
</ul>
<ul>
<li>storyline: high-quality pretrained models -&gt; combining high-quality pretrained models with other models and adjust them to a target distribution is a desirable objective -&gt; knowledge transfer for generative models has received significantly less attention, possibly due to its great difficulty, especially when transferring to target domains with few images -&gt; previous works severely limits the flexibility of the knowledge transfer, mode collapse -&gt; a miner network that transforms a multivariate normal distribution into a distribution on the input space of the pretrained GAN in such a way that the generated images resemble those of the target domain.</li>
</ul>
<ul>
<li><p>setting: big domain without considering the category of source domain and target domain. human faces vs. children faces.</p>
</li>
<li><p>experiment:</p>
<p>** dataset: CelebA→FFHQ children, <font color="blue">data size 100-1000</font> </p>
<p>** evaluation: MV, KMMD, FID</p>
</li>
</ul>
<h3 id="CharacterGAN-Few-Shot-Keypoint-Character-Animation-and-Reposing"><a href="#CharacterGAN-Few-Shot-Keypoint-Character-Animation-and-Reposing" class="headerlink" title="CharacterGAN: Few-Shot Keypoint Character Animation and Reposing"></a>CharacterGAN: Few-Shot Keypoint Character Animation and Reposing</h3><ul>
<li><a href="https://arxiv.org/pdf/2102.03141.pdf" target="_blank" rel="noopener">paper</a></li>
</ul>
<h3 id="GAN-Memory-with-No-Forgetting"><a href="#GAN-Memory-with-No-Forgetting" class="headerlink" title="GAN Memory with No Forgetting"></a>GAN Memory with No Forgetting</h3><ul>
<li><a href="https://papers.nips.cc/paper/2020/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></li>
</ul>
<h3 id="On-Leveraging-Pretrained-GANs-for-Generation-with-Limited-Data"><a href="#On-Leveraging-Pretrained-GANs-for-Generation-with-Limited-Data" class="headerlink" title="On Leveraging Pretrained GANs for Generation with Limited Data"></a>On Leveraging Pretrained GANs for Generation with Limited Data</h3><ul>
<li><a href="http://proceedings.mlr.press/v119/zhao20a/zhao20a.pdf" target="_blank" rel="noopener">ICML 2020 paper</a></li>
</ul>
<h3 id="Differentiable-Augmentation-for-Data-Efficient-GAN-Training"><a href="#Differentiable-Augmentation-for-Data-Efficient-GAN-Training" class="headerlink" title="Differentiable Augmentation for Data-Efficient GAN Training"></a>Differentiable Augmentation for Data-Efficient GAN Training</h3><ul>
<li><p><a href="https://proceedings.neurips.cc//paper/2020/file/55479c55ebd1efd3ff125f1337100388-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p>
</li>
<li><p>method:  improving the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples.</p>
</li>
<li><p>storyline:</p>
<p>** The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set.</p>
<p>** it is of critical importance to eliminate the need of immense datasets for GAN training.</p>
<p>** suggesting that the discriminator is simply memorizing the entire training set. This severe over-fitting problem disrupts the training dynamics and leads to degraded image quality.</p>
<p>** augmentations (cropping, flipping, scaling, color jittering, and region masking) in real images, the generator would be encouraged to match the distribution of the augmented images.</p>
<p>** augmentation need to be done in real images and generate images, which breaks the subtle balance between the generator and discriminator, leading to poor convergence as they are optimizing completely different objectives.</p>
<p>** DiffAugment, which applies the same differentiable augmentation to both real and fake images for both generator and discriminator training. It enables the gradients to be propagated through the augmentation back to the generator, regularizes the discriminator without manipulating the target distribution, and maintains the balance of training dynamics.</p>
</li>
<li><p>setting: training set  <font color="blue">data size 25% ~ 100% </font> </p>
</li>
</ul>
<h3 id="Training-Generative-Adversarial-Networks-with-Limited-Data"><a href="#Training-Generative-Adversarial-Networks-with-Limited-Data" class="headerlink" title="Training Generative Adversarial Networks with Limited Data"></a>Training Generative Adversarial Networks with Limited Data</h3><ul>
<li><p><a href="https://papers.nips.cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p>
</li>
<li><p>methods: augmentation strtegies in discriminator.</p>
</li>
<li><p>setting: training set <font color="blue">data size 1k ~ 200k </font> </p>
</li>
</ul>
<h3 id="Image-Generation-From-Small-Datasets-via-Batch-Statistics-Adaptation"><a href="#Image-Generation-From-Small-Datasets-via-Batch-Statistics-Adaptation" class="headerlink" title="Image Generation From Small Datasets via Batch Statistics Adaptation"></a>Image Generation From Small Datasets via Batch Statistics Adaptation</h3><ul>
<li><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">2019 ICCV paper</a></p>
</li>
<li><p>core idea: Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small datase.</p>
</li>
<li><p>method: we propose a novel method focusing on the pa- rameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small (<font color="blue">data size 25 ~ 500 </font> ).</p>
</li>
<li><p>setting: selecting pretrained model SNGAN and BigGAN. All experiments in this paper are not class conditional, but our method can be easily extended to be class-conditional by learning BatchNorm statistics for each class independently.</p>
</li>
<li><p>experiment:</p>
<p>** dataset: FFHQ dataset -&gt; anime face datase, imagenet -&gt; oxford flowers. </p>
<p>** metrics: FID, KMMD</p>
</li>
</ul>
<h2 id="Inspiration"><a href="#Inspiration" class="headerlink" title="Inspiration"></a>Inspiration</h2><ul>
<li><font color="red"> few-shot image generation - source domain: category information, conditional GAN -> targte domain: category information, presevring diversity information and distinctive feature among different categories. Given a few samples from different category of target domain </font> 

</li>
</ul>
<font color="red">  </font> 

<h2 id="Few-shot-Font-Generation"><a href="#Few-shot-Font-Generation" class="headerlink" title="Few-shot Font Generation"></a>Few-shot Font Generation</h2><h3 id="GANwriting-Content-Conditioned-Generation-of-Styled-Handwritten-Word-Images"><a href="#GANwriting-Content-Conditioned-Generation-of-Styled-Handwritten-Word-Images" class="headerlink" title="GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images"></a>GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images</h3><ul>
<li><p><a href="https://arxiv.org/pdf/2003.02567.pdf" target="_blank" rel="noopener">2020 ECCV paper</a></p>
</li>
<li><p>storyline:</p>
<p>** the classes had to be predefined beforehand during the training stage in conditional gan,  it was impossible to produce images from other unseen classes during inference.</p>
<p>** sequence -&gt; generate raw images. the produced results by previous works are not realistic, still exhibiting a poor quality, sometimes producing barely legible word images.</p>
<p>** we present a non-recurrent generative architecture conditioned to textual content sequences, that is specially tailored to produce realistic handwritten word images, indis- tinguishable to humans. our approach1 is able to artificially render realistic handwritten word images that match a certain textual content and that mimic some style features (text skew, slant, roundness, stroke width, ligatures, etc.) from an exemplar writer.</p>
</li>
</ul>
<h3 id="Few-Shot-Text-Style-Transfer-via-Deep-Feature-Similarity"><a href="#Few-Shot-Text-Style-Transfer-via-Deep-Feature-Similarity" class="headerlink" title="Few-Shot Text Style Transfer via Deep Feature Similarity"></a>Few-Shot Text Style Transfer via Deep Feature Similarity</h3><ul>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9098082" target="_blank" rel="noopener">TIP 2020 paper</a></li>
</ul>
<h3 id="JointFontGAN-Joint-Geometry-Content-GAN-for-Font-Generation-via-Few-Shot-Learning"><a href="#JointFontGAN-Joint-Geometry-Content-GAN-for-Font-Generation-via-Few-Shot-Learning" class="headerlink" title="JointFontGAN: Joint Geometry-Content GAN for Font Generation via Few-Shot Learning"></a>JointFontGAN: Joint Geometry-Content GAN for Font Generation via Few-Shot Learning</h3><ul>
<li><a href="https://par.nsf.gov/servlets/purl/10199594" target="_blank" rel="noopener">ACM MM 2020</a></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/few-shot-learning/" rel="tag"># few-shot learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/11/21/Shadow-Generation/" rel="next" title="Shadow Generation">
                <i class="fa fa-chevron-left"></i> Shadow Generation
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Hongyan</p>
              <p class="site-description motion-element" itemprop="description">New Rhythm</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">54</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">35</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hy-zpg" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yanhong.sjtu@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Few-shot-Image-Generation"><span class="nav-number">1.</span> <span class="nav-text">Few-shot Image Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Few-shot-Image-Generation-with-Elastic-Weight-Consolidation"><span class="nav-number">1.1.</span> <span class="nav-text">Few-shot Image Generation with Elastic Weight Consolidation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Few-shot-adaptation-of-generative-adversarial-networks"><span class="nav-number">1.2.</span> <span class="nav-text">Few-shot adaptation of generative adversarial networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Augmentation-Interpolative-AutoEncoders-for-Unsupervised-Few-Shot-Image-Generation"><span class="nav-number">1.3.</span> <span class="nav-text">Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MineGAN-effective-knowledge-transfer-from-GANs-to-target-domains-with-few-images"><span class="nav-number">1.4.</span> <span class="nav-text">MineGAN: effective knowledge transfer from GANs to target domains with few images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CharacterGAN-Few-Shot-Keypoint-Character-Animation-and-Reposing"><span class="nav-number">1.5.</span> <span class="nav-text">CharacterGAN: Few-Shot Keypoint Character Animation and Reposing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GAN-Memory-with-No-Forgetting"><span class="nav-number">1.6.</span> <span class="nav-text">GAN Memory with No Forgetting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-Leveraging-Pretrained-GANs-for-Generation-with-Limited-Data"><span class="nav-number">1.7.</span> <span class="nav-text">On Leveraging Pretrained GANs for Generation with Limited Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Differentiable-Augmentation-for-Data-Efficient-GAN-Training"><span class="nav-number">1.8.</span> <span class="nav-text">Differentiable Augmentation for Data-Efficient GAN Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Generative-Adversarial-Networks-with-Limited-Data"><span class="nav-number">1.9.</span> <span class="nav-text">Training Generative Adversarial Networks with Limited Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Generation-From-Small-Datasets-via-Batch-Statistics-Adaptation"><span class="nav-number">1.10.</span> <span class="nav-text">Image Generation From Small Datasets via Batch Statistics Adaptation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inspiration"><span class="nav-number">2.</span> <span class="nav-text">Inspiration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Few-shot-Font-Generation"><span class="nav-number">3.</span> <span class="nav-text">Few-shot Font Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GANwriting-Content-Conditioned-Generation-of-Styled-Handwritten-Word-Images"><span class="nav-number">3.1.</span> <span class="nav-text">GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Few-Shot-Text-Style-Transfer-via-Deep-Feature-Similarity"><span class="nav-number">3.2.</span> <span class="nav-text">Few-Shot Text Style Transfer via Deep Feature Similarity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JointFontGAN-Joint-Geometry-Content-GAN-for-Font-Generation-via-Few-Shot-Learning"><span class="nav-number">3.3.</span> <span class="nav-text">JointFontGAN: Joint Geometry-Content GAN for Font Generation via Few-Shot Learning</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hongyan</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === '') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
