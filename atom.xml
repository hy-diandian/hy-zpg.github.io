<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hongyan&#39;s Notes</title>
  
  <subtitle>Cheatsheet</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yanhong.me/"/>
  <updated>2019-02-23T11:33:10.865Z</updated>
  <id>https://www.yanhong.me/</id>
  
  <author>
    <name>Hongyan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Why-How-Pseudo</title>
    <link href="https://www.yanhong.me/2019/02/22/Why-How-Pseudo/"/>
    <id>https://www.yanhong.me/2019/02/22/Why-How-Pseudo/</id>
    <published>2019-02-22T02:53:37.000Z</published>
    <updated>2019-02-23T11:33:10.865Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Pseudo labels in multi-task learning</li><li>Pseudo data selection with density and distribution distance</li></ul><h2 id="Pseudo-labels"><a href="#Pseudo-labels" class="headerlink" title="Pseudo labels"></a>Pseudo labels</h2><p>$ Reason</p><ul><li><blockquote><p>reference <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper.pdf" target="_blank" rel="noopener">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a>, demonstrating that training a CNN from scratch using both clean and noisy data is better than just using the clean one, on the condition that the amount of pseudo data is limited.</p></blockquote></li><li><blockquote><p>reference <a href="https://arxiv.org/pdf/1609.06426.pdf" target="_blank" rel="noopener">From Facial Expression Recognition to Interpersonal Relation Prediction Zhanpeng</a>, stating explaination that pseudo data can bridge the gap between heterogeneous data.</p></blockquote></li><li><blockquote><p>reference <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Misra_Seeing_Through_the_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels</a>, ‘what’s the all attributs of images’ versus ‘what’s the labeled attributes’, labeling missing attributes. The novalty may be regarded as a baseline (waiting to understand the detail).</p></blockquote></li><li>data imbalance in joint training strategy, such as 20k~ pose dataset AFLW and 90k~ emotion dataset ExpW.</li></ul><h2 id="Pseudo-data-selection"><a href="#Pseudo-data-selection" class="headerlink" title="Pseudo data selection"></a>Pseudo data selection</h2><p>Because much noisy data can effect the model performance and even disturb the model training process, leading to poor model performance, pseudo data selection can control the ratio of pseudo data in the whole datasets.</p><h3 id="Density"><a href="#Density" class="headerlink" title="Density"></a>Density</h3><ul><li>reason: because clusters are easily detected by the local density of data points, in the pseudo data belong to same categoty have similar feature. Appling a density based clustering algorithm that measures the complexity of psuedo data using data distribution density in. each category.</li><li>implementation detail: measuring the purity of data with pseudo label based on its distribution density in a feature space, and rank the purity to generate pseudo weights, pseudo data with higher density is assigned larger weights, while smaller weights are assigned to low-density pseudo data.</li></ul><h3 id="Distribution-distance"><a href="#Distribution-distance" class="headerlink" title="Distribution distance"></a>Distribution distance</h3><ul><li>reason: <ul><li>‘transfer learning is one important method in machine learning, it can relax the condition of the independent identical distribution in train dataset and test dataset, so that knowledge can be transfered from source domain to target domain’,’its main application includes domain adaptation and multi-domain tranferation’.</li><li>instance-based domain adaption: calculating the distance between source domain data and target domain data, then adjusting the weights of target domain instance so that the target data can be matched with source domain data. In detail, the smaller distance, the higher similarity.</li></ul></li><li><p>implementation detail: </p><ol><li>GMM<ul><li>reason:</li><li>implementation: generating Gaussian Mixture models (GMM) based on A1 (data with pseuodo label) and B (data with ground truth) in each categoty. Assuming GMM has K Gaussian models, we obtain G(A1_1), …, G(A1_K), G(B1_1), …, G(B1_K); </li></ul></li><li><p>EMD</p><ul><li><p>reason: &gt;reference <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning</a>,  which takes domain scale into account by adding a scale factor. In this paper, transfer learning can be viewed as moving a set of images from the source domain S to the target domain T. The work needed to be done by moving an image to another can be defined as their feature Euaullean distance, so the distance between two domains can be defined as the least amount of total work needed. This definition of domain similarity can be calculated by the Earth Mover’s Distance (EMD). </p><ul><li><p>original EMD equation<br>\begin{aligned}<br>EMD(P,Q)={\frac{\sum _{i=1}^{m}\sum <em>{j=1}^{n}f</em>{i,j}d_{i,j}}{\sum _{i=1}^{m}\sum <em>{j=1}^{n}f</em>{i,j}}}<br>\end{aligned}</p><p>\begin{aligned}<br>P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}<br>\end{aligned}</p><p>\begin{aligned}<br>Q={(q_{1},w_{q1}),(q_{2},w_{q2}),…,(q_{n},w_{qn})}<br>\end{aligned}</p></li><li><p>reference paper EMD application<br>\begin{aligned}<br>EMD(S,T)={\frac{\sum _{i=1}^{m}\sum <em>{j=1}^{n}f</em>{i,j}d_{i,j}}{\sum _{i=1}^{m}\sum <em>{j=1}^{n}f</em>{i,j}}}<br>\end{aligned}</p><p>\begin{aligned}<br>S={(s_{1},w_{s1}),(s_{2},w_{s2}),…,(s_{m},w_{sm})}<br>\end{aligned}</p><p>\begin{aligned}<br>T={(t_{1},w_{t1}),(t_{2},w_{t2}),…,(t_{n},w_{tn})}<br>\end{aligned}</p><p>\begin{aligned}<br>D=[d_{i,j}] = || g(s_{i}) − g(t_{j})||<br>\end{aligned}</p><p>\begin{aligned}<br>g(s_{i}) = mean value of image features in category i from source domain<br>\end{aligned}</p><p>\begin{aligned}<br>g(t_{i}) = mean value of image features in category i from target domain<br>\end{aligned}</p><p>\begin{aligned}<br>sim(S, T) = e−γd(S,T )<br>\end{aligned}</p></li><li><p>EMD application for pseudo data selection<br>\begin{aligned}<br>EMD(S,T)={\frac{\sum _{i=1}^{m}\sum <em>{j=1}^{n}f</em>{i,j}d_{i,j}}{\sum _{i=1}^{m}\sum <em>{j=1}^{n}f</em>{i,j}}}<br>\end{aligned}</p><p>\begin{aligned}<br>P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}<br>\end{aligned}</p><p>\begin{aligned}<br>G={(g_{1},w_{g1}),(g_{2},w_{g2}),…,(g_{n},w_{gn})}<br>\end{aligned}</p><p>\begin{aligned}<br>D=[d_{i,j}] = || g(p_{i}) − g(g_{j})||<br>\end{aligned}</p><p>\begin{aligned}<br>g(p) = mean value of image features in specific cluster from the same category in source domain<br>\end{aligned}</p><p>\begin{aligned}<br>g(g_{i}) = mean value of image features in cluster i from the same category in target domain<br>\end{aligned}</p><p>\begin{aligned}<br>sim(S, T) = e−γd(S,T )<br>\end{aligned}</p></li></ul></li><li>implementation: calculating the distance between G(A1_i) and the whole cluster in G(B1). As for distance calculation with EMD method, we obtain P={G(A1_i)_mean,G(A1_i)_probs} and Q={[G(B1_1)_mean, …, G(B1_K)_mean],[G(B1_1)_probs, …, G(B1_K)_probs]}, so we can calculate distance vector D=(emd_1,emd_k), which is used for obtaining distribution weights weights_g.</li></ul></li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Pseudo labels in multi-task learning&lt;/li&gt;
&lt;li&gt;Pseudo data selection with density and distribution distance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Pseud
      
    
    </summary>
    
      <category term="Theory" scheme="https://www.yanhong.me/categories/Theory/"/>
    
    
      <category term="pesudo labels" scheme="https://www.yanhong.me/tags/pesudo-labels/"/>
    
  </entry>
  
  <entry>
    <title>Density &amp; Distribution</title>
    <link href="https://www.yanhong.me/2019/02/21/Density-Distribution/"/>
    <id>https://www.yanhong.me/2019/02/21/Density-Distribution/</id>
    <published>2019-02-21T07:46:01.000Z</published>
    <updated>2019-02-22T06:44:32.552Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Purity-based-density-amp-distribution-distance-based-GMM"><a href="#Purity-based-density-amp-distribution-distance-based-GMM" class="headerlink" title="Purity based density &amp; distribution distance based GMM"></a>Purity based density &amp; distribution distance based GMM</h2><h3 id="Purity-based-density"><a href="#Purity-based-density" class="headerlink" title="Purity based density"></a>Purity based density</h3><blockquote><p>reference：ECCV 2018 <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper.pdf" target="_blank" rel="noopener">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a></p></blockquote><blockquote><p>original reference： Science 2004 <a href="http://sites.psu.edu/mcnl/files/2017/03/9-2dhti48.pdf" target="_blank" rel="noopener">Clustering by fast search and find of density peaks</a></p></blockquote><h4 id="Creativity"><a href="#Creativity" class="headerlink" title="Creativity"></a>Creativity</h4><ul><li>leveraging data distribution density in feature space to evaluate the complexity of the data<br>ours – evaluating the purity of pseudo label data by data density in feature space</li><li>noisy data can be regared as regularied method to improve the model generalization<br>ours – pesudo labels can improve the model generalization </li></ul><h4 id="Technical-detail"><a href="#Technical-detail" class="headerlink" title="Technical detail"></a>Technical detail</h4><p>Density based clustering algorithm</p><ul><li>generating features</li><li>calculating Euclidean distance D_ij</li><li>calculating local density of each image</li><li>calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence </li></ul><h3 id="Distribution-distance-based-GMM"><a href="#Distribution-distance-based-GMM" class="headerlink" title="Distribution distance based GMM"></a>Distribution distance based GMM</h3><h4 id="Clustring-based-GMM"><a href="#Clustring-based-GMM" class="headerlink" title="Clustring based GMM"></a>Clustring based GMM</h4><h5 id="GMM-with-EM"><a href="#GMM-with-EM" class="headerlink" title="GMM with EM"></a>GMM with EM</h5><ul><li><p>original definition<br>\begin{aligned}<br>p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)<br>\end{aligned}<br>\pi_k  represent the possibility of sample belong to kth category</p></li><li><p>new definition for coding<br>\begin{aligned}<br>\sum_{k} z_k = 1<br>\end{aligned}</p></li></ul><h5 id="Related-code"><a href="#Related-code" class="headerlink" title="Related code"></a>Related code</h5><ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html" target="_blank" rel="noopener">sklearn.mixture.GaussianMixture</a></li></ul><h4 id="Data-distribution-distance-EMD"><a href="#Data-distribution-distance-EMD" class="headerlink" title="Data distribution distance EMD"></a>Data distribution distance EMD</h4><blockquote><p>reference： <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.pdf" target="_blank" rel="noopener"> Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning</a><br>original reference： <a href="http://robotics.stanford.edu/~rubner/papers/rubnerIjcv00.pdf" target="_blank" rel="noopener">The Earth Mover’s Distance as a Metric for Image Retrieval</a></p></blockquote><ul><li><p>EDM definition: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second</p></li><li><p>EMD data signature:<br>\begin{aligned}<br>s= (feature,weights)<br>\end{aligned}</p></li><li><p>related code</p></li><li><a href="https://github.com/chalmersgit/EMD" target="_blank" rel="noopener">general EMD</a> <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/RUBNER/emd.htm" target="_blank" rel="noopener">expaination</a></li><li><a href="https://pypi.org/project/pyemd/" target="_blank" rel="noopener">pyemd-1D data</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html" target="_blank" rel="noopener">scipy.atats.wasserstein_distance</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Purity-based-density-amp-distribution-distance-based-GMM&quot;&gt;&lt;a href=&quot;#Purity-based-density-amp-distribution-distance-based-GMM&quot; class=
      
    
    </summary>
    
      <category term="Theory" scheme="https://www.yanhong.me/categories/Theory/"/>
    
    
      <category term="pesudo labels" scheme="https://www.yanhong.me/tags/pesudo-labels/"/>
    
  </entry>
  
  <entry>
    <title>pseudo purity &amp;&amp; dataset distribution distance</title>
    <link href="https://www.yanhong.me/2019/02/17/Pseudo-purity&amp;distribution-distance/"/>
    <id>https://www.yanhong.me/2019/02/17/Pseudo-purity&amp;distribution-distance/</id>
    <published>2019-02-17T10:07:47.000Z</published>
    <updated>2019-02-21T11:36:27.934Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Purity-and-distribution-distance-based-density"><a href="#Purity-and-distribution-distance-based-density" class="headerlink" title="Purity and distribution distance based density"></a>Purity and distribution distance based density</h2><h3 id="Data-density"><a href="#Data-density" class="headerlink" title="Data density"></a>Data density</h3><blockquote><p>reference  ECCV 2018</p></blockquote><p>adding url links<br><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper.pdf" target="_blank" rel="noopener"> CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a></p><h4 id="creativity"><a href="#creativity" class="headerlink" title="creativity"></a>creativity</h4><ol><li>leveraging data distribution density in feature space to evaluate the complexity of the data<br> ours – evaluating the purity of pseudo label data by data density in feature space</li><li>noisy data can be regared as regularied method to improve the model generalization<br> ours – pesudo labels can improve the model generalization </li></ol><h4 id="technical-detail"><a href="#technical-detail" class="headerlink" title="technical detail"></a>technical detail</h4><ol><li>density based clustering algorithm<ul><li>generating features</li><li>calculating Euclidean distance D_ij<ul><li>calculating local density of each image</li><li>calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence </li></ul></li></ul></li></ol><h3 id="Data-distribution-distance-EMD"><a href="#Data-distribution-distance-EMD" class="headerlink" title="Data distribution distance EMD"></a>Data distribution distance EMD</h3><blockquote><p>reference IJCV 2000</p></blockquote><p>$ adding url links<br><a href="http://robotics.stanford.edu/~rubner/papers/rubnerIjcv00.pdf" target="_blank" rel="noopener">The Earth Mover’s Distance as a Metric for Image Retrieval</a></p><h4 id="definition"><a href="#definition" class="headerlink" title="definition"></a>definition</h4><ol><li>EDM: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second</li><li>signature:<br>\begin{aligned}<br>s= (feature,weights)<br>\end{aligned}</li></ol><h2 id="Purity-and-distribution-distance-based-gmm"><a href="#Purity-and-distribution-distance-based-gmm" class="headerlink" title="Purity and distribution distance based gmm"></a>Purity and distribution distance based gmm</h2><h3 id="GMM-with-EM"><a href="#GMM-with-EM" class="headerlink" title="GMM with EM"></a>GMM with EM</h3><ul><li>original definition<br>\begin{aligned}<br>p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)<br>\end{aligned}</li></ul><p>\pi_k  represent the possibility of sample belong to kth category</p><ul><li>new definition for coding<br>\begin{aligned}<br>\sum_{k} z_k = 1<br>\end{aligned}</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Purity-and-distribution-distance-based-density&quot;&gt;&lt;a href=&quot;#Purity-and-distribution-distance-based-density&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="Theory" scheme="https://www.yanhong.me/categories/Theory/"/>
    
    
      <category term="pesudo labels" scheme="https://www.yanhong.me/tags/pesudo-labels/"/>
    
  </entry>
  
  <entry>
    <title>syntax</title>
    <link href="https://www.yanhong.me/2019/02/13/syntax/"/>
    <id>https://www.yanhong.me/2019/02/13/syntax/</id>
    <published>2019-02-13T13:29:29.000Z</published>
    <updated>2019-02-16T09:30:37.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="the-plan-of-year"><a href="#the-plan-of-year" class="headerlink" title="the plan of year"></a>the plan of year</h1><p><em>Italics</em><br><strong>bold</strong><br><strong><em>italics and bold</em></strong><br><del>delete</del></p><h2 id="the-plan-of-season"><a href="#the-plan-of-season" class="headerlink" title="the plan of season"></a>the plan of season</h2><blockquote><p>reference paper</p><blockquote><p>reference paragraph</p><blockquote><p>reference sentence</p></blockquote></blockquote></blockquote><hr><p>using splite line to start new content </p><h3 id="the-plan-of-month"><a href="#the-plan-of-month" class="headerlink" title="the plan of month"></a>the plan of month</h3><p>$ adding pictures from internet<br><img src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=702257389,1274025419&amp;fm=27&amp;gp=0.jpg" alt="blockchain" title="Block chain"><br>$ adding url links<br><a href="http://baidu.com" target="_blank" rel="noopener">baidu</a></p><h4 id="the-plan-of-week"><a href="#the-plan-of-week" class="headerlink" title="the plan of week"></a>the plan of week</h4><table><thead><tr><th>name</th><th style="text-align:center">skills</th><th style="text-align:right">order</th></tr></thead><tbody><tr><td>liu bei</td><td style="text-align:center">cry</td><td style="text-align:right">the first</td></tr><tr><td>guan yu</td><td style="text-align:center">pat</td><td style="text-align:right">the second</td></tr><tr><td>zhang fei</td><td style="text-align:center">scold</td><td style="text-align:right">the third</td></tr></tbody></table><h5 id="the-plan-of-day"><a href="#the-plan-of-day" class="headerlink" title="the plan of day"></a>the plan of day</h5><p>$ single code<br><code>python alternative_ep.py</code></p><p>$ code block</p><p>(<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">    function fun()&#123;</span><br><span class="line">         echo &quot;this is code block&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    fun();</span><br><span class="line">(```)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ draw flow chart</span><br><span class="line"></span><br><span class="line">```flow</span><br><span class="line">st=&gt;start: 开始</span><br><span class="line">op=&gt;operation: My Operation</span><br><span class="line">cond=&gt;condition: Yes or No?</span><br><span class="line">e=&gt;end</span><br><span class="line">st-&gt;op-&gt;cond</span><br><span class="line">cond(yes)-&gt;e</span><br><span class="line">cond(no)-&gt;op</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;the-plan-of-year&quot;&gt;&lt;a href=&quot;#the-plan-of-year&quot; class=&quot;headerlink&quot; title=&quot;the plan of year&quot;&gt;&lt;/a&gt;the plan of year&lt;/h1&gt;&lt;p&gt;&lt;em&gt;Italics&lt;/e
      
    
    </summary>
    
      <category term="Grammar" scheme="https://www.yanhong.me/categories/Grammar/"/>
    
    
      <category term="markdown" scheme="https://www.yanhong.me/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>Cross_entropy_loss</title>
    <link href="https://www.yanhong.me/2019/01/16/cross-entropy-loss/"/>
    <id>https://www.yanhong.me/2019/01/16/cross-entropy-loss/</id>
    <published>2019-01-16T02:37:13.000Z</published>
    <updated>2019-02-16T09:41:18.012Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Cross-entropy-loss-in-tensorflow"><a href="#Cross-entropy-loss-in-tensorflow" class="headerlink" title="Cross_entropy_loss in tensorflow"></a>Cross_entropy_loss in tensorflow</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def softmax_cross_entropy_with_logits(</span><br><span class="line">    _sentinel=None,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=None,</span><br><span class="line">    logits=None,</span><br><span class="line">    dim=-1,</span><br><span class="line">    name=None):</span><br><span class="line">  _ensure_xent_args(<span class="string">"softmax_cross_entropy_with_logits"</span>, _sentinel, labels,</span><br><span class="line">                    logits)</span><br><span class="line">  with ops.name_scope(name, <span class="string">"softmax_cross_entropy_with_logits_sg"</span>,</span><br><span class="line">                      [logits, labels]) as name:</span><br><span class="line">    labels = array_ops.stop_gradient(labels, name=<span class="string">"labels_stop_gradient"</span>)</span><br><span class="line">  <span class="built_in">return</span> softmax_cross_entropy_with_logits_v2(</span><br><span class="line">      labels=labels, logits=logits, dim=dim, name=name)</span><br><span class="line">def softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    _sentinel=None,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=None,</span><br><span class="line">    logits=None,</span><br><span class="line">    dim=-1,</span><br><span class="line">    name=None):</span><br><span class="line">  _ensure_xent_args(<span class="string">"softmax_cross_entropy_with_logits"</span>, _sentinel, labels,</span><br><span class="line">                    logits)</span><br><span class="line">  with ops.name_scope(name, <span class="string">"softmax_cross_entropy_with_logits"</span>,</span><br><span class="line">                      [logits, labels]) as name:</span><br><span class="line">    logits = ops.convert_to_tensor(logits, name=<span class="string">"logits"</span>)</span><br><span class="line">    labels = ops.convert_to_tensor(labels, name=<span class="string">"labels"</span>)</span><br><span class="line">    convert_to_float32 = (</span><br><span class="line">        logits.dtype == dtypes.float16 or logits.dtype == dtypes.bfloat16)</span><br><span class="line">    precise_logits = math_ops.cast(</span><br><span class="line">        logits, dtypes.float32) <span class="keyword">if</span> convert_to_float32 <span class="keyword">else</span> logits</span><br><span class="line">    <span class="comment"># labels and logits must be of the same type</span></span><br><span class="line">    labels = math_ops.cast(labels, precise_logits.dtype)</span><br><span class="line">    input_rank = array_ops.rank(precise_logits)</span><br><span class="line">    shape = logits.get_shape()</span><br><span class="line">    <span class="keyword">if</span> dim is not -1:</span><br><span class="line">      def _move_dim_to_end(tensor, dim_index, rank):</span><br><span class="line">        <span class="built_in">return</span> array_ops.transpose(</span><br><span class="line">            tensor,</span><br><span class="line">            array_ops.concat([</span><br><span class="line">                math_ops.range(dim_index),</span><br><span class="line">                math_ops.range(dim_index + 1, rank), [dim_index]</span><br><span class="line">            ], 0))</span><br><span class="line">      precise_logits = _move_dim_to_end(precise_logits, dim, input_rank)</span><br><span class="line">      labels = _move_dim_to_end(labels, dim, input_rank)</span><br><span class="line">    input_shape = array_ops.shape(precise_logits)</span><br><span class="line">    precise_logits = _flatten_outer_dims(precise_logits)</span><br><span class="line">    labels = _flatten_outer_dims(labels)</span><br><span class="line">    cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits(</span><br><span class="line">        precise_logits, labels, name=name)</span><br><span class="line">    output_shape = array_ops.slice(input_shape, [0],</span><br><span class="line">                                   [math_ops.subtract(input_rank, 1)])</span><br><span class="line">    cost = array_ops.reshape(cost, output_shape)</span><br><span class="line">    <span class="keyword">if</span> not context.executing_eagerly(</span><br><span class="line">    ) and shape is not None and shape.dims is not None:</span><br><span class="line">      shape = shape.as_list()</span><br><span class="line">      del shape[dim]</span><br><span class="line">      cost.set_shape(shape)</span><br><span class="line">    <span class="keyword">if</span> convert_to_float32:</span><br><span class="line">      <span class="built_in">return</span> math_ops.cast(cost, logits.dtype)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="Cross-entropy-loss-in-Keras-with-backbone-of-tensorflow"><a href="#Cross-entropy-loss-in-Keras-with-backbone-of-tensorflow" class="headerlink" title="Cross_entropy_loss in Keras with backbone of tensorflow"></a>Cross_entropy_loss in Keras with backbone of tensorflow</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output)</span><br></pre></td></tr></table></figure><h3 id="why-the-value-of-loss-become-inf"><a href="#why-the-value-of-loss-become-inf" class="headerlink" title="why the value of loss become inf"></a>why the value of loss become inf</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. <span class="built_in">log</span>(x) when x -&gt; 0</span><br><span class="line">2. learning rate is too high</span><br><span class="line">3. some parameters of Nel appear inf</span><br><span class="line">4. input data appear inf</span><br><span class="line">$ result: the parameters of model no longer updated</span><br></pre></td></tr></table></figure><h3 id="Cross-entropy-loss-with-missing-labels"><a href="#Cross-entropy-loss-with-missing-labels" class="headerlink" title="Cross_entropy_loss with missing labels"></a>Cross_entropy_loss with missing labels</h3><pre><code class="bash">$ leveraging fixed zero array or ones array as ground truth and generating mask <span class="keyword">in</span> loss <span class="keyword">function</span>def mask_cross_entropy_loss(y_true,y_pred):    mask=K.all(K.equal(y_true,0),axis=-1)    mask=1-K.cast(mask,K.floatx())    loss = K.categorical_crossentropy(y_true,y_pred)*mask    <span class="built_in">return</span> (K.sum(loss)/K.sum(mask)</code></pre><pre><code class="bash">$ corresponding accuracy with maskdef mask_cross_entropy_acc(y_true,y_pred):    mask=K.all(K.equal(y_true,0),axis=-1)    mask=1-K.cast(mask,K.floatx())    acc = K.cast(K.equal(K.argmax(y_true,axis=-1),K.argmax(y_pred,axis=-1)),K.floatx()))*mask    <span class="built_in">return</span> (K.sum(acc)/K.sum(mask)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Cross-entropy-loss-in-tensorflow&quot;&gt;&lt;a href=&quot;#Cross-entropy-loss-in-tensorflow&quot; class=&quot;headerlink&quot; title=&quot;Cross_entropy_loss in tensor
      
    
    </summary>
    
      <category term="Deep learning" scheme="https://www.yanhong.me/categories/Deep-learning/"/>
    
    
      <category term="tensorflow" scheme="https://www.yanhong.me/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>MMD and Density</title>
    <link href="https://www.yanhong.me/2019/01/15/GMM/"/>
    <id>https://www.yanhong.me/2019/01/15/GMM/</id>
    <published>2019-01-15T02:31:01.000Z</published>
    <updated>2019-02-16T09:26:30.171Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Relation-between-MMD-and-Density"><a href="#Relation-between-MMD-and-Density" class="headerlink" title="Relation between MMD and Density"></a>Relation between MMD and Density</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. MMD</span><br><span class="line">first-order information</span><br><span class="line"></span><br><span class="line">2. Density</span><br><span class="line">second-order information, the larger variance, the sparser, the small variance, the denser </span><br><span class="line"></span><br><span class="line">$ combination of MMD and density: my understanding is that we can obtain MMD, density, prior weights from GMM, when GMM can be calculated from ground-trurh label of</span><br></pre></td></tr></table></figure><h3 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ Gaussian Mixed Model: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.</span><br><span class="line"></span><br><span class="line">$ Applied to clusters and density estimation</span><br><span class="line"></span><br><span class="line">$ Differentce between k-means and GMM: k-means notes that each point is assigned to different clusters, <span class="keyword">while</span> GMM can calculate the probabiliy of each point belong to each clusters.</span><br></pre></td></tr></table></figure><p>$ Import parameters of GMM: K Gaussion models, also K clusters, \pi_k, \mu_k, \Sigma_k</p><p>\begin{aligned}<br>p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)<br>\end{aligned}</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Relation-between-MMD-and-Density&quot;&gt;&lt;a href=&quot;#Relation-between-MMD-and-Density&quot; class=&quot;headerlink&quot; title=&quot;Relation between MMD and Den
      
    
    </summary>
    
      <category term="Theory" scheme="https://www.yanhong.me/categories/Theory/"/>
    
    
      <category term="algorithm" scheme="https://www.yanhong.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Numpy learning</title>
    <link href="https://www.yanhong.me/2019/01/14/numpy-learning/"/>
    <id>https://www.yanhong.me/2019/01/14/numpy-learning/</id>
    <published>2019-01-14T14:25:55.000Z</published>
    <updated>2019-02-18T13:09:40.581Z</updated>
    
    <content type="html"><![CDATA[<h2 id="one-hot-labels-preprocessing"><a href="#one-hot-labels-preprocessing" class="headerlink" title="one-hot labels preprocessing"></a>one-hot labels preprocessing</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ weighting samples with confidence score</span><br><span class="line">index = np.ragmax(predicted_result,axis=1)</span><br><span class="line">arg = np_utils.to_categorical(index,classes)</span><br><span class="line">weighted_one_hot=predicted_result*arg</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ selection &amp;&amp; weighting samples with confidence score</span><br><span class="line">weighted_selected_one_hot=np.where(predicted_result&gt;k,predicted_result,0)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ extending vector into matrix</span><br><span class="line">xx = [np.full(classes,value) <span class="keyword">for</span> value <span class="keyword">in</span> x]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;one-hot-labels-preprocessing&quot;&gt;&lt;a href=&quot;#one-hot-labels-preprocessing&quot; class=&quot;headerlink&quot; title=&quot;one-hot labels preprocessing&quot;&gt;&lt;/a&gt;on
      
    
    </summary>
    
      <category term="Codes" scheme="https://www.yanhong.me/categories/Codes/"/>
    
    
      <category term="numpy" scheme="https://www.yanhong.me/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>experimental analysis</title>
    <link href="https://www.yanhong.me/2019/01/06/experimental-analysis/"/>
    <id>https://www.yanhong.me/2019/01/06/experimental-analysis/</id>
    <published>2019-01-06T09:37:55.000Z</published>
    <updated>2019-01-06T09:37:55.567Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
</feed>
