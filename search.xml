<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Why-How-Pseudo]]></title>
    <url>%2F2019%2F02%2F22%2FWhy-How-Pseudo%2F</url>
    <content type="text"><![CDATA[Pseudo labels in multi-task learning Pseudo data selection with density and distribution distance Pseudo labels$ Reason reference CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images, demonstrating that training a CNN from scratch using both clean and noisy data is better than just using the clean one, on the condition that the amount of pseudo data is limited. reference From Facial Expression Recognition to Interpersonal Relation Prediction Zhanpeng, stating explaination that pseudo data can bridge the gap between heterogeneous data. reference Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels, ‘what’s the all attributs of images’ versus ‘what’s the labeled attributes’, labeling missing attributes. The novalty may be regarded as a baseline (waiting to understand the detail). data imbalance in joint training strategy, such as 20k~ pose dataset AFLW and 90k~ emotion dataset ExpW. Pseudo data selectionBecause much noisy data can effect the model performance and even disturb the model training process, leading to poor model performance, pseudo data selection can control the ratio of pseudo data in the whole datasets. Density reason: because clusters are easily detected by the local density of data points, in the pseudo data belong to same categoty have similar feature. Appling a density based clustering algorithm that measures the complexity of psuedo data using data distribution density in. each category. implementation detail: measuring the purity of data with pseudo label based on its distribution density in a feature space, and rank the purity to generate pseudo weights, pseudo data with higher density is assigned larger weights, while smaller weights are assigned to low-density pseudo data. Distribution distance reason: ‘transfer learning is one important method in machine learning, it can relax the condition of the independent identical distribution in train dataset and test dataset, so that knowledge can be transfered from source domain to target domain’,’its main application includes domain adaptation and multi-domain tranferation’. instance-based domain adaption: calculating the distance between source domain data and target domain data, then adjusting the weights of target domain instance so that the target data can be matched with source domain data. In detail, the smaller distance, the higher similarity. implementation detail: GMM reason: implementation: generating Gaussian Mixture models (GMM) based on A1 (data with pseuodo label) and B (data with ground truth) in each categoty. Assuming GMM has K Gaussian models, we obtain G(A1_1), …, G(A1_K), G(B1_1), …, G(B1_K); EMD reason: &gt;reference Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning, which takes domain scale into account by adding a scale factor. In this paper, transfer learning can be viewed as moving a set of images from the source domain S to the target domain T. The work needed to be done by moving an image to another can be defined as their feature Euaullean distance, so the distance between two domains can be defined as the least amount of total work needed. This definition of domain similarity can be calculated by the Earth Mover’s Distance (EMD). original EMD equation\begin{aligned}EMD(P,Q)=\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min \sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}\end{aligned} \begin{aligned}P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}\end{aligned} \begin{aligned}Q={(q_{1},w_{q1}),(q_{2},w_{q2}),…,(q_{n},w_{qn})}\end{aligned} reference paper EMD application\begin{aligned}EMD(P,Q)=\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min \left{\ \sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}\ \right}\end{aligned} \begin{aligned}S={(s_{1},w_{s1}),(s_{2},w_{s2}),…,(s_{m},w_{sm})}\end{aligned} \begin{aligned}T={(t_{1},w_{t1}),(t_{2},w_{t2}),…,(t_{n},w_{tn})}\end{aligned} \begin{aligned}D=[d_{i,j}] = || g(s_{i}) − g(t_{j})||\end{aligned} \begin{aligned}g(s_{i}) = mean value of image features in category i from source domain\end{aligned} \begin{aligned}g(t_{i}) = mean value of image features in category i from target domain\end{aligned} \begin{aligned}sim(S, T) = e−γd(S,T )\end{aligned} EMD application for pseudo data selection\begin{aligned}EMD(P,Q)=\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min \left{\ \sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}\ \right}\end{aligned} \begin{aligned}P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}\end{aligned} \begin{aligned}G={(g_{1},w_{g1}),(g_{2},w_{g2}),…,(g_{n},w_{gn})}\end{aligned} \begin{aligned}D=[d_{i,j}] = || g(p_{i}) − g(g_{j})||\end{aligned} \begin{aligned}g(p) = mean value of image features in specific cluster from the same category in source domain\end{aligned} \begin{aligned}g(g_{i}) = mean value of image features in cluster i from the same category in target domain\end{aligned} \begin{aligned}sim(S, T) = e−γd(S,T )\end{aligned} implementation: calculating the distance between G(A1_i) and the whole cluster in G(B1). As for distance calculation with EMD method, we obtain P={G(A1_i)_mean,G(A1_i)_probs} and Q={[G(B1_1)_mean, …, G(B1_K)_mean],[G(B1_1)_probs, …, G(B1_K)_probs]}, so we can calculate distance vector D=(emd_1,emd_k), which is used for obtaining distribution weights weights_g.]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Density & Distribution]]></title>
    <url>%2F2019%2F02%2F21%2FDensity-Distribution%2F</url>
    <content type="text"><![CDATA[Purity based density &amp; distribution distance based GMMPurity based density reference：ECCV 2018 CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images original reference： Science 2004 Clustering by fast search and find of density peaks Creativity leveraging data distribution density in feature space to evaluate the complexity of the dataours – evaluating the purity of pseudo label data by data density in feature space noisy data can be regared as regularied method to improve the model generalizationours – pesudo labels can improve the model generalization Technical detailDensity based clustering algorithm generating features calculating Euclidean distance D_ij calculating local density of each image calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence Distribution distance based GMMClustring based GMMGMM with EM original definition\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}\pi_k represent the possibility of sample belong to kth category new definition for coding\begin{aligned}\sum_{k} z_k = 1\end{aligned} Related code sklearn.mixture.GaussianMixture Data distribution distance EMD reference： Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learningoriginal reference： The Earth Mover’s Distance as a Metric for Image Retrieval EDM definition: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second EMD data signature:\begin{aligned}s= (feature,weights)\end{aligned} related code general EMD expaination pyemd-1D data scipy.atats.wasserstein_distance]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pseudo Purity && Dataset Distribution Distance]]></title>
    <url>%2F2019%2F02%2F17%2FPseudo-purity%26distribution-distance%2F</url>
    <content type="text"><![CDATA[Purity and distribution distance based densityData density reference ECCV 2018 CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images creativity leveraging data distribution density in feature space to evaluate the complexity of the data ours – evaluating the purity of pseudo label data by data density in feature space noisy data can be regared as regularied method to improve the model generalization ours – pesudo labels can improve the model generalization technical detail density based clustering algorithm generating features calculating Euclidean distance D_ij calculating local density of each image calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence Data distribution distance EMD reference IJCV 2000 The Earth Mover’s Distance as a Metric for Image Retrieval definition EDM: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second signature:\begin{aligned}s= (feature,weights)\end{aligned} Purity and distribution distance based gmmGMM with EM original definition\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned} \pi_k represent the possibility of sample belong to kth category new definition for coding\begin{aligned}\sum_{k} z_k = 1\end{aligned}]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[syntax]]></title>
    <url>%2F2019%2F02%2F13%2Fsyntax%2F</url>
    <content type="text"><![CDATA[the plan of yearItalicsbolditalics and bolddelete the plan of season reference paper reference paragraph reference sentence using splite line to start new content the plan of month$ adding pictures from internet$ adding url linksbaidu the plan of week name skills order liu bei cry the first guan yu pat the second zhang fei scold the third the plan of day$ single codepython alternative_ep.py $ code block (1234567891011121314151617 function fun()&#123; echo &quot;this is code block&quot;; &#125; fun();(```)$ draw flow chart```flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op]]></content>
      <categories>
        <category>Grammar</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross_entropy_loss]]></title>
    <url>%2F2019%2F01%2F16%2Fcross-entropy-loss%2F</url>
    <content type="text"><![CDATA[Cross entropy loss calculation in different circumstances Cross_entropy_loss in tensorflow 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def softmax_cross_entropy_with_logits( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits_sg", [logits, labels]) as name: labels = array_ops.stop_gradient(labels, name="labels_stop_gradient") return softmax_cross_entropy_with_logits_v2( labels=labels, logits=logits, dim=dim, name=name)def softmax_cross_entropy_with_logits_v2( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits", [logits, labels]) as name: logits = ops.convert_to_tensor(logits, name="logits") labels = ops.convert_to_tensor(labels, name="labels") convert_to_float32 = ( logits.dtype == dtypes.float16 or logits.dtype == dtypes.bfloat16) precise_logits = math_ops.cast( logits, dtypes.float32) if convert_to_float32 else logits # labels and logits must be of the same type labels = math_ops.cast(labels, precise_logits.dtype) input_rank = array_ops.rank(precise_logits) shape = logits.get_shape() if dim is not -1: def _move_dim_to_end(tensor, dim_index, rank): return array_ops.transpose( tensor, array_ops.concat([ math_ops.range(dim_index), math_ops.range(dim_index + 1, rank), [dim_index] ], 0)) precise_logits = _move_dim_to_end(precise_logits, dim, input_rank) labels = _move_dim_to_end(labels, dim, input_rank) input_shape = array_ops.shape(precise_logits) precise_logits = _flatten_outer_dims(precise_logits) labels = _flatten_outer_dims(labels) cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits( precise_logits, labels, name=name) output_shape = array_ops.slice(input_shape, [0], [math_ops.subtract(input_rank, 1)]) cost = array_ops.reshape(cost, output_shape) if not context.executing_eagerly( ) and shape is not None and shape.dims is not None: shape = shape.as_list() del shape[dim] cost.set_shape(shape) if convert_to_float32: return math_ops.cast(cost, logits.dtype) else: return cost Cross_entropy_loss in Keras with backbone of tensorflow 1tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output) The reason why the value of loss become inf log(x) when x -&gt; 0 learning rate is too high some parameters of Nel appear inf input data appear inf the parameters of model no longer updated Cross_entropy_loss with missing labels leveraging fixed zero array or ones array as ground truth and generating mask in loss function def mask_cross_entropy_loss(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) loss = K.categorical_crossentropy(y_true,y_pred)*mask return (K.sum(loss)/K.sum(mask) corresponding accuracy with mask def mask_cross_entropy_acc(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) acc = K.cast(K.equal(K.argmax(y_true,axis=-1),K.argmax(y_pred,axis=-1)),K.floatx()))*mask return (K.sum(acc)/K.sum(mask)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MMD and Density]]></title>
    <url>%2F2019%2F01%2F15%2FGMM%2F</url>
    <content type="text"><![CDATA[Relation between MMD and density for pseudo data selectioncombination of MMD and density: my understanding is that we can obtain MMD, density, prior weights from GMM, when GMM can be calculated from ground-trurh label of MMD: first-order information Density: second-order information, the larger variance, the sparser, the small variance, the denser GMM Gaussian Mixed Model: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. Differentce between k-means and GMM: k-means notes that each point is assigned to different clusters, while GMM can calculate the probabiliy of each point belong to each clusters. Import parameters of GMM: K Gaussion models, also K clusters, \pi, \mu, \Sigma\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pseudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy learning]]></title>
    <url>%2F2019%2F01%2F14%2Fnumpy-learning%2F</url>
    <content type="text"><![CDATA[One-hot labels preprocessing Weighting samples with confidence score 123index = np.ragmax(predicted_result,axis=1)arg = np_utils.to_categorical(index,classes)weighted_one_hot=predicted_result*arg Selection &amp;&amp; weighting samples with confidence score 1weighted_selected_one_hot=np.where(predicted_result&gt;k,predicted_result,0) Extending vector into matrix 1xx = [np.full(classes,value) for value in x]]]></content>
      <categories>
        <category>Codes</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[experimental analysis]]></title>
    <url>%2F2019%2F01%2F06%2Fexperimental-analysis%2F</url>
    <content type="text"></content>
  </entry>
</search>
