<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Cross_entropy_loss]]></title>
    <url>%2F2019%2F01%2F16%2Fcross-entropy-loss%2F</url>
    <content type="text"><![CDATA[modification cross_entropy_lossCross_entropy_loss in tensorflow123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def softmax_cross_entropy_with_logits( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits_sg", [logits, labels]) as name: labels = array_ops.stop_gradient(labels, name="labels_stop_gradient") return softmax_cross_entropy_with_logits_v2( labels=labels, logits=logits, dim=dim, name=name)def softmax_cross_entropy_with_logits_v2( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits", [logits, labels]) as name: logits = ops.convert_to_tensor(logits, name="logits") labels = ops.convert_to_tensor(labels, name="labels") convert_to_float32 = ( logits.dtype == dtypes.float16 or logits.dtype == dtypes.bfloat16) precise_logits = math_ops.cast( logits, dtypes.float32) if convert_to_float32 else logits # labels and logits must be of the same type labels = math_ops.cast(labels, precise_logits.dtype) input_rank = array_ops.rank(precise_logits) shape = logits.get_shape() if dim is not -1: def _move_dim_to_end(tensor, dim_index, rank): return array_ops.transpose( tensor, array_ops.concat([ math_ops.range(dim_index), math_ops.range(dim_index + 1, rank), [dim_index] ], 0)) precise_logits = _move_dim_to_end(precise_logits, dim, input_rank) labels = _move_dim_to_end(labels, dim, input_rank) input_shape = array_ops.shape(precise_logits) precise_logits = _flatten_outer_dims(precise_logits) labels = _flatten_outer_dims(labels) cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits( precise_logits, labels, name=name) output_shape = array_ops.slice(input_shape, [0], [math_ops.subtract(input_rank, 1)]) cost = array_ops.reshape(cost, output_shape) if not context.executing_eagerly( ) and shape is not None and shape.dims is not None: shape = shape.as_list() del shape[dim] cost.set_shape(shape) if convert_to_float32: return math_ops.cast(cost, logits.dtype) else: return cost Cross_entropy_loss in Keras with backbone of tensorflow1tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output) why the value of loss become inf log(x) when x -&gt; 0 learning rate is too high some parameters of Nel appear inf input data appear infresult: the parameters of model no longer updated]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MMD and Density]]></title>
    <url>%2F2019%2F01%2F15%2FGMM%2F</url>
    <content type="text"><![CDATA[Relation between MMD and Density MMDfirst-order information Densitysecond-order information, the larger variance, the sparser, the small variance, the denser $ combination of MMD and density: my understanding is that we can obtain MMD, density, prior weights from GMM, when GMM can be calculated from ground-trurh label of ###GMM $ Gaussian Mixed Model: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. $ Applied to clusters and density estimation $ Differentce between k-means and GMM: k-means notes that each point is assigned to different clusters, while GMM can calculate the probabiliy of each point belong to each clusters. $ Import parameters of GMM: K Gaussion models, also K clusters, \pi_k, \mu_k, \Sigma_k ‘’‘\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}’‘’]]></content>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy learning]]></title>
    <url>%2F2019%2F01%2F14%2Fnumpy-learning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[experimental analysis]]></title>
    <url>%2F2019%2F01%2F06%2Fexperimental-analysis%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[New life, New rymthm]]></title>
    <url>%2F2019%2F01%2F06%2FNew-life-New-rymthm%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F06%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
