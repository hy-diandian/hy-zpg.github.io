<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[syntax]]></title>
    <url>%2F2019%2F02%2F13%2Fsyntax%2F</url>
    <content type="text"><![CDATA[the plan of yearItalicsbolditalics and bolddelete the plan of season reference paper reference paragraph reference sentence using splite line to start new content the plan of month$ adding pictures from internet$ adding url linksbaidu the plan of week name skills order liu bei cry the first guan yu pat the second zhang fei scold the third the plan of day$ single codepython alternative_ep.py $ code block (1234567891011121314151617 function fun()&#123; echo &quot;this is code block&quot;; &#125; fun();(```)$ draw flow chart```flowst=&gt;start: 开始op=&gt;operation: My Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op]]></content>
      <categories>
        <category>Grammar</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross_entropy_loss]]></title>
    <url>%2F2019%2F01%2F16%2Fcross-entropy-loss%2F</url>
    <content type="text"><![CDATA[Cross_entropy_loss in tensorflow123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def softmax_cross_entropy_with_logits( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits_sg", [logits, labels]) as name: labels = array_ops.stop_gradient(labels, name="labels_stop_gradient") return softmax_cross_entropy_with_logits_v2( labels=labels, logits=logits, dim=dim, name=name)def softmax_cross_entropy_with_logits_v2( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits", [logits, labels]) as name: logits = ops.convert_to_tensor(logits, name="logits") labels = ops.convert_to_tensor(labels, name="labels") convert_to_float32 = ( logits.dtype == dtypes.float16 or logits.dtype == dtypes.bfloat16) precise_logits = math_ops.cast( logits, dtypes.float32) if convert_to_float32 else logits # labels and logits must be of the same type labels = math_ops.cast(labels, precise_logits.dtype) input_rank = array_ops.rank(precise_logits) shape = logits.get_shape() if dim is not -1: def _move_dim_to_end(tensor, dim_index, rank): return array_ops.transpose( tensor, array_ops.concat([ math_ops.range(dim_index), math_ops.range(dim_index + 1, rank), [dim_index] ], 0)) precise_logits = _move_dim_to_end(precise_logits, dim, input_rank) labels = _move_dim_to_end(labels, dim, input_rank) input_shape = array_ops.shape(precise_logits) precise_logits = _flatten_outer_dims(precise_logits) labels = _flatten_outer_dims(labels) cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits( precise_logits, labels, name=name) output_shape = array_ops.slice(input_shape, [0], [math_ops.subtract(input_rank, 1)]) cost = array_ops.reshape(cost, output_shape) if not context.executing_eagerly( ) and shape is not None and shape.dims is not None: shape = shape.as_list() del shape[dim] cost.set_shape(shape) if convert_to_float32: return math_ops.cast(cost, logits.dtype) else: return cost Cross_entropy_loss in Keras with backbone of tensorflow1tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output) why the value of loss become inf123451. log(x) when x -&gt; 02. learning rate is too high3. some parameters of Nel appear inf4. input data appear inf$ result: the parameters of model no longer updated Cross_entropy_loss with missing labels$ leveraging fixed zero array or ones array as ground truth and generating mask in loss function def mask_cross_entropy_loss(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) loss = K.categorical_crossentropy(y_true,y_pred)*mask return (K.sum(loss)/K.sum(mask) $ corresponding accuracy with mask def mask_cross_entropy_acc(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) acc = K.cast(K.equal(K.argmax(y_true,axis=-1),K.argmax(y_pred,axis=-1)),K.floatx()))*mask return (K.sum(acc)/K.sum(mask)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MMD and Density]]></title>
    <url>%2F2019%2F01%2F15%2FGMM%2F</url>
    <content type="text"><![CDATA[Relation between MMD and Density12345671. MMDfirst-order information2. Densitysecond-order information, the larger variance, the sparser, the small variance, the denser $ combination of MMD and density: my understanding is that we can obtain MMD, density, prior weights from GMM, when GMM can be calculated from ground-trurh label of GMM12345$ Gaussian Mixed Model: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.$ Applied to clusters and density estimation$ Differentce between k-means and GMM: k-means notes that each point is assigned to different clusters, while GMM can calculate the probabiliy of each point belong to each clusters. $ Import parameters of GMM: K Gaussion models, also K clusters, \pi_k, \mu_k, \Sigma_k \begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy learning]]></title>
    <url>%2F2019%2F01%2F14%2Fnumpy-learning%2F</url>
    <content type="text"><![CDATA[one-hot labels preprocessing1234$ weighting samples with confidence scoreindex = np.ragmax(predicted_result,axis=1)arg = np_utils.to_categorical(index,classes)weighted_one_hot=predicted_result*arg 12$ selection &amp;&amp; weighting samples with confidence scoreweighted_selected_one_hot=np.where(predicted_result&gt;k,predicted_result,0)]]></content>
      <categories>
        <category>Codes</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[experimental analysis]]></title>
    <url>%2F2019%2F01%2F06%2Fexperimental-analysis%2F</url>
    <content type="text"></content>
  </entry>
</search>
