<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Few-shot Image generation]]></title>
    <url>%2F2019%2F04%2F04%2FFew-shot-Image-generation%2F</url>
    <content type="text"><![CDATA[Meta learningDefinition “learn to learn”, intends to design models that can learn new skills or adapt to new environment rapidly with a few traing samples, like human learning. The detail can be posted in Meta-Learning: Learning to Learn Fast Optimization aims:$$\theta^* = \arg\min_\theta \mathbb{E}{\mathcal{D}\sim p(\mathcal{D})} [\mathcal{L}\theta(\mathcal{D})]$$ where $\mathcal{D}=\langle S, B\rangle$, Support set and Batch set. Training steps sample a subset of labels $L\subset\mathcal{L}$. samples a support set $S^L \subset \mathcal{D}$ and a training batch $B^L \subset \mathcal{D}$. Both of them belong to the sampled label set $L$, $y \in L, \forall (x, y) \in S^L, B^L$. support set is the input of model. the update of model parameters is based on the loss in backpropagation calculated from the mini-batch $B^L$. each pair of sampled dataset $(S^L, B^L)$ is regarded as one data point, such that trained models can generalize to other datasets. Symbols in red are added for meta-learning in addition to the general supervised learning objective.\begin{aligned}\theta = \arg\max_\theta \color{red}{E_{L\subset\mathcal{L}}[} E_{\color{red}{S^L \subset\mathcal{D}, }B^L \subset\mathcal{D}} [\sum_{(x, y)\in B^L} P_\theta(x, y\color{red}{, S^L})] \color{red}{]}\end{aligned} Traning stages meta-learner: a optimizer $g_\phi$ learns how to update the learner model’s parameters via the support set $S$, $\theta’ = g_\phi(\theta, S)$ learner: A classifier $f_\theta$ is the “learner” model, trained for operating a given task.final learning objective is$$ \mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]] $$ Methodsmodel-based: $f_\theta(\mathbf{x}, S)$ use recurrent network with internal (or external) memory metric-based: $\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i$ learn efficient distance metric optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$ optimize the model parameter explicitly for fast learning]]></content>
      <categories>
        <category>Meta-learning</category>
      </categories>
      <tags>
        <tag>few-shot learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Manifold Learning]]></title>
    <url>%2F2019%2F03%2F18%2FManifold-Learning%2F</url>
    <content type="text"><![CDATA[Manifold Leanring for Semi-supervised Learning the model trained with labeled samples and unlabeled samples introducing manifold learning to learn the geometry of marginal distribution &gt;reference first paperManifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples explanation in Chinese(https://zhuanlan.zhihu.com/p/33006509) aim: data-depend regularization to exploit geometry of the marginal distribution Manifold learning non-linear dimensionality reduction method to obtain intrinsic feature &gt;reference PAMI Semi-supervised Learning with Manifold Learning reason: leveraging a few of training samples to train model always fail to reflect the dataset distribution, which means that the trained model only adapt to the supervised label without learning the intrinsic feature. Introducing manifold learning to combine labeled samples and unlabeled samples can exploit the geometr of marginal distribution. Semi-supervised Multi-task Learning with Manifold Learning reference [Semisupervised feature analysis by mining correlations among multiple tasks]{https://arxiv.org/pdf/1411.6232.pdf} aim: feature selection in semi-supervised MTL method: sparce coefficients learnt \begin{aligned} p(x) &amp; = \min_{w_t} \sum_(l=1}^t (loss(w_l) + \alpha ||w_l||_{1,2} + \gamma ||w||_{\*}) \end{aligned} including $l_{1,2}$ norm and Laplacian norm: \begin{aligned} \gamma ||w|| = \min_{w,b}\sum_{l=1}{t}Tr(w^Tx_lL_lx_l^Tw) \end{aligned} reference [Semi-supervised multitask learning]{http://papers.nips.cc/paper/3198-semi-supervised-multitask-learning.pdf} reference [Semi-supervised multi-task learning with task regularizations]{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.194.1854&amp;rep=rep1&amp;type=pdf} reference [Semi-supervised multitask learning for scene recognition]{https://www.researchgate.net/profile/Lichao_Mou/publication/268880603_Semi-Supervised_Multitask_Learning_for_Scene_Recognition/links/567a67f608ae7fea2e9a08f1.pdf}]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>feature regularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semi-MTL Related]]></title>
    <url>%2F2019%2F03%2F09%2FSemi-MTL-Related%2F</url>
    <content type="text"><![CDATA[MTLLearning Methods Supervised Semi-supervised Scenarios Homogeneous Heterogeneous Heterogeneous Tasks (Document &amp; Images &amp; Audio) Hterogeneous Datsets (Each dataset with a set of labels) Improvement Ideas Network Structure Feature Selection Application CV NLP ALL MTL is the combination of above mentioned.Our method is semi-supervised MTL with heterogeneous datasets, comparison including supervised or semi-supervised MTL with heterogeneous dataset. reference Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks semi-supervised, heterogeneou datasets, feature selection. scenario: CV, dataset A for tasks A[labeled and unlabeled], dataset B for tasks B[labeled and unlabeled] method: manifold learning, mining feature correlation by sparse coefficients. No reference Deep Cross Residual Learning for Multitask Visual Recognition supervised, heterogeneou datasets, network structure. scenario: CV, dataset A for tasks A[labeled], dataset B for tasks B[labeled] method: enables intuitive learning across multiple related tasks using cross-connections called cross-residuals Yes? Netowrk realization? reference A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data semi-supervised, heterogeneou datasets, network structure. scenario: method: No reference Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces supervised, heterogeneou datasets, network structure. scenario: text classification, dataset A for tasks A[labeled], dataset B for tasks B[labeled] method: combining multi-task learning and semi- supervised learning by inducing a joint embed- ding space between disparate label spaces and learning transfer functions between label embeddings. LTN can be used to label unlabelled and auxiliary task data by utilising the ‘distilling knowledge’ contained in auxiliary model predictions. not only model their relationship, but also to directly estimate the cor- responding label of the target task based on auxil- iary predictions Yes: embedding, prediction, output, temperature=1, embedding can be realized with cross-stich network reference Neural Network for Heterogeneous Annotations supervised, heterogeneou datasets, multi-view &amp; stacking setting scenario: NLP, dataset A for tasks A[labeled], dataset B for tasks B[labeled] method: multi-view, stacking, neurual network Yes: not understand]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semi-Heteroneous-Multitask-Baseline]]></title>
    <url>%2F2019%2F02%2F28%2FSemi-Heteroneous-Multitask-Baseline%2F</url>
    <content type="text"><![CDATA[high-related reference Learning without Forgetting distilled knowledge reference An All-In-One Convolutional Neural Network for Face Analysis naive joint reference From Facial Expression Recognition to Interpersonal Relation PredictionZhanpeng pseudo label reference NLP Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces‘we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions.’ probelm: jointly using unlabelled data and auxiliary, annotated datasets; domain gap; missing labels method: training the Label Transfer Network, minimise the squared error between the model predictions and the pseudo label baseline: yes (how to calculate loss funtion [squared error] based on pseudo label) reference Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels Ishan confidence score same apply scene refer to paper writting reference non-DL Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks problem: ‘Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence.’ ‘These previous works, however, independently select features for each task, which fails to consider correlations among multiple related tasks.’ ‘Despite of their good performances, these classical algorithms are all implemented only with labeled training data.’ method: ‘ignoring the correlations among different features -&gt;apply the sparse coefficients to the feature vectors -&gt; proposing multiple feature selection’ baseline: undetermined (hard to complete) Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing—and Back related work conclude papers of joint training of models for multiple tasks. ‘how learned structure is shared across tasks’: supervise different tasks at different depths of the shared structure duplicate the shared structure into columns and define mechanisms for sharing information across columns Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition , using intermediate representations as auxiliary supervision for low-level task recognition to improve final task performance. baseline: no( pose-emotion both are high-level task, not similar to keypoint detection) pseudo method related reference CVPR 2018Pseudo Mask Augmented Object Detection ‘proposing an effective learning approach that progressively improves the quality pseudo from a coarse initialization,the detection network parameters Θ and pseudo masks Mpseudo are alternatively optimized following a EM-like way’ probelm: object detection without mask annotation method: pseudo mask (alternatively,initialization,pseudo mask with refinement algorithm[graphical model, because the pixel charaterization]) baseline: yes(uing graphical model with learned information as input to refine pseudo label) reference Learning from Noisy Labels with Distillation interpolation between noisy label and distilled knowledge baseline: yes reference Clustered Multi-Task Learning Via Alternating Structure Optimization ‘ASO which aims to identify a shared low-dimensional predictive structure for all tasks’,’based on the standard assump- tion that each task can learn equally well from any other task’,’the clustering view of ASO has not been explored before’ hard to understand, pure theories.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Research-Point-Multitask]]></title>
    <url>%2F2019%2F02%2F28%2FResearch-Point-Multitask%2F</url>
    <content type="text"><![CDATA[Several tips for heterogeneous multitask learning core problem - heterogeneity structrues in tasks[age and gender], source[documents and images], feature[generated from model for classification or regression tasks], samples[one labeled with age while another annotated with gender] network with suitabel initialization and learning rate construct feature relationship in network, offering trainable parameters, refer to [cross-stitch network] feature selection, sparsity and factorization, which means that the specific branch layers to ontain task-specific fearures in a common feature space. Research directions feature relationship from the level of network. feature processing from the level fo feature space, such as selection, sparsity and facorization.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Critical-Points-in-Multitask]]></title>
    <url>%2F2019%2F02%2F28%2FCritical-Points-in-Multitask-0%2F</url>
    <content type="text"><![CDATA[Several tips for heterogeneous multitask learning core problem - heterogeneity structrues in tasks[age and gender], source[documents and images], feature[generated from model for classification or regression tasks], samples[one labeled with age while another annotated with gender] network with suitabel initialization and learning rate construct feature relationship in network, offering trainable parameters, refer to [cross-stitch network] feature selection, sparsity and factorization, which means that the specific branch layers to ontain task-specific fearures in a common feature space. Research directions feature relationship from the level of network. feature processing from the level fo feature space, such as selection, sparsity and facorization.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parameters-Selection-for-Pseudo]]></title>
    <url>%2F2019%2F02%2F27%2FParameters-Selection-for-Pseudo%2F</url>
    <content type="text"><![CDATA[temperature in distilled knowledge reference-1 Distilling the Knowledge in a Neural Network aim: distilling the knowledge in an ensemble of models into a single modeltemperature: the higher the temperature T, the softer the probability distribution over classes. ‘For the distillation we tried temperatures of [1, 2, 5, 10] and used a relative weight of 0.5 on the cross-entropy for the hard targets, where bold font indicates the best value’ reference-2 from reference-1 Learning without Forgetting aim: useing only new task data to train the network while preserving the original capabilitiestemperature: T=2, grid search method density threshold in local feature densitydistribution distance is transfered into weights]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross-stitch-network]]></title>
    <url>%2F2019%2F02%2F27%2FCross-stitch-network%2F</url>
    <content type="text"><![CDATA[cross-stitch-network reference CVPR 2016Cross-stitch Networks for Multi-task Learning tensorflow-code problem: existing multi-task approaches rely on enumerating multiple net- work architectures specific to the tasks at hand, that do not generalize. method: proposing a new sharing unit: “cross-stitch” unit. These units combine the activations from multiple networks. self-defined cross-stitch layer in keras with tensorflow as backbone12345678910111213141516171819202122232425262728class Cross_stitch(Layer): # basic parameter setting def __init__(self,input_shape_1,input_shape_2, **kwargs): super(Cross_stitch, self).__init__(**kwargs) self.input_shape_1 = input_shape_1 self.input_shape_2 = input_shape_2 # apply trainable parameters in network, similar to convolutional layer # shape is important, you must to calculate specific size based on the shape of input and output # in cross-stitch network: [xa,xb]*[papameter]=[xa',xb'], the detail refer to the paper def build(self, input_shape): shape = self.input_shape_1 + self.input_shape_2 self.cross_stitch = self.add_weight( shape=(shape,shape), initializer=tf.initializers.identity(), name='cross_stitch') self.built = True # conduct implement of the detailed algorithm calculation # inputs represent the output of upper layer, such as x=Dense(parameter)(inputs) def call(self,inputs): inputss = tf.concat((inputs[0], inputs[1]), axis=1) output = tf.matmul(inputss, self.cross_stitch) output1 = tf.reshape(output[:,:self.input_shape_1],shape=[-1,self.input_shape_1]) output2 = tf.reshape(output[:,self.input_shape_2:],shape=[-1,self.input_shape_2]) return [output1, output2] def get_config(self): config = &#123;'input_shape_1': self.input_shape_1,'input_shape_2': self.input_shape_2&#125; base_config = super(Cross_stitch, self).get_config() return dict(list(base_config.items()) + list(config.items()))]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Heterogeneous Multitask]]></title>
    <url>%2F2019%2F02%2F26%2FHeterogeneous-Multitask%2F</url>
    <content type="text"><![CDATA[Large-scale Heterogeneous Learning in Big Data Analytics reference slide Large-scale Heterogeneous Learning in Big Data Analytics multi-task learning‘Multitask learning is an inductive transfer mechanism for improving generalization performance [Caruana, Machine Learning’97]’ multi-label learning: multi-view learning: Heterogeneous outputs related to each other with the same set of inputs reference-1 Heterogeneous Multitask Learning with Joint Sparsity Constraints probelm: dealing with homogeneous tasks, such as purely regression (continue) or classification (discrete) task, from a common set of high-dimensional feature space. method: modeling the joint sparsity as L1/L∞ or L1/L2 norm of the model parameters, achieving joint sparse feature selection. datasets: discovering genetic markers that influence multiple correlated traits jointly, datasets with different clinical traits including continuous and discrete labels. Heterogeneous inputs from multiple domains reference-2 Heterogeneous Multitask Metric LearningAcross Multiple Domains Exploiting High-Order Information in Heterogeneous Multi-Task Feature Learning ‘Multitask metric learning (MTML), which can be regarded as a special case of transfer metric learning (TML) by performing transfer across all related domains.’ ‘Heterogeneous transfer learning approaches can be adopted to remedy this drawback by deriving a metric from the learned transformation across different domains’ probelm: inputs from heterogeneous domain can be solved by deriving a metric from the learned transformation from two domains, but pratical aims is to deal with multiple domain by learning the metric from all domains. method: metrics -&gt; transformation -&gt; subspace -&gt; maximize high-order ovariance among the predictive structures of these domains, because high-order statistics (correlation information), which can only be exploited by simultaneously examining all domains, thus obtaining more reliable feature transformations and metrics. datasets: Document Categorization, Scene Classification and Image Annotation Heterogeneous features for different task reference-3 Semantic Feature Learning for HeterogeneousMultitask Classification via Non-NegativeMatrix Factorization problem: different tasks have heterogenous feature in real world. method: leveraging a non-negative matrix factorization-based multitask method (MTNMF) to learn a common semantic feature space underlying different heterogeneous feature spaces of each task. it is similar to reference-1, feature selection vs feature factorization. datasets: 20Newsgroups&amp;ImageNet (image and document), Email Spam Detection (15 persons), Sentiment Classification (books; dvd; electronics; kitchen)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Why-How-Pseudo]]></title>
    <url>%2F2019%2F02%2F22%2FWhy-How-Pseudo%2F</url>
    <content type="text"><![CDATA[Pseudo labels in multi-task learning Pseudo data selection with density and distribution distance Pseudo labelsThe reason why need to apply pseudo data into model training reference CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images, demonstrating that training a CNN from scratch using both clean and noisy data is better than just using the clean one, on the condition that the amount of pseudo data is limited. reference From Facial Expression Recognition to Interpersonal Relation Prediction Zhanpeng, stating explaination that pseudo data can bridge the gap between heterogeneous data. reference Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels, ‘what’s the all attributs of images’ versus ‘what’s the labeled attributes’, labeling missing attributes. The novalty may be regarded as a baseline (waiting to understand the detail). data imbalance in joint training strategy, such as 20k~ pose dataset AFLW and 90k~ emotion dataset ExpW. Pseudo data selectionBecause much noisy data can effect the model performance and even disturb the model training process, leading to poor model performance, pseudo data selection can control the ratio of pseudo data in the whole datasets. Density reason: because clusters are easily detected by the local density of data points, in the pseudo data belong to same categoty have similar feature. Appling a density based clustering algorithm that measures the complexity of psuedo data using data distribution density in. each category. implementation detail: measuring the purity of data with pseudo label based on its distribution density in a feature space, and rank the purity to generate pseudo weights, pseudo data with higher density is assigned larger weights, while smaller weights are assigned to low-density pseudo data. Distribution distance reason: ‘transfer learning is one important method in machine learning, it can relax the condition of the independent identical distribution in train dataset and test dataset, so that knowledge can be transfered from source domain to target domain’,’its main application includes domain adaptation and multi-domain tranferation’. instance-based domain adaption: calculating the distance between source domain data and target domain data, then adjusting the weights of target domain instance so that the target data can be matched with source domain data. In detail, the smaller distance, the higher similarity. implementation detail: GMM reason: implementation: generating Gaussian Mixture models (GMM) based on A1 (data with pseuodo label) and B (data with ground truth) in each categoty. Assuming GMM has K Gaussian models, we obtain G(A1_1), …, G(A1_K), G(B1_1), …, G(B1_K); EMD reason: &gt;reference Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning, which takes domain scale into account by adding a scale factor. In this paper, transfer learning can be viewed as moving a set of images from the source domain S to the target domain T. The work needed to be done by moving an image to another can be defined as their feature Euaullean distance, so the distance between two domains can be defined as the least amount of total work needed. This definition of domain similarity can be calculated by the Earth Mover’s Distance (EMD). original EMD equation \begin{aligned}\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}\end{aligned} \begin{aligned}EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}\end{aligned} \begin{aligned}P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}\end{aligned} \begin{aligned}Q={(q_{1},w_{q1}),(q_{2},w_{q2}),…,(q_{n},w_{qn})}\end{aligned} reference paper EMD applicationg(s_{i}) from the mean value of image features in category i from source domaing(t_{i}) from the mean value of image features in category i from target domain\begin{aligned}\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}\end{aligned} \begin{aligned}EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}\end{aligned} \begin{aligned}S={(s_{1},w_{s1}),(s_{2},w_{s2}),…,(s_{m},w_{sm})}\end{aligned} \begin{aligned}T={(t_{1},w_{t1}),(t_{2},w_{t2}),…,(t_{n},w_{tn})}\end{aligned} \begin{aligned}D=[d_{i,j}] = || g(s_{i}) − g(t_{j})||\end{aligned} \begin{aligned}sim(S, T) = e−γd(S,T )\end{aligned} EMD application for pseudo data selection g(p) = mean value of image features in specific cluster from the same category in source domain g(g_{i}) = mean value of image features in cluster i from the same category in target domain\begin{aligned}\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}\end{aligned} \begin{aligned}EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}\end{aligned} \begin{aligned}P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}\end{aligned} \begin{aligned}G={(g_{1},w_{g1}),(g_{2},w_{g2}),…,(g_{n},w_{gn})}\end{aligned} \begin{aligned}D=[d_{i,j}] = || g(p_{i}) − g(g_{j})||\end{aligned} \begin{aligned}sim(S, T) = e−γd(S,T )\end{aligned} implementation: calculating the distance between G(A1_i) and the whole cluster in G(B1). As for distance calculation with EMD method, we obtain P={G(A1_i)_mean,G(A1_i)_probs} and Q={[G(B1_1)_mean, …, G(B1_K)_mean],[G(B1_1)_probs, …, G(B1_K)_probs]}, so we can calculate distance vector D=(emd_1,emd_k), which is used for obtaining distribution weights weights_g.]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Density & Distribution]]></title>
    <url>%2F2019%2F02%2F21%2FDensity-Distribution%2F</url>
    <content type="text"><![CDATA[Purity based density &amp; distribution distance based GMMPurity based density reference：ECCV 2018 CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images original reference： Science 2004 Clustering by fast search and find of density peaks Creativity leveraging data distribution density in feature space to evaluate the complexity of the dataours – evaluating the purity of pseudo label data by data density in feature space noisy data can be regared as regularied method to improve the model generalizationours – pesudo labels can improve the model generalization Technical detailDensity based clustering algorithm generating features calculating Euclidean distance D_ij calculating local density of each image calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence Distribution distance based GMMClustring based GMMGMM with EM original definition\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}\pi_k represent the possibility of sample belong to kth category new definition for coding\begin{aligned}\sum_{k} z_k = 1\end{aligned} Related code sklearn.mixture.GaussianMixture Data distribution distance EMD reference： Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learningoriginal reference： The Earth Mover’s Distance as a Metric for Image Retrieval EDM definition: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second EMD data signature:\begin{aligned}s= (feature,weights)\end{aligned} related code general EMD expaination pyemd-1D data scipy.atats.wasserstein_distance]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pseudo Purity && Dataset Distribution Distance]]></title>
    <url>%2F2019%2F02%2F17%2FPseudo-purity%26distribution-distance%2F</url>
    <content type="text"><![CDATA[Purity and distribution distance based densityData density reference ECCV 2018 CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images creativity leveraging data distribution density in feature space to evaluate the complexity of the data ours – evaluating the purity of pseudo label data by data density in feature space noisy data can be regared as regularied method to improve the model generalization ours – pesudo labels can improve the model generalization technical detail density based clustering algorithm generating features calculating Euclidean distance D_ij calculating local density of each image calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence Data distribution distance EMD reference IJCV 2000 The Earth Mover’s Distance as a Metric for Image Retrieval definition EDM: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second signature:\begin{aligned}s= (feature,weights)\end{aligned} Purity and distribution distance based gmmGMM with EM original definition\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned} \pi_k represent the possibility of sample belong to kth category new definition for coding\begin{aligned}\sum_{k} z_k = 1\end{aligned}]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross_entropy_loss]]></title>
    <url>%2F2019%2F01%2F16%2FCross_entropy_loss%2F</url>
    <content type="text"><![CDATA[Cross entropy loss calculation in different circumstances Cross_entropy_loss in tensorflow 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def softmax_cross_entropy_with_logits( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits_sg", [logits, labels]) as name: labels = array_ops.stop_gradient(labels, name="labels_stop_gradient") return softmax_cross_entropy_with_logits_v2( labels=labels, logits=logits, dim=dim, name=name)def softmax_cross_entropy_with_logits_v2( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits", [logits, labels]) as name: logits = ops.convert_to_tensor(logits, name="logits") labels = ops.convert_to_tensor(labels, name="labels") convert_to_float32 = ( logits.dtype == dtypes.float16 or logits.dtype == dtypes.bfloat16) precise_logits = math_ops.cast( logits, dtypes.float32) if convert_to_float32 else logits # labels and logits must be of the same type labels = math_ops.cast(labels, precise_logits.dtype) input_rank = array_ops.rank(precise_logits) shape = logits.get_shape() if dim is not -1: def _move_dim_to_end(tensor, dim_index, rank): return array_ops.transpose( tensor, array_ops.concat([ math_ops.range(dim_index), math_ops.range(dim_index + 1, rank), [dim_index] ], 0)) precise_logits = _move_dim_to_end(precise_logits, dim, input_rank) labels = _move_dim_to_end(labels, dim, input_rank) input_shape = array_ops.shape(precise_logits) precise_logits = _flatten_outer_dims(precise_logits) labels = _flatten_outer_dims(labels) cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits( precise_logits, labels, name=name) output_shape = array_ops.slice(input_shape, [0], [math_ops.subtract(input_rank, 1)]) cost = array_ops.reshape(cost, output_shape) if not context.executing_eagerly( ) and shape is not None and shape.dims is not None: shape = shape.as_list() del shape[dim] cost.set_shape(shape) if convert_to_float32: return math_ops.cast(cost, logits.dtype) else: return cost Cross_entropy_loss in Keras with backbone of tensorflow 1tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output) The reason why the value of loss become inf log(x) when x -&gt; 0 learning rate is too high some parameters of Nel appear inf input data appear inf the parameters of model no longer updated Cross_entropy_loss with missing labels leveraging fixed zero array or ones array as ground truth and generating mask in loss function def mask_cross_entropy_loss(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) loss = K.categorical_crossentropy(y_true,y_pred)*mask return (K.sum(loss)/K.sum(mask) corresponding accuracy with mask def mask_cross_entropy_acc(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) acc = K.cast(K.equal(K.argmax(y_true,axis=-1),K.argmax(y_pred,axis=-1)),K.floatx()))*mask return (K.sum(acc)/K.sum(mask)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MMD and Density]]></title>
    <url>%2F2019%2F01%2F15%2FGMM%2F</url>
    <content type="text"><![CDATA[Relation between MMD and density for pseudo data selectioncombination of MMD and density: my understanding is that we can obtain MMD, density, prior weights from GMM, when GMM can be calculated from ground-trurh label of MMD: first-order information Density: second-order information, the larger variance, the sparser, the small variance, the denser GMM Gaussian Mixed Model: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. Differentce between k-means and GMM: k-means notes that each point is assigned to different clusters, while GMM can calculate the probabiliy of each point belong to each clusters. Import parameters of GMM: K Gaussion models, also K clusters, \pi, \mu, \Sigma\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pseudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy learning]]></title>
    <url>%2F2019%2F01%2F14%2FNumpy%20learning%2F</url>
    <content type="text"><![CDATA[One-hot labels preprocessing Weighting samples with confidence score 123index = np.ragmax(predicted_result,axis=1)arg = np_utils.to_categorical(index,classes)weighted_one_hot=predicted_result*arg Selection &amp;&amp; weighting samples with confidence score 1weighted_selected_one_hot=np.where(predicted_result&gt;k,predicted_result,0) Extending vector into matrix 1xx = [np.full(classes,value) for value in x]]]></content>
      <categories>
        <category>Codes</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy learning]]></title>
    <url>%2F2019%2F01%2F14%2Fnumpy-learning%2F</url>
    <content type="text"><![CDATA[One-hot labels preprocessing Weighting samples with confidence score 123index = np.ragmax(predicted_result,axis=1)arg = np_utils.to_categorical(index,classes)weighted_one_hot=predicted_result*arg Selection &amp;&amp; weighting samples with confidence score 1weighted_selected_one_hot=np.where(predicted_result&gt;k,predicted_result,0) Extending vector into matrix 1xx = [np.full(classes,value) for value in x]]]></content>
      <categories>
        <category>Codes</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
</search>
