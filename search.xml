<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Few-shot Image Generation Methods and Applications</title>
      <link href="/2021/04/10/few-shot-image-generation-methods/"/>
      <url>/2021/04/10/few-shot-image-generation-methods/</url>
      
        <content type="html"><![CDATA[<h2 id="Few-shot-Image-Generation"><a href="#Few-shot-Image-Generation" class="headerlink" title="Few-shot Image Generation"></a>Few-shot Image Generation</h2><h3 id="Few-shot-Image-Generation-with-Elastic-Weight-Consolidation"><a href="#Few-shot-Image-Generation-with-Elastic-Weight-Consolidation" class="headerlink" title="Few-shot Image Generation with Elastic Weight Consolidation"></a>Few-shot Image Generation with Elastic Weight Consolidation</h3><ul><li><p><a href="https://proceedings.neurips.cc/paper/2020/file/b6d767d2f8ed5d21a44b0e5886680cb9-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p></li><li><p>core idea: preserving the diversity of source domain, adapting the information of source domain to the target domain with a few sampels from target domain with adapting the pretrained model’s weights.</p></li><li><p>storyline:</p><p>** 70,000 images for just a specific domain (aligned faces) [16] or 1.3M images across different classes -&gt; in the artistic domain, it is at best cumbersome, and at times prohibitive, to hire artists to make thousands of creations -&gt; our goal is to generalize from a few, new examples.  <font color="blue">our setting need much data belonging to same domain such as flowers, animal faces, then using a few samples from different category like funit regarding category as domain</font>.</p><p>** training an algorithm to generate more data of a target domain, given only a few examples. An underlying assumption with this setup is that the source and target domains share some latent factors, with some differences related to their distinct difference in appearance. <font color="blue">finding shared some latent code, how near or how far</font>.</p><p>** propose a straightforward and effective adaptation technique, adapting the pretrained model’s weights, without introducing additional parameters. Fixing the architecture implies that tedious manual designs on new parameters are not necessary.</p><p>** A key property to note is that weights have different levels of importance; thus, each parameter should not be treated equally in the adaptation, or tuning process.  we consider there will always be an inherent trade-off between preserving information from the source and adapting to the target domain. <font color="blue">the number of target examples and the dissimilarity between the source and target domain</font>.</p></li><li><p>method:<br>** we do the adaptation with a few examples, some weights in the last layer are more important and should be better preserved than those in other layers.</p><p>** we could directly use F as an importance measure for weights and add a regularization loss to penalize the weight change.</p></li><li><p>setting: big domain without considering the category of source domain and target domain. real faces vs. emoji faces.</p></li><li><p>experiment:</p><p><em>* datasets: real faces (200k) vs. emoji faces (80k), animal faces (10 </em> 5000) vs. portrait paintings (16 * 10). landscape photos (5000) vs. pencil landscape drawings(10). <font color="blue">data size 10-80k</font></p><p>** evaluation metrics: FID(user study) for more taget images (less target images) and LPIPS.</p><p>** visualization.</p></li></ul><h3 id="Few-shot-adaptation-of-generative-adversarial-networks"><a href="#Few-shot-adaptation-of-generative-adversarial-networks" class="headerlink" title="Few-shot adaptation of generative adversarial networks"></a>Few-shot adaptation of generative adversarial networks</h3><ul><li><a href="https://openreview.net/pdf?id=6R51jA4fOB" target="_blank" rel="noopener">ICLR 2021 paper</a></li><li><p>core idea: proposesing a simple and effective method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than 100 images) by repurposing component analysis techniques and learns to adapt the singular values of the pre-trained weights while freezing the corresponding singular vectors.</p></li><li><p>storyline:</p><p>**  Training these models, however, typically requires large, diverse datasets in a target visual domain -&gt; low-data regime (e.g., less than 1,000 samples), GANs frequently suffer from memorization or instability, leading to a lack of diversity or poor visual quality -&gt; transfer learning, images generated by GAN-based methods are blurry and lack details, flow-based models require compute- and memory-intensive architectures.</p><p>** we restrict the space of trainable parameters to a small number of highly-expressive parameters that modulate orthogonal features of the pre-trained weight space.</p><p>** Our method first applies singular value decomposition (SVD) to the network weights of a pretrained GAN (generator + discriminator). We then adapts the singular values using GAN optimization on the target few-shot domain, with fixed left/right singular vectors. We show that varying singular values in the weight space corresponds to semantically meaningful changes of the synthesized image while preserving natural structure. proposed method achieves higher image quality after adaptation. </p></li><li><p>setting: big domain without considering the category of source domain and target domain. real faces vs. emoji faces.</p></li><li><p>experiments:</p><p>** datasets: LSUN Churches→ Van Gogh paintings(30), FFHQ→Art portraits(5-100), FFHQ→Anime Rem ID(25).  <font color="blue">data size 5-100</font></p><p>** metrics: FID, sharpness, and face quality index (FQI).</p></li></ul><h3 id="Augmentation-Interpolative-AutoEncoders-for-Unsupervised-Few-Shot-Image-Generation"><a href="#Augmentation-Interpolative-AutoEncoders-for-Unsupervised-Few-Shot-Image-Generation" class="headerlink" title="Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation"></a>Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image Generation</h3><ul><li><a href="https://arxiv.org/pdf/2011.13026.pdf" target="_blank" rel="noopener">paper</a></li><li><p>core idea: autoencoders generalize extremely well to new domains, even when trained on highly constrained data.</p></li><li><p>storyline: </p><p>** generating high-quality, diverse, high-resolution images requires a large dataset -&gt; need generative models that can train on one set of image classes, and then generalize to a new class using only a small quantity of new images: few-shot image generation.</p><p>** previous methods need large labeled datasets of hundreds of classes, substantial computation at test time, highly domain-specific, generalizing only across very similar classes.</p><p>** We leverage the finding that although the latent spaces of powerful generative models, such as VAEs and GANs, do not generalize to new classes, the representations learned by autoencoders (AEs) generalize extremely well -&gt; Interpolative AutoEncoders -&gt; Augmentation-Interpolative Autoencoder (AugIntAE) achieves simple, robust, highly general, and completely unsupervised few-shot image generation.</p></li><li><p>setting: given  a large, unlabelled collection of images depicting objects from a set of seen classes and  a very small set of images - as few as two - belonging to a novel class, Our goal is to train a network on seen classes and generates images clearly belonging to the novel class. interpolating a pair of unseen images to generate new images belonging to the novel class.</p></li><li><p>experiments:</p><p>** datasets:MNIST → EMNIST , Omniglot (train → test), CelebA (male → female) , CIFAR-10 → CIFAR-100</p><p>** metrics: FID score and train/test classification error.</p></li></ul><h3 id="MineGAN-effective-knowledge-transfer-from-GANs-to-target-domains-with-few-images"><a href="#MineGAN-effective-knowledge-transfer-from-GANs-to-target-domains-with-few-images" class="headerlink" title="MineGAN: effective knowledge transfer from GANs to target domains with few images"></a>MineGAN: effective knowledge transfer from GANs to target domains with few images</h3><ul><li><p><a href="https://arxiv.org/pdf/1912.05270.pdf" target="_blank" rel="noopener">CVPR 2020 paper</a></p></li><li><p>core idea: using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain.</p></li></ul><ul><li>storyline: high-quality pretrained models -&gt; combining high-quality pretrained models with other models and adjust them to a target distribution is a desirable objective -&gt; knowledge transfer for generative models has received significantly less attention, possibly due to its great difficulty, especially when transferring to target domains with few images -&gt; previous works severely limits the flexibility of the knowledge transfer, mode collapse -&gt; a miner network that transforms a multivariate normal distribution into a distribution on the input space of the pretrained GAN in such a way that the generated images resemble those of the target domain.</li></ul><ul><li><p>setting: big domain without considering the category of source domain and target domain. human faces vs. children faces.</p></li><li><p>experiment:</p><p>** dataset: CelebA→FFHQ children, <font color="blue">data size 100-1000</font> </p><p>** evaluation: MV, KMMD, FID</p></li></ul><h3 id="CharacterGAN-Few-Shot-Keypoint-Character-Animation-and-Reposing"><a href="#CharacterGAN-Few-Shot-Keypoint-Character-Animation-and-Reposing" class="headerlink" title="CharacterGAN: Few-Shot Keypoint Character Animation and Reposing"></a>CharacterGAN: Few-Shot Keypoint Character Animation and Reposing</h3><ul><li><a href="https://arxiv.org/pdf/2102.03141.pdf" target="_blank" rel="noopener">paper</a></li></ul><h3 id="GAN-Memory-with-No-Forgetting"><a href="#GAN-Memory-with-No-Forgetting" class="headerlink" title="GAN Memory with No Forgetting"></a>GAN Memory with No Forgetting</h3><ul><li><a href="https://papers.nips.cc/paper/2020/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></li></ul><h3 id="On-Leveraging-Pretrained-GANs-for-Generation-with-Limited-Data"><a href="#On-Leveraging-Pretrained-GANs-for-Generation-with-Limited-Data" class="headerlink" title="On Leveraging Pretrained GANs for Generation with Limited Data"></a>On Leveraging Pretrained GANs for Generation with Limited Data</h3><ul><li><a href="http://proceedings.mlr.press/v119/zhao20a/zhao20a.pdf" target="_blank" rel="noopener">ICML 2020 paper</a></li></ul><h3 id="Differentiable-Augmentation-for-Data-Efficient-GAN-Training"><a href="#Differentiable-Augmentation-for-Data-Efficient-GAN-Training" class="headerlink" title="Differentiable Augmentation for Data-Efficient GAN Training"></a>Differentiable Augmentation for Data-Efficient GAN Training</h3><ul><li><p><a href="https://proceedings.neurips.cc//paper/2020/file/55479c55ebd1efd3ff125f1337100388-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p></li><li><p>method:  improving the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples.</p></li><li><p>storyline:</p><p>** The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set.</p><p>** it is of critical importance to eliminate the need of immense datasets for GAN training.</p><p>** suggesting that the discriminator is simply memorizing the entire training set. This severe over-fitting problem disrupts the training dynamics and leads to degraded image quality.</p><p>** augmentations (cropping, flipping, scaling, color jittering, and region masking) in real images, the generator would be encouraged to match the distribution of the augmented images.</p><p>** augmentation need to be done in real images and generate images, which breaks the subtle balance between the generator and discriminator, leading to poor convergence as they are optimizing completely different objectives.</p><p>** DiffAugment, which applies the same differentiable augmentation to both real and fake images for both generator and discriminator training. It enables the gradients to be propagated through the augmentation back to the generator, regularizes the discriminator without manipulating the target distribution, and maintains the balance of training dynamics.</p></li><li><p>setting: training set  <font color="blue">data size 25% ~ 100% </font> </p></li></ul><h3 id="Training-Generative-Adversarial-Networks-with-Limited-Data"><a href="#Training-Generative-Adversarial-Networks-with-Limited-Data" class="headerlink" title="Training Generative Adversarial Networks with Limited Data"></a>Training Generative Adversarial Networks with Limited Data</h3><ul><li><p><a href="https://papers.nips.cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf" target="_blank" rel="noopener">NIPS 2020 paper</a></p></li><li><p>methods: augmentation strtegies in discriminator.</p></li><li><p>setting: training set <font color="blue">data size 1k ~ 200k </font> </p></li></ul><h3 id="Image-Generation-From-Small-Datasets-via-Batch-Statistics-Adaptation"><a href="#Image-Generation-From-Small-Datasets-via-Batch-Statistics-Adaptation" class="headerlink" title="Image Generation From Small Datasets via Batch Statistics Adaptation"></a>Image Generation From Small Datasets via Batch Statistics Adaptation</h3><ul><li><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">2019 ICCV paper</a></p></li><li><p>core idea: Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small datase.</p></li><li><p>method: we propose a novel method focusing on the pa- rameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small (<font color="blue">data size 25 ~ 500 </font> ).</p></li><li><p>setting: selecting pretrained model SNGAN and BigGAN. All experiments in this paper are not class conditional, but our method can be easily extended to be class-conditional by learning BatchNorm statistics for each class independently.</p></li><li><p>experiment:</p><p>** dataset: FFHQ dataset -&gt; anime face datase, imagenet -&gt; oxford flowers. </p><p>** metrics: FID, KMMD</p></li></ul><h2 id="Inspiration"><a href="#Inspiration" class="headerlink" title="Inspiration"></a>Inspiration</h2><ul><li><font color="red"> few-shot image generation - source domain: category information, conditional GAN -> targte domain: category information, presevring diversity information and distinctive feature among different categories. Given a few samples from different category of target domain </font> </li></ul><font color="red">  </font> <h2 id="Few-shot-Font-Generation"><a href="#Few-shot-Font-Generation" class="headerlink" title="Few-shot Font Generation"></a>Few-shot Font Generation</h2><h3 id="GANwriting-Content-Conditioned-Generation-of-Styled-Handwritten-Word-Images"><a href="#GANwriting-Content-Conditioned-Generation-of-Styled-Handwritten-Word-Images" class="headerlink" title="GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images"></a>GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images</h3><ul><li><p><a href="https://arxiv.org/pdf/2003.02567.pdf" target="_blank" rel="noopener">2020 ECCV paper</a></p></li><li><p>storyline:</p><p>** the classes had to be predefined beforehand during the training stage in conditional gan,  it was impossible to produce images from other unseen classes during inference.</p><p>** sequence -&gt; generate raw images. the produced results by previous works are not realistic, still exhibiting a poor quality, sometimes producing barely legible word images.</p><p>** we present a non-recurrent generative architecture conditioned to textual content sequences, that is specially tailored to produce realistic handwritten word images, indis- tinguishable to humans. our approach1 is able to artificially render realistic handwritten word images that match a certain textual content and that mimic some style features (text skew, slant, roundness, stroke width, ligatures, etc.) from an exemplar writer.</p></li></ul><h3 id="Few-Shot-Text-Style-Transfer-via-Deep-Feature-Similarity"><a href="#Few-Shot-Text-Style-Transfer-via-Deep-Feature-Similarity" class="headerlink" title="Few-Shot Text Style Transfer via Deep Feature Similarity"></a>Few-Shot Text Style Transfer via Deep Feature Similarity</h3><ul><li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9098082" target="_blank" rel="noopener">TIP 2020 paper</a></li></ul><h3 id="JointFontGAN-Joint-Geometry-Content-GAN-for-Font-Generation-via-Few-Shot-Learning"><a href="#JointFontGAN-Joint-Geometry-Content-GAN-for-Font-Generation-via-Few-Shot-Learning" class="headerlink" title="JointFontGAN: Joint Geometry-Content GAN for Font Generation via Few-Shot Learning"></a>JointFontGAN: Joint Geometry-Content GAN for Font Generation via Few-Shot Learning</h3><ul><li><a href="https://par.nsf.gov/servlets/purl/10199594" target="_blank" rel="noopener">ACM MM 2020</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Meta-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shadow Generation</title>
      <link href="/2020/11/21/Shadow-Generation/"/>
      <url>/2020/11/21/Shadow-Generation/</url>
      
        <content type="html"><![CDATA[<h3 id="Shadow-Detection"><a href="#Shadow-Detection" class="headerlink" title="Shadow Detection"></a>Shadow Detection</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><ul><li>SBU</li><li>UCF</li><li>CUHK-Shadow <a href="https://arxiv.org/pdf/1911.06998.pdf" target="_blank" rel="noopener">Revisiting Shadow Detection: A New Benchmark Dataset for Complex World, NOT ACCEPTED</a></li></ul><h4 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h4><ul><li><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Distraction-Aware_Shadow_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Distraction-aware Shadow Detection,CVPR2019</a></p></li><li><p><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Lei_Zhu_Bi-directional_Feature_Pyramid_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Bidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection</a></p></li></ul><h3 id="Shadow-Removal"><a href="#Shadow-Removal" class="headerlink" title="Shadow Removal"></a>Shadow Removal</h3><h4 id="Datasets-1"><a href="#Datasets-1" class="headerlink" title="Datasets"></a>Datasets</h4><ul><li>LRSS</li><li>UIUC</li><li>LRSS</li><li>ISTD <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal, CVPR2018</a></li></ul><h4 id="Papers-1"><a href="#Papers-1" class="headerlink" title="Papers"></a>Papers</h4><ul><li><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.pdf" target="_blank" rel="noopener">DeshadowNet: A Multi-context Embedding Deep Network for Shadow Removal, CVPR2017</a></li></ul><p><strong> dataset: UIUC(76 pairs,  {shadow, shadow-free}), LRSS (37 pairs,  {shadow, shadow-free}), New constructed SRD (3088 pairs {shadow, shadow-free})</strong> input-output: shadow — shadow-free </p><ul><li><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal, CVPR2018</a></li></ul><p><strong> dataset: SRD, UIUC, LRSS, SBU(4727, pairs{shadow, shadow mask}), UCF (245, {shadow, shadow mask}), New constructed ISTD (1870 tuples {shadow, shadow mask, shadow-free})</strong> input-output: shadow — shadow-free (using shadow detector to obtain shadow mask)</p><ul><li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data, ICCV2019</a></li></ul><p><strong> dataset: USR, SRD, ISTD</strong> input-output: {shadow,  shadow mask} — shadow-free </p><ul><li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ARGAN_Attentive_Recurrent_Generative_Adversarial_Network_for_Shadow_Detection_and_ICCV_2019_paper.pdf" target="_blank" rel="noopener">ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal, ICCV2019</a></li></ul><p><strong> dataset: SBU, SRD, ISTD</strong> input-output: shadow — shadow-free </p><ul><li><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Shadow_Removal_via_Shadow_Image_Decomposition_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Shadow Removal via Shadow Image Decomposition, ICCV2019</a></li></ul><p><strong> dataset: ISTD</strong> input-output: {shadow, shadow mask} — shadow-free</p><ul><li><a href="http://www.chengjianglong.com/publications/RISGAN_AAAI.pdf" target="_blank" rel="noopener">RIS-GAN: Explore Residual and Illumination with Generative Adversarial Networks for Shadow Removal, AAAI 2020</a></li></ul><p><strong> dataset: SRD, ISTD</strong> input-output: shadow — shadow-free </p><ul><li><a href="https://www3.cs.stonybrook.edu/~cvl/content/papers/2020/Hieu_ECCV2020.pdf" target="_blank" rel="noopener">From Shadow Segmentation to Shadow Removal, ECCV2020</a></li></ul><p><strong> dataset: ISTD</strong> input-output: {shadow, shadow mask} — shadow-free </p><ul><li><a href="https://arxiv.org/pdf/1911.08718.pdf" target="_blank" rel="noopener">Towards Ghost-free Shadow Removal via<br>Dual Hierarchical Aggregation Network and Shadow Matting GAN, AAAI2020</a></li></ul><p><strong> dataset: ISTD</strong> input-output: {shadow, shadow mask} — shadow-free </p><h3 id="Instance-Shadow"><a href="#Instance-Shadow" class="headerlink" title="Instance Shadow"></a>Instance Shadow</h3><h4 id="Datasets-2"><a href="#Datasets-2" class="headerlink" title="Datasets"></a>Datasets</h4><ul><li>SOBA <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Shadow_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Instance Shadow Detection, CVPR2020</a></li></ul><h4 id="Papers-2"><a href="#Papers-2" class="headerlink" title="Papers"></a>Papers</h4><ul><li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Shadow_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Instance Shadow Detection, CVPR2020</a></li></ul><h3 id="Shadow-Generation"><a href="#Shadow-Generation" class="headerlink" title="Shadow Generation"></a>Shadow Generation</h3><h4 id="Datasets-3"><a href="#Datasets-3" class="headerlink" title="Datasets"></a>Datasets</h4><ul><li>3D rendering generated <a href="https://link.springer.com/article/10.1007/s41095-019-0136-1" target="_blank" rel="noopener">ShadowGAN: Shadow synthesis for virtual objects with conditional adversarial networks</a></li></ul><h4 id="Papers-3"><a href="#Papers-3" class="headerlink" title="Papers"></a>Papers</h4><ul><li><p><a href="https://link.springer.com/article/10.1007/s41095-019-0136-1" target="_blank" rel="noopener">ShadowGAN: Shadow synthesis for virtual objects with conditional adversarial networks</a></p></li><li><p><a href="https://arxiv.org/pdf/2009.08255.pdf" target="_blank" rel="noopener">Adversarial Image Composition with Auxiliary Illumination</a></p></li><li><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_ARShadowGAN_Shadow_Generative_Adversarial_Network_for_Augmented_Reality_in_Single_CVPR_2020_paper.pdf" target="_blank" rel="noopener">ARShadowGAN: Shadow Generative Adversarial Network for Augmented Reality in Single Light Scenes, CVPR2020</a></p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Shadow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Abnormal Generation for Abnormal Detection</title>
      <link href="/2020/11/20/Detection/"/>
      <url>/2020/11/20/Detection/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/yzhao062/anomaly-detection-resources" target="_blank" rel="noopener">Updated Links1 Related General AD</a><br><a href="https://github.com/hoya012/awesome-anomaly-detection" target="_blank" rel="noopener">Updated Links2 Related Image AD</a></p><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><ul><li>not a binary classification because of the limited defected images</li></ul><h2 id="Coarse-Categories"><a href="#Coarse-Categories" class="headerlink" title="Coarse Categories"></a>Coarse Categories</h2><p><a href="https://zhuanlan.zhihu.com/p/116235115" target="_blank" rel="noopener">refer</a></p><ul><li>open-set recognition: labeled data (category information) + unknown data </li><li>unlabeled data: clean (training samples are all positive), polluted(there are a few defected images in training set)<br><img src="figures/AD_category.jpg" alt=""></li></ul><h2 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h2><ul><li><p>open-set recognition (labeled data): classifier -&gt; (predicted category + confidence, {confidence &lt; threshold} is abnormal sample)</p></li><li><p>unlabeled data: training data (normal samples) + testing data(normal + abnormal)</p></li></ul><h3 id="One-Class-Anomaly-Classification-target"><a href="#One-Class-Anomaly-Classification-target" class="headerlink" title="One Class (Anomaly) Classification target"></a>One Class (Anomaly) Classification target</h3><h3 id="Out-of-Distribution-OOD-Detection-target"><a href="#Out-of-Distribution-OOD-Detection-target" class="headerlink" title="Out-of-Distribution(OOD) Detection target"></a>Out-of-Distribution(OOD) Detection target</h3><h3 id="Unsupervised-Anomaly-Segmentation-target"><a href="#Unsupervised-Anomaly-Segmentation-target" class="headerlink" title="Unsupervised Anomaly Segmentation target"></a>Unsupervised Anomaly Segmentation target</h3><h2 id="Defect-Generation"><a href="#Defect-Generation" class="headerlink" title="Defect Generation"></a>Defect Generation</h2><p><a href="https://ieeexplore.ieee.org/document/9000806" target="_blank" rel="noopener">Defect Image Sample Generation With GAN for Improving Defect Recognition</a></p><ul><li>using cycleGAN</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> AD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Disentangle_GAN</title>
      <link href="/2020/09/28/Disentangle-GAN/"/>
      <url>/2020/09/28/Disentangle-GAN/</url>
      
        <content type="html"><![CDATA[<h4 id="Deformation-aware-Unpaired-Image-Translation-for-Pose-Estimation-on-Laboratory-Animals"><a href="#Deformation-aware-Unpaired-Image-Translation-for-Pose-Estimation-on-Laboratory-Animals" class="headerlink" title="Deformation-aware Unpaired Image Translation for Pose Estimation on Laboratory Animals"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Deformation-Aware_Unpaired_Image_Translation_for_Pose_Estimation_on_Laboratory_Animals_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Deformation-aware Unpaired Image Translation for Pose Estimation on Laboratory Animals</a></h4><ul><li>Deformation-aware</li></ul><h4 id="Reusing-Discriminators-for-Encoding-Towards-Unsupervised-Image-to-Image-Translation"><a href="#Reusing-Discriminators-for-Encoding-Towards-Unsupervised-Image-to-Image-Translation" class="headerlink" title="Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Reusing_Discriminators_for_Encoding_Towards_Unsupervised_Image-to-Image_Translation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation</a></h4><ul><li>reusing part parameters of discriminator in generator</li></ul><h4 id="Semi-supervised-Learning-for-Few-shot-Image-to-Image-Translation"><a href="#Semi-supervised-Learning-for-Few-shot-Image-to-Image-Translation" class="headerlink" title="Semi-supervised Learning for Few-shot Image-to-Image Translation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Semi-Supervised_Learning_for_Few-Shot_Image-to-Image_Translation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Semi-supervised Learning for Few-shot Image-to-Image Translation</a></h4><ul><li>noisy label</li></ul><h4 id="Unsupervised-Multi-Modal-Image-Registration-via-Geometry-Preserving-Image-to-Image-Translation"><a href="#Unsupervised-Multi-Modal-Image-Registration-via-Geometry-Preserving-Image-to-Image-Translation" class="headerlink" title="Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Arar_Unsupervised_Multi-Modal_Image_Registration_via_Geometry_Preserving_Image-to-Image_Translation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation</a></h4><ul><li>spatial transformation network and a translation network</li></ul><h4 id="One-Shot-Domain-Adaptation-For-Face-Generation"><a href="#One-Shot-Domain-Adaptation-For-Face-Generation" class="headerlink" title="One-Shot Domain Adaptation For Face Generation"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_One-Shot_Domain_Adaptation_for_Face_Generation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">One-Shot Domain Adaptation For Face Generation</a></h4><ul><li><p>how to mimic a complete distribution of the target image given just one example. </p></li><li><p>our method aims to mimic a complete distribution of the target domain given just one example.</p></li><li><p>we directly manipulate the distribution in the image space.</p></li><li><p>a face manip- ulation detector.</p></li><li><p>For each random style vector s that we sampled with the mapping network, we replace the final layers of s with those of sI before giving it as input to the generator so that the generated random image g(s) inherits the low-level color and textures from I.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> disentangle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>video-based Re-identification</title>
      <link href="/2020/09/25/Re-identification/"/>
      <url>/2020/09/25/Re-identification/</url>
      
        <content type="html"><![CDATA[<h5 id="Video-to-video"><a href="#Video-to-video" class="headerlink" title="Video-to-video"></a>Video-to-video</h5><h6 id="Co-segmentation-Inspired-Attention-Networks-for-Video-based-Person-Re-identification"><a href="#Co-segmentation-Inspired-Attention-Networks-for-Video-based-Person-Re-identification" class="headerlink" title="Co-segmentation Inspired Attention Networks for Video-based Person Re-identification"></a><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identification_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Co-segmentation Inspired Attention Networks for Video-based Person Re-identification</a></h6><ul><li>setting: v-t-v</li><li>issue: severe occlusions, background clutter, viewpoint change, etc. noisy background features from irrelevant non-salient regions may get misinterpreted as the person’s features and get aggregated in the video descriptor.  the subject alignment and scene variation aggravate the prolem and result in a drastic drop in Re-ID accuracy. misses out the salient accessories associated with the subject (e.g., backpack, bag, hat and coat) that are also important cues for Re-ID.</li><li>problem: background nosiy need to be ignored, associated with the subject (e.g., backpack, bag, hat and coat) need to be paid attention.</li><li>previous method: Co-attention by leveraging inter-video (probe vs. gallery video snippets) co-attention(computationally expensive).</li><li>method:discovered a set of distinctive body parts using diverse spatial attentions and discriminative frames by a temporal attention model.  effectively captures the attention between frames of a video.</li><li>effects:  interpretable.</li></ul><h6 id="Global-Local-Temporal-Representations-For-Video-Person-Re-Identification"><a href="#Global-Local-Temporal-Representations-For-Video-Person-Re-Identification" class="headerlink" title="Global-Local Temporal Representations For Video Person Re-Identification"></a><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Global-Local Temporal Representations For Video Person Re-Identification</a></h6><ul><li>setting: v-t-v</li><li>issue: however they still show certain limitations in the aspects of either efficiency or the capability of temporal cues modeling.</li><li>motivation: the short-term temporal cue among adjacent frames helps to distinguish visually similar pedestrians. The long-term temporal cue helps to alleviate the occlusions and noises in video sequences.</li><li>method: exploit the multi-scale temporal cues in video sequences. short-term temporal cues among adjacent frames, then capturing the long-term relations among inconsecutive frames. the short-term temporal cues are modeled by parallel dilated convolutions with different temporal dilation rates to represent the motion and appearance of pedestriain. The long-term relations are captured by a temporal self-attention model to alleviate the occlusions and noises in video sequences.</li></ul><h6 id="Spatial-Temporal-Graph-Convolutional-Network-for-Video-based-Person-Re-identification"><a href="#Spatial-Temporal-Graph-Convolutional-Network-for-Video-based-Person-Re-identification" class="headerlink" title="Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification</a></h6><ul><li>setting: v-t-v</li><li>issue: the occlusion problem and the visual ambiguity problem for visually similar negative samples.</li><li>motivation: modeling the temporal relations of different frames and the spatial relations within a frame has the potential for solving the above problems.</li><li>method: two GCN branches, a spatial one and a temporal one. The spatial branch extracts structural information of a human body. The temporal branch mines discriminative cues from adjacent frames.</li></ul><h6 id="Learning-Multi-Granular-Hypergraphs-for-Video-Based-Person-Re-Identification"><a href="#Learning-Multi-Granular-Hypergraphs-for-Video-Based-Person-Re-Identification" class="headerlink" title="Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Learning_Multi-Granular_Hypergraphs_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification</a></h6><ul><li>setting: v-t-v</li><li>issue: misalignment and occlusion</li></ul><h6 id="Appearance-Preserving-3D-Convolution-for-Video-based-Person-Re-identification"><a href="#Appearance-Preserving-3D-Convolution-for-Video-based-Person-Re-identification" class="headerlink" title="Appearance-Preserving 3D Convolution for Video-based Person Re-identification"></a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470222.pdf" target="_blank" rel="noopener">Appearance-Preserving 3D Convolution for Video-based Person Re-identification</a></h6><ul><li>setting: v-to-v</li><li>issue:  temporal coherence </li><li>method:  we disentangle the video representation into the temporal coherence and motion parts and randomly change the scale of the temporal motion features as the adversarial noise.</li></ul><h6 id="Temporal-Complementary-Learning-for-Video-Person-Re-Identification"><a href="#Temporal-Complementary-Learning-for-Video-Person-Re-Identification" class="headerlink" title="Temporal Complementary Learning for Video Person Re-Identification"></a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700392.pdf" target="_blank" rel="noopener">Temporal Complementary Learning for Video Person Re-Identification</a></h6><ul><li>setting: v-to-v</li><li>issue:  do not take full advantage of the rich spatial-temporal clues in the video. The redundant features typically attend to the same local salient parts, which are difficult to distinguish the persons with similar appearance.  </li><li>method:  discover diverse visual cues for different frames of a video to form a full characteristic of each identify.  erases the most salient part of the second and subsequent frames of the input video sequence.</li></ul><h6 id="Temporal-Coherence-or-Temporal-Motion-Which-is-More-Critical-for-Video-based-Person-Re-identification"><a href="#Temporal-Coherence-or-Temporal-Motion-Which-is-More-Critical-for-Video-based-Person-Re-identification" class="headerlink" title="Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?"></a><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530647.pdf" target="_blank" rel="noopener">Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</a></h6><ul><li>setting: v-to-v</li><li>issue: temporal appearance misalignment </li><li>method:  Appearance-Preserving 3D Convolution</li></ul><h5 id="Image-to-image"><a href="#Image-to-image" class="headerlink" title="Image-to-image"></a>Image-to-image</h5><h5 id="Image-to-video"><a href="#Image-to-video" class="headerlink" title="Image-to-video"></a>Image-to-video</h5><h4 id="Paper1"><a href="#Paper1" class="headerlink" title="Paper1"></a>Paper1</h4><ul><li><p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720256.pdf" target="_blank" rel="noopener">Exploiting Temporal Coherence for Self-Supervised One-shot Video Re-identification</a></p></li><li><p>one-shot: one tracklet</p></li><li><p>combining pseudo-labels and representation learning from temperoal cohenrence</p></li><li><p>novel: representation learning.</p></li></ul><h5 id="Paper2"><a href="#Paper2" class="headerlink" title="Paper2"></a>Paper2</h5><ul><li><p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</a></p></li><li><p>one-shot: one tracklet</p></li></ul><h4 id="Paper3"><a href="#Paper3" class="headerlink" title="Paper3"></a>Paper3</h4><ul><li><p><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Bak_One-Shot_Metric_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">One-Shot Metric Learning for Person Re-identification</a></p></li><li><p>one-shot: single-pair</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Person Re-identification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> video reid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Incremental/Real-time Generative Adversarial Network</title>
      <link href="/2020/09/22/Network/"/>
      <url>/2020/09/22/Network/</url>
      
        <content type="html"><![CDATA[<ul><li>Paper1: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Incremental Learning Using Conditional Adversarial Networks</a></li></ul><ul><li>Paper2: <a href="https://arxiv.org/pdf/1910.01568.pdf" target="_blank" rel="noopener">Incremental learning for the detection and classification of GAN-generated images</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Few-shot Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Incremental/Real-time </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few-shot Learning Baselines</title>
      <link href="/2020/09/20/Few-shot-Learning-Baselines/"/>
      <url>/2020/09/20/Few-shot-Learning-Baselines/</url>
      
        <content type="html"><![CDATA[<h4 id="Few-shot-image-Classification"><a href="#Few-shot-image-Classification" class="headerlink" title="Few-shot image Classification"></a>Few-shot image Classification</h4><h5 id="Related-baselines"><a href="#Related-baselines" class="headerlink" title="Related baselines"></a>Related baselines</h5><ul><li><p>Paper1: <a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="noopener">Matching Networks for One Shot Learning</a>, <a href="https://github.com/AntreasAntoniou/MatchingNetworks" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper2: <a href="https://arxiv.org/pdf/1703.03400.pdf" target="_blank" rel="noopener">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>, <a href="https://github.com/cbfinn/maml" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper3: <a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="noopener">On First-Order Meta-Learning Algorithms</a>, <a href="https://github.com/openai/supervised-reptile" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper4: <a href="https://arxiv.org/pdf/1711.06025.pdf" target="_blank" rel="noopener">Learning to Compare: Relation Network for Few-Shot Learning</a>, <a href="https://github.com/floodsung/LearningToCompare_FSL" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper5: <a href="https://arxiv.org/pdf/2003.14247.pdf" target="_blank" rel="noopener">DPGN: Distribution Propagation Graph Network for Few-shot Learning</a>, <a href="https://github.com/megvii-research/DPGN" target="_blank" rel="noopener">Code links</a></p></li></ul><ul><li><p>Paper6: <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Meta-Transfer Learning for Few-Shot Learning</a>, <a href="https://github.com/y2l/meta-transfer-learning-tensorflow" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper7: <a href="https://arxiv.org/pdf/2001.08735.pdf" target="_blank" rel="noopener">Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation</a>, <a href="https://github.com/hytseng0509/CrossDomainFewShot" target="_blank" rel="noopener">Code links</a></p></li></ul><h5 id="Related-datasets"><a href="#Related-datasets" class="headerlink" title="Related datasets"></a>Related datasets</h5><ul><li><p>Dataset1: <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="noopener">Omniglot</a></p></li><li><p>Dataset2: <a href="https://www.nist.gov/itl/products-and-services/emnist-dataset" target="_blank" rel="noopener">EMNIST</a></p></li><li><p>Dataset3: <a href="https://mtl.yyliu.net/download/" target="_blank" rel="noopener">miniImagenet</a></p></li><li><p>Dataset4: <a href="https://github.com/ElementAI/TADAM" target="_blank" rel="noopener">Fewshot-CIFAR100</a></p></li><li><p>Dataset5: <a href="https://mtl.yyliu.net/download/" target="_blank" rel="noopener">𝒕𝒊𝒆𝒓𝒆𝒅ImageNet</a></p></li></ul><h4 id="Few-shot-image-generation"><a href="#Few-shot-image-generation" class="headerlink" title="Few-shot image generation"></a>Few-shot image generation</h4><h5 id="Related-baselines-1"><a href="#Related-baselines-1" class="headerlink" title="Related baselines"></a>Related baselines</h5><ul><li><p>Paper1: <a href="http://proceedings.mlr.press/v84/bartunov18a/bartunov18a.pdf" target="_blank" rel="noopener">Few-shot Generative Modelling with Generative Matching Networks</a>, <a href="hhttps://github.com/sbos/gmn" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper2: <a href="https://arxiv.org/pdf/1901.02199.pdf" target="_blank" rel="noopener">FIGR: Few-shot Image Generation with Reptile</a>, <a href="https://github.com/LuEE-C/FIGR" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper3: <a href="https://arxiv.org/pdf/2001.00576" target="_blank" rel="noopener">DAWSON: A do- main adaptive few shot generation framework.</a>, <a href="https://github.com/LC1905/musegan/" target="_blank" rel="noopener">Code links</a></p></li><li><p>Paper4: <a href="https://arxiv.org/pdf/1711.04340" target="_blank" rel="noopener">Data Augmentation Generative Adversarial Networks</a>, <a href="https://github.com/AntreasAntoniou/DAGAN" target="_blank" rel="noopener">Code links</a></p></li></ul><h5 id="Related-datasets-1"><a href="#Related-datasets-1" class="headerlink" title="Related datasets"></a>Related datasets</h5><ul><li><p>Dataset1: <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="noopener">Omniglot</a></p></li><li><p>Dataset2: <a href="https://www.nist.gov/itl/products-and-services/emnist-dataset" target="_blank" rel="noopener">EMNIST</a></p></li><li><p>Dataset3: <a href="https://drive.google.com/drive/folders/15x2C11OrNeKLMzBDHrv8NPOwyre6H3O5" target="_blank" rel="noopener">VGGFace</a></p></li><li><p>Dataset4: <a href="https://github.com/NVlabs/FUNIT" target="_blank" rel="noopener">Animal Faces</a></p></li><li><p>Dataset5: <a href="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/" target="_blank" rel="noopener">Flowers</a></p></li><li><p>Dataset6: <a href="http://dl.allaboutbirds.org/nabirds" target="_blank" rel="noopener">NABbirds</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Meta-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Normalization in Deep Learning</title>
      <link href="/2020/09/08/Learning/"/>
      <url>/2020/09/08/Learning/</url>
      
        <content type="html"><![CDATA[<h4 id="Related-links"><a href="#Related-links" class="headerlink" title="Related links"></a>Related links</h4><ul><li><a href="https://zhuanlan.zhihu.com/p/43200897" target="_blank" rel="noopener">1</a></li><li><a href="http://xiaofengshi.com/2019/03/06/深度学习-Normalization/" target="_blank" rel="noopener">2</a></li><li><a href="https://zhuanlan.zhihu.com/p/75920414" target="_blank" rel="noopener">3</a></li></ul><h4 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h4><ul><li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">BN</a></li><li>applicable in cnn and mlp, not applicable in rnn.</li><li>statistical data of mini-batch</li><li>sensitve to the batchsize</li><li>widely used on classification task, generally avoid being used in generative tasks.</li><li>training phase: mu and sigma are calculated from feature map of all mini-batch, scale and shift are learned from data.</li><li>testing phase: using the global mu and sigma as the inputs of bn layer, because single sample is tested in inference phase instead of mini-batch.</li></ul><h4 id="InstanceNorm"><a href="#InstanceNorm" class="headerlink" title="InstanceNorm"></a>InstanceNorm</h4><ul><li><a href="https://arxiv.org/pdf/1607.08022.pdf" target="_blank" rel="noopener">IN</a></li><li>mu and sigma are calculated from feature map of single sample.</li><li>scale and shift are different at each channel.</li><li>not applicable for 2-D feature map, like mlp and rnn.</li><li>applicable for generative tasks.</li></ul><h4 id="LayerNorm"><a href="#LayerNorm" class="headerlink" title="LayerNorm"></a>LayerNorm</h4><ul><li><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">LN paper</a></li><li>applicable in rnn.</li></ul><h4 id="GroupNorm"><a href="#GroupNorm" class="headerlink" title="GroupNorm"></a>GroupNorm</h4><ul><li><a href="https://arxiv.org/pdf/1803.08494.pdf" target="_blank" rel="noopener">GN paper</a></li><li>tradeoff between LN and IN</li></ul><h4 id="WeightNorm"><a href="#WeightNorm" class="headerlink" title="WeightNorm"></a>WeightNorm</h4><ul><li><a href="https://zhuanlan.zhihu.com/p/55102378" target="_blank" rel="noopener">relationship between bn and wn</a></li><li><a href="https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf" target="_blank" rel="noopener">WN paper</a></li><li>applicable in rnn and gan.</li><li>normalization in weights of model parameters.</li></ul><h4 id="CosineNorm"><a href="#CosineNorm" class="headerlink" title="CosineNorm"></a>CosineNorm</h4><ul><li><a href="https://arxiv.org/pdf/1702.05870.pdf" target="_blank" rel="noopener">CN paper</a></li></ul><h4 id="AdaptiveInstanceNorm"><a href="#AdaptiveInstanceNorm" class="headerlink" title="AdaptiveInstanceNorm"></a>AdaptiveInstanceNorm</h4><ul><li><a href="https://zhuanlan.zhihu.com/p/57875010" target="_blank" rel="noopener">from bn to in, from in to cin, from cin to adain</a></li><li><a href="https://arxiv.org/pdf/1703.06868.pdf" target="_blank" rel="noopener">AdaIN paper</a></li><li>learning shift and scale from other samples.</li></ul><h4 id="AttentiveNorm"><a href="#AttentiveNorm" class="headerlink" title="AttentiveNorm"></a>AttentiveNorm</h4><ul><li><a href="https://arxiv.org/pdf/2004.03828.pdf" target="_blank" rel="noopener">AN paper</a></li><li>model long-range dependency in class-conditional image generation. It introduces a self-attention module in the convolution-based generator, which is helpful for capturing the relation of distant regions.</li><li>based on IN, preserving semantics spatially. </li><li><a href="https://github.com/shepnerd/AttenNorm/blob/466d727d27fc17dbccd1a5e2090fe91491a26483/inpaint-attnorm/net/network.py#L8" target="_blank" rel="noopener">tf code</a></li><li>can be used in F2GAN(smilar to Class-conditional Image Generation, using the interpolation coefficients as class information)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fewshot Feature Generation</title>
      <link href="/2020/09/01/Fewshot-Feature-Generation/"/>
      <url>/2020/09/01/Fewshot-Feature-Generation/</url>
      
        <content type="html"><![CDATA[<h4 id="Paper1"><a href="#Paper1" class="headerlink" title="Paper1"></a>Paper1</h4><ul><li><p>paper links: <a href="https://arxiv.org/pdf/1806.04734.pdf" target="_blank" rel="noopener">∆-encoder: an effective sample synthesis method for few-shot object recognition</a></p></li><li><p>feature augmentation.</p></li></ul><h4 id="Paper2"><a href="#Paper2" class="headerlink" title="Paper2"></a>Paper2</h4><ul><li>paper links: <a href="https://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks.pdf" target="_blank" rel="noopener">Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks</a></li><li><p>feature augmentation.</p></li><li><p>addressing the problem of <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Hariharan_Low-Shot_Visual_Recognition_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Low-shot Visual Recognition by Shrinking and Hallucinating Features</a>: learn a finite set of transformation mappings between examples in each base category and directly apply them to seed novel points for extra data. However, since mappings are<br>enumerable (even in large amount), this model suffers from poor generalization. </p></li><li><p>intra-class variances of base classes are shareable with any novel classes.</p></li><li><p>building upon the assumption that related classes should have similar intra-class variance, we introduce a new loss term for preserving covariance during the translation process.</p></li><li><p>incorporating extra noise from a mixture of Gaussian distributions could result in more<br>diverse results.</p></li><li><p>preserving covariance information from relevant base classes to a novel class can improve low-shot generation quality.</p></li><li><p>weighted loss: similarity, weights assigned to each training samples (the distance between noval class and base class). No reconstruction method, 2way-2shot.</p></li></ul><h4 id="Paper3"><a href="#Paper3" class="headerlink" title="Paper3"></a>Paper3</h4><ul><li><p>paper links: <a href="https://arxiv.org/pdf/1803.09014.pdf" target="_blank" rel="noopener">Feature Transfer Learning for Face Recognition with Under-Represented Data</a></p></li><li><p>A Gaussian prior of the variance is assumed across all subjects and the variance from regular ones are transferred to the under-represented ones.</p></li><li><p>generating vague images: translatede results.</p></li></ul><h4 id="Paper4"><a href="#Paper4" class="headerlink" title="Paper4"></a>Paper4</h4><ul><li><p>paper links: <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Dixit_AGA_Attribute-Guided_Augmentation_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Attribute-Guided Augmentation</a></p></li><li><p>feature augmentation.</p></li><li><p>learns a mapping that allows syn- thesis of data such that an attribute of a synthesized sample is at a desired value or strength.</p></li><li><p>This is particularly inter- esting in situations where little data with no attribute an- notation is available for learning, but we have access to an external corpus of heavily annotated samples.</p></li></ul><h4 id="Paper5"><a href="#Paper5" class="headerlink" title="Paper5"></a>Paper5</h4><ul><li><p>paper links: <a href="https://arxiv.org/pdf/2002.10826.pdf" target="_blank" rel="noopener">Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective</a></p></li><li><p>Constructing the feature cloud for long-tailed data.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> FewShot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> delta_related </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few-shot Image Translation/Generation Baselines</title>
      <link href="/2020/07/01/Translation-Generation/"/>
      <url>/2020/07/01/Translation-Generation/</url>
      
        <content type="html"><![CDATA[<h4 id="few-shot-translation"><a href="#few-shot-translation" class="headerlink" title="few-shot translation"></a>few-shot translation</h4><h5 id="FUNIT"><a href="#FUNIT" class="headerlink" title="FUNIT"></a>FUNIT</h5><ul><li><a href="https://arxiv.org/pdf/1905.01723.pdf" target="_blank" rel="noopener">Few-Shot Unsupervised Image-to-Image Translation</a></li></ul><h6 id="Main-idea"><a href="#Main-idea" class="headerlink" title="Main idea"></a>Main idea</h6><ul><li>disentangled into appearance code and pose code.</li><li>disadvantage: using seen categories in testing phase, failing to maintain the category information for translated images. </li></ul><h6 id="Datsets"><a href="#Datsets" class="headerlink" title="Datsets"></a>Datsets</h6><ul><li>Animals Faces</li><li>Flowers</li><li>Nabirds</li><li>Food</li></ul><h6 id="Metrcis"><a href="#Metrcis" class="headerlink" title="Metrcis"></a>Metrcis</h6><ul><li>Translation accuracy: the class information of translated images.</li><li>Domain-invariant perceptual distance: feature distance between content images and translated images.</li><li>IS</li><li>mFID</li></ul><h6 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h6><ul><li>StarGAN <a href="https://arxiv.org/pdf/1711.09020.pdf" target="_blank" rel="noopener">Stargan: Unified genera- tive adversarial networks for multi-domain image-to-image translation</a></li><li>CycleGAN   <a href="https://arxiv.org/pdf/1703.10593" target="_blank" rel="noopener">Unpaired image-to-image translation using cycle- consistent adversarial networks</a> </li><li>MUNIT <a href="https://arxiv.org/pdf/1804.04732.pdf" target="_blank" rel="noopener">Multimodal unsupervised image-to-image translation</a></li><li>UNIT <a href="https://arxiv.org/pdf/1901.08242v1.pdf" target="_blank" rel="noopener">Unsupervised Image-to-Image Translation with Self-Attention Networks</a></li></ul><h6 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h6><ul><li>loss term</li><li>1/5/15 shot setting</li></ul><h5 id="SEMIT"><a href="#SEMIT" class="headerlink" title="SEMIT"></a>SEMIT</h5><ul><li><a href="https://arxiv.org/pdf/2003.13853.pdf" target="_blank" rel="noopener">Semi-supervised Learning for Few-shot Image-to-Image Translation</a></li></ul><h6 id="Main-idea-1"><a href="#Main-idea-1" class="headerlink" title="Main idea"></a>Main idea</h6><ul><li>disentangled into appearance code and pose code.</li><li>reducing the amount of required labeled data also from the source domain during training, adding unlabeled data.</li><li>Pseudo-Labeling approach that is trained progressively with a soft- labeling scheme to avoid the noise accumulation problem.</li></ul><h6 id="Datsets-1"><a href="#Datsets-1" class="headerlink" title="Datsets"></a>Datsets</h6><ul><li>Animals Faces</li><li>Flowers</li><li>Nabirds</li><li>Food</li></ul><h6 id="Metrcis-1"><a href="#Metrcis-1" class="headerlink" title="Metrcis"></a>Metrcis</h6><ul><li>Translation accuracy: the class information of translated images.</li><li>IS</li><li>mFID</li></ul><h6 id="Baselines-1"><a href="#Baselines-1" class="headerlink" title="Baselines"></a>Baselines</h6><ul><li>StarGAN <a href="https://arxiv.org/pdf/1711.09020.pdf" target="_blank" rel="noopener">Stargan: Unified genera- tive adversarial networks for multi-domain image-to-image translation</a></li><li>CycleGAN   <a href="https://arxiv.org/pdf/1703.10593" target="_blank" rel="noopener">Unpaired image-to-image translation using cycle- consistent adversarial networks</a> </li><li>MUNIT <a href="https://arxiv.org/pdf/1804.04732.pdf" target="_blank" rel="noopener">Multimodal unsupervised image-to-image translation</a></li><li>FUNIT <a href="https://arxiv.org/pdf/1905.01723.pdf" target="_blank" rel="noopener">Few-Shot Unsupervised Image-to-Image Translation</a></li></ul><h6 id="Ablation-Study-1"><a href="#Ablation-Study-1" class="headerlink" title="Ablation Study"></a>Ablation Study</h6><ul><li>removing/maintain component</li></ul><h5 id="TUNIT"><a href="#TUNIT" class="headerlink" title="TUNIT"></a>TUNIT</h5><ul><li><a href="https://arxiv.org/pdf/2006.06500.pdf" target="_blank" rel="noopener">Rethinking the Truly Unsupervised Image-to-Image Translation</a></li></ul><h6 id="Main-idea-2"><a href="#Main-idea-2" class="headerlink" title="Main idea"></a>Main idea</h6><ul><li>disentangled into appearance code and pose code.</li><li>image-to-image translation in a fully unsupervised setting, using unlabeled data.</li><li>setting the number of domains, clustering, using style code to classify the image into corresponding domains.</li></ul><h6 id="Datsets-2"><a href="#Datsets-2" class="headerlink" title="Datsets"></a>Datsets</h6><ul><li>AFHQ</li><li>FFHQ</li><li>LSUN Car </li><li>AnimalFaces-10</li><li>Summer2winter</li></ul><h6 id="Metrcis-2"><a href="#Metrcis-2" class="headerlink" title="Metrcis"></a>Metrcis</h6><ul><li>mFID</li></ul><h6 id="Baselines-2"><a href="#Baselines-2" class="headerlink" title="Baselines"></a>Baselines</h6><ul><li>FUNIT <a href="https://arxiv.org/pdf/1905.01723.pdf" target="_blank" rel="noopener">Few-Shot Unsupervised Image-to-Image Translation</a></li><li>MSGAN <a href="https://arxiv.org/pdf/1903.05628.pdf" target="_blank" rel="noopener">Mode seeking generative adversarial networks for diverse image synthesis</a></li></ul><h6 id="Ablation-Study-2"><a href="#Ablation-Study-2" class="headerlink" title="Ablation Study"></a>Ablation Study</h6><ul><li>without labels, with few labels, vary the ratio of labeled data.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Baselines </category>
          
      </categories>
      
      
        <tags>
            
            <tag> image generation/translation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020CVPR Paper Reading</title>
      <link href="/2020/06/11/2020CVPR-Paper-Reading/"/>
      <url>/2020/06/11/2020CVPR-Paper-Reading/</url>
      
        <content type="html"><![CDATA[<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><ul><li><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_SEAN_Image_Synthesis_With_Semantic_Region-Adaptive_Normalization_CVPR_2020_paper.pdf" target="_blank" rel="noopener">SEAN: Image Synthesis with Semantic Region-Adaptive Normalization</a></li><li><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Attentive_Normalization_for_Conditional_Image_Generation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Attentive Normalization for Conditional Image Generation</a></li></ul><h2 id="Diversity"><a href="#Diversity" class="headerlink" title="Diversity"></a>Diversity</h2><ul><li>[Diverse Image Generation via Self-Conditioned GANs](<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Diverse_Image_Generation_via_Self-Conditioned_GANs_CVPR_2020_paper.pdfs" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Diverse_Image_Generation_via_Self-Conditioned_GANs_CVPR_2020_paper.pdfs</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Geometry-aware Generation</title>
      <link href="/2020/06/08/Geometry-aware-Generation/"/>
      <url>/2020/06/08/Geometry-aware-Generation/</url>
      
        <content type="html"><![CDATA[<h4 id="TransGaGa-Geometry-Aware-Unsupervised-Image-to-Image-Translation"><a href="#TransGaGa-Geometry-Aware-Unsupervised-Image-to-Image-Translation" class="headerlink" title="TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation"></a>TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation</h4><ul><li><a href="https://arxiv.org/pdf/1904.09571.pdf" target="_blank" rel="noopener">paper</a></li><li>problem: large geometry variations in image-to-image translation.</li><li>idea: disentangle image space into a Cartesian product of the appearance and the geometry latent spaces.</li><li>method: a geometry prior loss and a conditional VAE loss.</li><li>possible problems: two different domain.</li></ul><h4 id="Conditional-Image-Generation-for-Learning-the-Structure-of-Visual-Objects"><a href="#Conditional-Image-Generation-for-Learning-the-Structure-of-Visual-Objects" class="headerlink" title="Conditional Image Generation for Learning the Structure of Visual Objects"></a>Conditional Image Generation for Learning the Structure of Visual Objects</h4><ul><li><a href="https://128.84.21.199/pdf/1806.07823v1.pdf" target="_blank" rel="noopener">paper-Geometry transfermer used in Transgaga</a></li><li><a href="https://github.com/tomasjakab/imm" target="_blank" rel="noopener">code</a></li></ul><h4 id="Unsupervised-Discovery-of-Object-Landmarks-as-Structural-Representations"><a href="#Unsupervised-Discovery-of-Object-Landmarks-as-Structural-Representations" class="headerlink" title="Unsupervised Discovery of Object Landmarks as Structural Representations"></a>Unsupervised Discovery of Object Landmarks as Structural Representations</h4><ul><li><a href="http://www.ytzhang.net/projects/lmdis-rep/" target="_blank" rel="noopener">paper</a></li><li><a href="https://github.com/YutingZhang/lmdis-rep" target="_blank" rel="noopener">code</a></li></ul><h4 id="Unsupervised-learning-of-object-landmarks-by-factorized-spatial-embeddings"><a href="#Unsupervised-learning-of-object-landmarks-by-factorized-spatial-embeddings" class="headerlink" title="Unsupervised learning of object landmarks by factorized spatial embeddings"></a>Unsupervised learning of object landmarks by factorized spatial embeddings</h4><ul><li><a href="http://www.robots.ox.ac.uk/~vedaldi//assets/pubs/thewlis17unsupervised.pdf" target="_blank" rel="noopener">paper</a></li><li><a href="https://github.com/alldbi/Factorized-Spatial-Embeddings" target="_blank" rel="noopener">code</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Survey </category>
          
      </categories>
      
      
        <tags>
            
            <tag> image generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>From Image Generation to Fewshot Image Generation</title>
      <link href="/2020/06/08/From-Generation-to-Fewshot-Generation/"/>
      <url>/2020/06/08/From-Generation-to-Fewshot-Generation/</url>
      
        <content type="html"><![CDATA[<h4 id="pix2pix"><a href="#pix2pix" class="headerlink" title="pix2pix"></a><a href="https://arxiv.org/pdf/1611.07004.pdf" target="_blank" rel="noopener">pix2pix</a></h4><ul><li>conditional discrimiantor (channel-wise concat)</li><li>unet generator with skip connections</li><li>patch discriminator</li></ul><h4 id="pix2pixHD"><a href="#pix2pixHD" class="headerlink" title="pix2pixHD"></a><a href="https://research.nvidia.com/sites/default/files/pubs/2017-12_High-Resolution-Image-Synthesis/1711.11585.pdf" target="_blank" rel="noopener">pix2pixHD</a></h4><ul><li>generating high-resolution images.</li><li>a coarse-to-fine generator: a globale generator and a local enhancer networks (G1-G2)</li><li>a multi-scale discriminator architecture: multiple discriminators are used to deal with different scale images.</li><li>a robust adversarial learning objective function: feature matching between discrinative features from real samples and fake samples.</li></ul><h4 id="SPADE"><a href="#SPADE" class="headerlink" title="SPADE"></a><a href="https://arxiv.org/pdf/1903.07291.pdf" target="_blank" rel="noopener">SPADE</a></h4><ul><li>SPADE is a generalization of several existing normalization layers, the proposed SPADE is better suited for semantic image synthesis.</li><li>SPADE can be added into generator and discriminator.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Survey </category>
          
      </categories>
      
      
        <tags>
            
            <tag> image generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Style Translation Framework</title>
      <link href="/2020/03/24/Style-Translation-Framework/"/>
      <url>/2020/03/24/Style-Translation-Framework/</url>
      
        <content type="html"><![CDATA[<h2 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h2><h3 id="cycleGAN"><a href="#cycleGAN" class="headerlink" title="cycleGAN"></a>cycleGAN</h3><ul><li><p><a href="https://arxiv.org/pdf/1703.10593.pdf" target="_blank" rel="noopener">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></p></li><li><p>main idea: F(G(X)) ≈ X for unpaired images translation from two domain.</p></li><li>optimization: two pairs of GAN LOSS and a cycle loss.</li><li>resluts: the structure of translated images is don’t change.</li><li>image size: 128, 256,…</li><li>networks: residual networks (increasing blocks for those images with higher resolution)</li><li>applicable points for ours: cycle reconstruction</li><li>not applicable points for ours: image-to-image translation, not change image structure.</li></ul><h2 id="From-StyleGAN1-0-to-StyleGAN2-0"><a href="#From-StyleGAN1-0-to-StyleGAN2-0" class="headerlink" title="From StyleGAN1.0 to StyleGAN2.0"></a>From StyleGAN1.0 to StyleGAN2.0</h2><h3 id="styleGAN1-0"><a href="#styleGAN1-0" class="headerlink" title="styleGAN1.0"></a>styleGAN1.0</h3><ul><li><a href="https://arxiv.org/pdf/1812.04948.pdf" target="_blank" rel="noopener">A Style-Based Generator Architecture for Generative Adversarial Networks</a></li><li>main idea: designing alternative generator to learn unsupervised separation of high-level attributes. scale-specific modifications to the styles.</li><li>diversity: global noise -&gt; {w,A} for all feature map, local noise -&gt; {noise, B}noise broadcast for each layer feature map, adding style mixing (global adain,prevents the network from assuming that adjacent styles are correlated.)  Stochastic variation(add local noise).</li><li>Disentanglement</li><li>optimization: gan</li><li>network: A -&gt; affine transformation, scale all feature map, global attributs(hairs, stubble, freckles, or skin pores). B -&gt; noise added to each layer, local attributes(pose, lighting, or background style), leaving the overall composition and high-level aspects such as identity intact.</li><li>image size: &gt;=256.</li><li>results: achieve better interpolation and better disentanglement.</li><li>applicable points for ours: {noise, B} module, to change local style maintain global style like identity (fine-grained category), intra-class translation like our deltgan.</li><li>problem: how to design as conditional generator</li></ul><h3 id="styleGAN2-0"><a href="#styleGAN2-0" class="headerlink" title="styleGAN2.0"></a>styleGAN2.0</h3><ul><li><a href="https://arxiv.org/pdf/1912.04958.pdf" target="_blank" rel="noopener">Analyzing and Improving the Image Quality of StyleGAN</a></li><li>main idea: based on styleganv1, characteristic artifacts -&gt; change architecture and training methods. modified adain module.</li><li>network: a skip generator and a residual discriminator.</li><li>results: improve gan metrics, modify artifacts.</li><li>applicable points for ours: combining a skip generator and a residual discriminator. modifying adain?</li></ul><h2 id="From-StarGAN1-0-to-StarGAN2-0"><a href="#From-StarGAN1-0-to-StarGAN2-0" class="headerlink" title="From StarGAN1.0 to StarGAN2.0"></a>From StarGAN1.0 to StarGAN2.0</h2><h3 id="starGAN1-0"><a href="#starGAN1-0" class="headerlink" title="starGAN1.0"></a>starGAN1.0</h3><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf" target="_blank" rel="noopener">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></li><li>main idea: scalability, more domains, using a single model</li><li>method: multiple domain, discirminator distinguish, generator regard domain as conditional information.</li><li>applicable points for ours: domain has statistical information? can learn a distribution? category-to-category, difficult to learn statistical information? domain information has specific definition, however, our targan conditional information is the feature of target image and no specific meaning?</li></ul><h3 id="starGAN2-0"><a href="#starGAN2-0" class="headerlink" title="starGAN2.0"></a>starGAN2.0</h3><ul><li><a href="https://arxiv.org/pdf/1912.01865.pdf" target="_blank" rel="noopener">StarGAN v2: Diverse Image Synthesis for Multiple Domains</a></li><li>main idea: diversity, translation can be achieved among same domain also can be among dofferent domain.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Translation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> style translation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fewshot-classifier-baselines</title>
      <link href="/2020/01/17/fewshot-classifier-baselines/"/>
      <url>/2020/01/17/fewshot-classifier-baselines/</url>
      
        <content type="html"><![CDATA[<h4 id="Meta-transfer-learning-for-few-shot-learning"><a href="#Meta-transfer-learning-for-few-shot-learning" class="headerlink" title="Meta-transfer learning for few-shot learning"></a><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Meta-transfer learning for few-shot learning</a></h4><h4 id="Revisiting-Local-Descriptor-based-Image-to-Class-Measure-for-Few-shot-Learning"><a href="#Revisiting-Local-Descriptor-based-Image-to-Class-Measure-for-Few-shot-Learning" class="headerlink" title="Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning"></a><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Revisiting_Local_Descriptor_Based_Image-To-Class_Measure_for_Few-Shot_Learning_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning</a></h4><h4 id="Learning-to-Compare-Relation-Network-for-Few-Shot-Learning"><a href="#Learning-to-Compare-Relation-Network-for-Few-Shot-Learning" class="headerlink" title="Learning to Compare: Relation Network for Few-Shot Learning"></a><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Learning to Compare: Relation Network for Few-Shot Learning</a></h4>]]></content>
      
      
      <categories>
          
          <category> Baselines </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diversity tricks</title>
      <link href="/2020/01/05/Diversity-tricks/"/>
      <url>/2020/01/05/Diversity-tricks/</url>
      
        <content type="html"><![CDATA[<h4 id="increase-the-diversity-of-generated-images"><a href="#increase-the-diversity-of-generated-images" class="headerlink" title="increase the diversity of generated images"></a>increase the diversity of generated images</h4><h6 id="mode-seeking-loss"><a href="#mode-seeking-loss" class="headerlink" title="mode seeking loss"></a><a href="https://arxiv.org/pdf/1903.05628.pdf" target="_blank" rel="noopener">mode seeking loss</a></h6><h6 id="style-diversity"><a href="#style-diversity" class="headerlink" title="style diversity"></a><a href="https://arxiv.org/pdf/1912.01865.pdf" target="_blank" rel="noopener">style diversity</a></h6><h6 id="interpolation-regression"><a href="#interpolation-regression" class="headerlink" title="interpolation regression"></a><a href="https://arxiv.org/pdf/1908.07269.pdf" target="_blank" rel="noopener">interpolation regression</a></h6>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> generation diversity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention-Related</title>
      <link href="/2019/12/19/Attention-Related/"/>
      <url>/2019/12/19/Attention-Related/</url>
      
        <content type="html"><![CDATA[<h2 id="Setting"><a href="#Setting" class="headerlink" title="Setting"></a>Setting</h2><ul><li>given feature map $f(w,h,c1)$</li><li>given feature map $g(w,h,c2)$</li><li>assume learned attention map for g</li></ul><h3 id="Separation-between-channel-and-spatial-Attention-based-Fusion-for-Multi-source-Human-Image-Generation"><a href="#Separation-between-channel-and-spatial-Attention-based-Fusion-for-Multi-source-Human-Image-Generation" class="headerlink" title="Separation between channel and spatial Attention-based Fusion for Multi-source Human Image Generation"></a>Separation between channel and spatial <a href="https://arxiv.org/pdf/1905.02655v1.pdf" target="_blank" rel="noopener">Attention-based Fusion for Multi-source Human Image Generation</a></h3><ul><li>spatial attention s[w,h,1]: concat[f,g] -&gt; feature [w,h,c1+c2] -&gt; convolutional layer [1,1,1] -&gt;sigmoid -&gt; [w,h,1]</li><li>channel attention c[1,1,c1/c2]: concat[f,g] -&gt; feature [w,h,c1+c2] -&gt; avgpool layer -&gt; FC layer[c2] -&gt; sigmoid -&gt; [c2]</li><li>attention map a[w,h,c2]: s[w,h,1] * c[1,1,c2]</li><li>attention feature[w,h,c2]: f/g * a</li></ul><h3 id="Global-attention"><a href="#Global-attention" class="headerlink" title="Global attention"></a>Global attention</h3><ul><li>global attention map a[w,h,c2]: concat[f,g]-&gt; feature [w,h,c1+c2] -&gt; convolutional layer[1,1,c2] -&gt; [w,h,c2]</li><li>attention feature[w,h,c2]: f/g * a</li></ul><h3 id="Self-AttentionSelf-Attention-Generative-Adversarial-Networks"><a href="#Self-AttentionSelf-Attention-Generative-Adversarial-Networks" class="headerlink" title="Self AttentionSelf-Attention Generative Adversarial Networks"></a>Self Attention<a href="https://arxiv.org/pdf/1805.08318.pdf" target="_blank" rel="noopener">Self-Attention Generative Adversarial Networks</a></h3><ul><li>specified channels: channels</li><li>f[w,h,c1] -&gt; convolutional layer[1,1,channels/8] -&gt; ff[w,h,channels/8]</li><li>g[w,h,c2] -&gt; convolutional layer[1,1,channels/8] -&gt; gg[w,h,channels/8]</li><li>attention map: resized ff[w<em>h,channels/8] matmul gg[w</em>h,channels/8] (matrix) -&gt; softmax -&gt; attention map[w<em>h,w</em>h]</li><li>g[w,h,c2] -&gt; convolutional layer[1,1,channels] -&gt; hh[w,h,channels]</li><li>attention feature: resized hh[w<em>h,channels] matmul attention map[w</em>h,w<em>h] -&gt; [w</em>h,channels] -&gt; reshape [w,h,channels]</li></ul><h3 id="Non-local-attention-Non-local-Neural-Networks"><a href="#Non-local-attention-Non-local-Neural-Networks" class="headerlink" title="Non local attention Non-local Neural Networks"></a>Non local attention <a href="https://www.zpascal.net/cvpr2018/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Non-local Neural Networks</a></h3>]]></content>
      
      
      <categories>
          
          <category> Attention </category>
          
      </categories>
      
      
        <tags>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ICCV-Paper-Reading</title>
      <link href="/2019/10/31/ICCV-Paper-Reading/"/>
      <url>/2019/10/31/ICCV-Paper-Reading/</url>
      
        <content type="html"><![CDATA[<h4 id="sinGAN"><a href="#sinGAN" class="headerlink" title="sinGAN"></a><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf" target="_blank" rel="noopener">sinGAN</a></h4><h5 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h5><ul><li>unconditional generative model</li><li>capture internal distribution of patches within the image</li><li>generate diverse and high-quality samples that carry the same visual content as the image</li><li>containing a pyramid of fully convolutional GANs, learning the patch distribution at a different scale of image, which can ensure significant variability but remain the global structure.</li><li>generating images from noisy</li></ul><h5 id="method"><a href="#method" class="headerlink" title="method"></a>method</h5><h6 id="training-strategy"><a href="#training-strategy" class="headerlink" title="training strategy"></a>training strategy</h6><ul><li>coarse-to-fine fashion</li><li>multi-scale</li></ul><h6 id="mult-scale-patch-generator"><a href="#mult-scale-patch-generator" class="headerlink" title="mult-scale patch generator"></a>mult-scale patch generator</h6><h6 id="mult-scale-patch-discriminator"><a href="#mult-scale-patch-discriminator" class="headerlink" title="mult-scale patch discriminator"></a>mult-scale patch discriminator</h6><h5 id="RELATED"><a href="#RELATED" class="headerlink" title="RELATED"></a>RELATED</h5><ul><li>patch discriminator </li><li>the standard deviation of noise Z, can be propotional to the root RMSE between reconstructed image and real image.</li></ul><h4 id="inGAN"><a href="#inGAN" class="headerlink" title="inGAN"></a><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Shocher_InGAN_Capturing_and_Retargeting_the_DNA_of_a_Natural_Image_ICCV_2019_paper.pdf" target="_blank" rel="noopener">inGAN</a></h4><h5 id="abstract-1"><a href="#abstract-1" class="headerlink" title="abstract"></a>abstract</h5><ul><li>each image hase its internal statistics captured by its unique distribution of patches.</li><li>image-specific gan, patch-distribution</li><li>generating diverse images that can maintain internal patch distribution</li><li><h5 id="method-1"><a href="#method-1" class="headerlink" title="method"></a>method</h5><h5 id="RELATED-1"><a href="#RELATED-1" class="headerlink" title="RELATED"></a>RELATED</h5></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative </category>
          
      </categories>
      
      
        <tags>
            
            <tag> image generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Variations of GAN</title>
      <link href="/2019/09/19/Variations-of-GAN/"/>
      <url>/2019/09/19/Variations-of-GAN/</url>
      
        <content type="html"><![CDATA[<p>refer to <a href="https://blog.csdn.net/qq_25737169/article/details/84864808" target="_blank" rel="noopener">blogs1</a>, <a href="http://www.sohu.com/a/302267498_473283" target="_blank" rel="noopener">blogs2</a>, <a href="https://juejin.im/post/5d234eb16fb9a07f0870b9be" target="_blank" rel="noopener">blogs3</a></p><h5 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h5><p><a href="https://arxiv.org/pdf/1511.06434.pdf" target="_blank" rel="noopener">paper link</a></p><ul><li>network design</li><li>latent interpolation</li><li>using feature from discriminator to train classifier</li></ul><h5 id="conditional-GAN"><a href="#conditional-GAN" class="headerlink" title="conditional GAN"></a>conditional GAN</h5><p><a href="https://arxiv.org/pdf/1411.1784.pdf" target="_blank" rel="noopener">paper link</a></p><ul><li>using label information to stablize the training of GAN</li><li>showing that how to leverage prior information to achieve image-to-image ot text-to-image</li></ul><h5 id="bigGAN"><a href="#bigGAN" class="headerlink" title="bigGAN"></a>bigGAN</h5><p><a href="https://arxiv.org/abs/1809.11096" target="_blank" rel="noopener">paper link</a></p><ul><li>self-attention</li><li>Spectral Normalization</li><li>cGAN</li></ul><h5 id="styleGAN"><a href="#styleGAN" class="headerlink" title="styleGAN"></a>styleGAN</h5><p><a href="https://arxiv.org/abs/1812.04948" target="_blank" rel="noopener">paper link</a></p><ul><li>control latent space</li><li>using AdaIN mechanism</li></ul><h5 id="cycleGAN"><a href="#cycleGAN" class="headerlink" title="cycleGAN"></a>cycleGAN</h5><p><a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">paper link</a></p><ul><li>cycle-Consistency loss</li><li>the stablity of GAN training</li></ul><h5 id="Pix2Pix"><a href="#Pix2Pix" class="headerlink" title="Pix2Pix"></a>Pix2Pix</h5><p><a href="https://arxiv.org/abs/1611.07004" target="_blank" rel="noopener">paper link</a></p><ul><li>Unet</li><li>skip connection in ResUnet</li><li>to distinguish the fake image from the real image by patchGAN which observe the small area of image</li></ul><h5 id="stackGAN"><a href="#stackGAN" class="headerlink" title="stackGAN"></a>stackGAN</h5><p><a href="https://arxiv.org/abs/1612.03242" target="_blank" rel="noopener">paper link</a></p><ul><li>text-to-image</li><li>multi-stage (from small to large)</li></ul><h5 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h5><p><a href="https://arxiv.org/pdf/1701.07875.pdf" target="_blank" rel="noopener">paper link</a></p><ul><li><p>aim: minimize the distribution distance between real data and generated data.<br>\begin{equation}<br>\delta\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\sup _{A \in \Sigma}\left|\mathbb{P}_{r}(A)-\mathbb{P}_{g}(A)\right|<br>\end{equation}</p></li><li><p>problem: superior discriminator lead to the gradient vanish of the generator, while the gradient of the generator is unaccuracy with inferior discriminator. the ability of discriminator need to be controlled. </p></li></ul><h6 id="different-distance"><a href="#different-distance" class="headerlink" title="different distance"></a>different distance</h6><ul><li>KL: assymetric and possibly infinite</li><li>JS: symmetrical and always defined</li><li>EM(earch mover): the “cost” of the optimal transport plan.<br>\begin{equation}<br>W\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\inf _{\gamma \in \Pi\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)} \mathbb{E}_{(x, y) \sim \gamma}[|x-y|]<br>\end{equation}</li></ul><h6 id="WGAN-1"><a href="#WGAN-1" class="headerlink" title="WGAN"></a>WGAN</h6><ul><li>using EM to evaluate the distribution distance</li><li>remove sigmoid from the last layer of discriminator</li><li>without log operation for discriminator loss</li><li>restrict the parameter of discriminator to fixted value after each update</li><li>using RMSProp or SGD</li></ul><p>\begin{equation}<br>\max _{|f|_{L} \leq 1} \mathbb{E}_{x \sim \mathbb{P}_{r}}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_{\theta}}[f(x)]<br>\end{equation}</p><p>where Lipschitz:</p><p>\begin{equation}<br>\left|f\left(x_{1}\right)-f\left(x_{2}\right)\right| \leq K\left|x_{1}-x_{2}\right|<br>\end{equation}</p><p>means that the absolute value of Derivative function lower than fixted K.</p><p>in the restriction, the total loss of WGAN:<br>\begin{equation}<br>L=\mathbb{E}_{x \sim P_{r}}\left[f_{w}(x)\right]-\mathbb{E}_{x \sim P_{g}}\left[f_{w}(x)\right]<br>\end{equation} </p><p>for generator:<br>\begin{equation}<br>-\mathbb{E}_{x \sim P_{g}}\left[f_{w}(x)\right]<br>\end{equation}</p><p>for discriminator:<br>\begin{equation}<br>\mathbb{E}_{x \sim P_{g}}\left[f_{w}(x)\right]-\mathbb{E}_{x \sim P_{r}}\left[f_{w}(x)\right]<br>\end{equation}</p><h5 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h5><p>\begin{equation}<br>L=\underset{\tilde{\boldsymbol{x}} \sim \mathbb{P}_{g}}{\mathbb{E}}[D(\tilde{\boldsymbol{x}})]-\underset{\boldsymbol{x} \sim \mathbb{P}_{r}}{\mathbb{E}}[D(\boldsymbol{x})]+ \lambda \underset{\hat{\boldsymbol{x}} \sim \mathbb{P}_{\boldsymbol{x}}}{\mathbb{E}}\left[\left(\left|\nabla_{\hat{\boldsymbol{x}}} D(\hat{\boldsymbol{x}})\right|_{2}-1\right)^{2}\right]<br>\end{equation}</p><p>for generator:<br>\begin{equation}<br>-\mathbb{E}_{x \sim P_{g}}\left[f_{w}(x)\right]<br>\end{equation}</p><p>for discriminator:<br>\begin{equation}<br>\mathbb{E}_{x \sim P_{g}}\left[f_{w}(x)\right]-\mathbb{E}_{x \sim P_{r}}\left[f_{w}(x)\right] + \lambda \underset{\hat{\boldsymbol{x}} \sim \mathbb{P}_{\boldsymbol{x}}}{\mathbb{E}}\left[\left(\left|\nabla_{\hat{\boldsymbol{x}}} D(\hat{\boldsymbol{x}})\right|_{2}-1\right)^{2}\right]<br>\end{equation}</p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> variation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Loss for VAE-GAN</title>
      <link href="/2019/09/05/Loss-for-VAE-GAN/"/>
      <url>/2019/09/05/Loss-for-VAE-GAN/</url>
      
        <content type="html"><![CDATA[<h4 id="L1-loss-for-image-translation"><a href="#L1-loss-for-image-translation" class="headerlink" title="L1 loss for image translation"></a>L1 loss for image translation</h4><p>(Image-to-Image Translation with Conditional Adversarial Networks)[<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf</a>]</p><p>\begin{aligned}<br>\mathcal{L}_{L<em>1}(G)=\mathbb{E}\</em>{x, y, z}\left[|y-G(x, z)|_{1}\right]<br>\end{aligned}</p><p>L1 term to force low-frequency correctness, L1 alone leads to reason- able but blurry results ($\lamda = 100$) </p><h4 id="Mean-feature-matching-for-image-generation-in-general-setting"><a href="#Mean-feature-matching-for-image-generation-in-general-setting" class="headerlink" title="Mean feature matching for image generation in general setting"></a>Mean feature matching for image generation in general setting</h4><p>(CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training<br>)[<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.pdf</a>]</p><!-- \begin{aligned}\mathcal{L}\_{G D}=\frac{1}{2}\left\|\mathbb{E}\_{\boldsymbol{x} \sim P_{r}} f_{D}(\boldsymbol{x})-\mathbb{E}\_{\boldsymbol{z} \sim P_{\boldsymbol{z}}} f_{D}(G(\boldsymbol{z}))\right\|\_{2}^{2}\end{aligned} --><p>\begin{equation}<br>\mathcal{L}_{G D}=\frac{1}{2}\left|\mathbb{E}_{\boldsymbol{x} \sim P_{r}} f<em>{D}(\boldsymbol{x})-\mathbb{E}\</em>{\boldsymbol{z} \sim P_{\boldsymbol{z}}} f_{D}(G(\boldsymbol{z}))\right|_{2}^{2}<br>\end{equation}</p><p>to deal with the unstable gradient of G</p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> design of loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning without Forgetting</title>
      <link href="/2019/08/16/Learning-without-Forgetting/"/>
      <url>/2019/08/16/Learning-without-Forgetting/</url>
      
        <content type="html"><![CDATA[<h4 id="problem-definition"><a href="#problem-definition" class="headerlink" title="problem definition"></a>problem definition</h4><p>how to uing new data to train network while preserving the original capabilities</p><h4 id="comparable-methods"><a href="#comparable-methods" class="headerlink" title="comparable methods"></a>comparable methods</h4><ul><li><p>feature extraction: extracting feature from unchanged parameters trained on old tasks in training new branches for new tasks. however, the shared parameters fail to represent discriminative feature for new task.</p></li><li><p>fine-tuning FC: shared layers are fixed and finetuning the fc layers. however, the finetuned shared parameters degrade performance on previous tasks because there is not new guidance for original datasets.</p></li><li><p>joint training: upper bound of the lwf, need original datasets and new datasets.</p></li></ul><h4 id="learning-without-forgetting"><a href="#learning-without-forgetting" class="headerlink" title="learning without forgetting"></a>learning without forgetting</h4><ul><li>similar to joint training, don’t need original images and labels</li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multitask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MTL related Papers</title>
      <link href="/2019/07/23/MTL-related-Papers/"/>
      <url>/2019/07/23/MTL-related-Papers/</url>
      
        <content type="html"><![CDATA[<h5 id="Learning-a-Deep-ConvNet-for-Multi-label-Classification-with-Partial-Labels"><a href="#Learning-a-Deep-ConvNet-for-Multi-label-Classification-with-Partial-Labels" class="headerlink" title="Learning a Deep ConvNet for Multi-label Classification with Partial Labels"></a><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Durand_Learning_a_Deep_ConvNet_for_Multi-Label_Classification_With_Partial_Labels_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Learning a Deep ConvNet for Multi-label Classification with Partial Labels</a></h5><ul><li><p>novel: labeling strategies+ new classification loss for leanring from partial labels + curriculum-based missing labels prediction methods</p></li><li><p>overview of method: from learning label correlations between observed and unobserved partial labels to learn a accurate model used to predict missing labels with a curriculum-based approch.</p></li><li><p>detail: using fully-connected graph to model correlations between all categories. GNN (message update function[MLP] + hidden state update function[GRU]) + MTL framework, alternative minimized</p><p>GNN: nodes represent categoriesthe input is feature extracted from the CNN, output is the predicted labels </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multitask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Papers related Few-shot Image Generation</title>
      <link href="/2019/07/15/Paper-related-Image-Generation/"/>
      <url>/2019/07/15/Paper-related-Image-Generation/</url>
      
        <content type="html"><![CDATA[<h4 id="∆-encoder-an-effective-sample-synthesis-method-for-few-shot-object-recognition"><a href="#∆-encoder-an-effective-sample-synthesis-method-for-few-shot-object-recognition" class="headerlink" title="∆-encoder: an effective sample synthesis method for few-shot object recognition"></a><a href="http://papers.nips.cc/paper/7549-delta-encoder-an-effective-sample-synthesis-method-for-few-shot-object-recognition.pdf" target="_blank" rel="noopener">∆-encoder: an effective sample synthesis method for few-shot object recognition</a></h4><ul><li><p>datasets: miniImageNet, CIFAR100, CUB, Caltech-256, APY, SUN and AWA2</p></li><li><p>how to generate images: learning to extract transferable intra-class deformations between same-class pairs of training examples and using this deformations to generate samples conditioned on few provided examples. </p></li><li><p>how to aid classification task: constructing feeding augmented images into a simple linear N-class classifier (one dense layer followed by softmax) over about 1024 samples of each category</p></li><li><p>how to evaluate the effectiveness of generated images: using trained generative model to generate thousands of images conditioned on provided few shots and using those generated images to train a simple classifier (training fc layer with convolutional layers of vgg16 or resnet18 pre-trained on Imagenet, another way is to pre-train those convulutional layers by samples from all training categories). Finally, comparing the results with the accuracy of other few-shot classifiers.</p></li></ul><h4 id="DADA-Deep-Adversarial-Data-Augmentation-for-Extremely-Low-Data-Regime-Classification"><a href="#DADA-Deep-Adversarial-Data-Augmentation-for-Extremely-Low-Data-Regime-Classification" class="headerlink" title="DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification"></a><a href="https://arxiv.org/pdf/1809.00981.pdf" target="_blank" rel="noopener">DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification</a></h4><ul><li><p>how to generate images: using a class-conditional GAN with a novel loss to generate diverse and category-specific images.</p></li><li><p>how to aid classification task: coupling the generation process and classification process.  Using 2k loss to distinguish fake data from real data, also to predict the category of fake data and real data. </p></li><li><p>how to evaluate the effectiveness of generated images: experiments are not conducted in few-shot setting, instead sampling 50-1000 samples of each category to simulate extremely low data regime. comparing the results with other GAN methods, such as Improved-GAN.</p></li></ul><h4 id="FEW-SHOT-AUTOREGRESSIVE-DENSITY-ESTIMATION-TOWARDS-LEARNING-TO-LEARN-DISTRIBUTIONS"><a href="#FEW-SHOT-AUTOREGRESSIVE-DENSITY-ESTIMATION-TOWARDS-LEARNING-TO-LEARN-DISTRIBUTIONS" class="headerlink" title="FEW-SHOT AUTOREGRESSIVE DENSITY ESTIMATION: TOWARDS LEARNING TO LEARN DISTRIBUTIONS"></a><a href="https://arxiv.org/pdf/1710.10304.pdf" target="_blank" rel="noopener">FEW-SHOT AUTOREGRESSIVE DENSITY ESTIMATION: TOWARDS LEARNING TO LEARN DISTRIBUTIONS</a></h4><ul><li><p>how to generate images: modified pixelCNN to achieve few-shot density estimation and extend the model to generate natural images.</p></li><li><p>how to aid classification task: None, only density extimation NLL</p></li><li><p>how to evaluate the effectiveness of generated images: visualization of the generated images, comparing the results with the NLL of other pixelCNN methods.</p></li></ul><h4 id="The-Variational-Homoencoder-Learning-to-learn-high-capacity-generative-models-from-few-examples"><a href="#The-Variational-Homoencoder-Learning-to-learn-high-capacity-generative-models-from-few-examples" class="headerlink" title="The Variational Homoencoder:Learning to learn high capacity generative models from few examples"></a><a href="https://arxiv.org/pdf/1807.08919.pdf" target="_blank" rel="noopener">The Variational Homoencoder:Learning to learn high capacity generative models from few examples</a></h4><ul><li><p>how to generate images: modified VAE to produces a hierarchical latent variable model which better utilises latent variables.</p></li><li><p>how to aid classification task: classifying an example x the estimation of the expected conditional likelihood under the variational posterior.</p></li><li><p>how to evaluate the effectiveness of generated images: one-shot generation, one-shot classification and the value of likelihood.</p></li></ul><h4 id="Distribution-Matching-in-Variational-Inference"><a href="#Distribution-Matching-in-Variational-Inference" class="headerlink" title="Distribution Matching in Variational Inference"></a><a href="https://arxiv.org/pdf/1802.06847" target="_blank" rel="noopener">Distribution Matching in Variational Inference</a></h4>]]></content>
      
      
      <categories>
          
          <category> Few-shot learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot image generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Combination between GAN and VAE</title>
      <link href="/2019/07/09/Combination-between-GAN-and-VAE/"/>
      <url>/2019/07/09/Combination-between-GAN-and-VAE/</url>
      
        <content type="html"><![CDATA[<h3 id="Reference-paper"><a href="#Reference-paper" class="headerlink" title="Reference paper"></a>Reference paper</h3><ul><li><p><a href="https://arxiv.org/abs/1703.10155" target="_blank" rel="noopener">CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training</a></p></li><li><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Joint_Discriminative_and_Generative_Learning_for_Person_Re-Identification_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Joint Discriminative and Generative Learning for Person Re-identification</a></p></li></ul><h4 id="the-detail-of-CVAE-GAN"><a href="#the-detail-of-CVAE-GAN" class="headerlink" title="the detail of CVAE-GAN"></a>the detail of CVAE-GAN</h4><ul><li><p>solved problem: synthesizing images in fine-grained categories(??)</p></li><li><p>method: modeling an image as a composition of label and latent attributesin a probabilistic model(??)</p></li><li><p>novalty: loss design for the discriminator and the generator, the classifier. an encoder network is used to learn the relationship between the latent space and image space.</p></li><li><p>new objective loss: minimizing the l2 distance of the mean feature and the real data, especially, for multi-class image generation, the generated image need to match the mean feature of real data of that category (<font color="red">similar to our recognition mudule</font>, which can reduce mode collapse??).</p></li><li><p>to keep the diversity of generated samples, combining the vae and gan, introducing encoder network to map real image to the latent vector. in this way, we explicitly set up the relationship between latent space and image sapce.</p></li><li><p>CVAE-GAN: the encoder network E(x-&gt;z), the generative network G(z-&gt;\tilt{x}), the discriminatove network D(distinguish x from \tilt{x}), the classifier network C(measure the class probability of the data), cascaded together and trained end-to-end.  </p></li><li><p>application: generation, inpainting, attribute morphing.</p></li><li><p>l2 loss and pari-wise feature matching enable the G to generate structure-preserving samples.</p></li></ul><h6 id="formulation"><a href="#formulation" class="headerlink" title="formulation"></a>formulation</h6><ul><li><p>mean feature matching in basic GAN (G+D): extracting feature from the discriminator to form mean feature matching to optimize the generator loss(L_GD). also can combining the feature form multi-layer.(<font color="red">can be applied in our dagan</font>)</p></li><li><p>mean feature mathcing in classifier(G+C): in the classifier, extracting feature from the classifier to form mean feature matching to optimize the generator loss(L_GC),also can combining the feature form multi-layer.(<font color="red">can be applied in our dagan</font>)</p></li><li><p>pair-wise feature matching: combining l2 loss and pair-wise feature matching loss</p></li></ul><h4 id="the-detail-of-Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification"><a href="#the-detail-of-Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification" class="headerlink" title="the detail of Joint Discriminative and Generative Learning for Person Re-identification"></a>the detail of Joint Discriminative and Generative Learning for Person Re-identification</h4><ul><li><p>solved problem: intra-class variations across different cameras.</p></li><li><p>method: improve learned reid embedding by better using agmented images with a joint framwork that couple the data generation and reid learning end-to-end. the generative module (appearance code and structure code) and the discrinative (sharing appearance code with the generative module). so appearance code plays a role of category-related embedding.</p></li><li><p>coupling the generative module for image generation and the discriminative module for reid learning.</p></li></ul><ul><li><p>reason: let the classification module can ‘see’ these variations (intr-class variations and inter-class variations) during training process.</p></li><li><p>generate images by swith the appearance code and structure code given the same identities or the different identities.</p></li></ul><h6 id="formulation-1"><a href="#formulation-1" class="headerlink" title="formulation"></a>formulation</h6><ul><li><p>generative module: self-identity generation, cross-identity generation.</p></li><li><p>discriminative module: primary feature learning, fine-grained feature mining, which is codesigned with the generative module.</p></li></ul><h5 id="my-thinking"><a href="#my-thinking" class="headerlink" title="my thinking"></a>my thinking</h5><ul><li><p>the generative module is resposible to generate images that contain significant intra-class variations (imagenet dataset including different background, FIGR-8 dataset including different style, omniglot dataset including different style, vggFace including different ages, skin colors). the generative module can be designed as two part including category-related code and category-unrelated code. the generative module is responsible to generate high-quality cross-category images. the category-related module can encode category related semantics, while the category-unrelated module can enclose geometry and posion related structure information</p></li><li><p>the discriminative module accept the cross-category images generated by the generative module to improve the discriminative module. additionaly, the classification module can be combined with the discriminative module, so the whole process can make contributions to the classification.</p></li><li><p>‘realism’: generated images should close domain gap between the genrated images ans the real ones. </p></li><li><p>‘diversity’: generated images should contain sufficient diversity cover ‘unseen’ intra-class variations.</p></li><li><p>the generative process should align with the classification task to enlarge the gain from the generated images.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN and VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CVAE-GAN</title>
      <link href="/2019/07/01/CVAE-GAN/"/>
      <url>/2019/07/01/CVAE-GAN/</url>
      
        <content type="html"><![CDATA[<h4 id="Paper-Understanding"><a href="#Paper-Understanding" class="headerlink" title="Paper Understanding"></a>Paper Understanding</h4><ul><li><a href="https://arxiv.org/abs/1703.10155" target="_blank" rel="noopener">paper links</a></li></ul><h5 id="problem-method-effect"><a href="#problem-method-effect" class="headerlink" title="problem,method,effect"></a>problem,method,effect</h5><ul><li><p>problem: synthesizing images in fine-grained categories.</p></li><li><p>method: gan+vae<br>gan: asymmetric loss(G_loss[mean discrepancy] + D_loss[cross entropy])<br>vae: encoder(relatio between latent space and real images) -&gt; pairwise feature matching -&gt; keep structure of real images</p></li><li><p>effect: generating realistic and diverse samples with fine-grained category labels.</p></li></ul><h5 id="detail-explanation"><a href="#detail-explanation" class="headerlink" title="detail explanation"></a>detail explanation</h5><ul><li><p>the loss of generator (mean feature matching): minimizing the distance of mean feature to the real images, reduce mode collapse. using an intermediate layer of the discriminator to extract feature for real images and generated fake images. before the last fc layer of the discriminator.</p></li><li><p>conditional image generation: using mean feature matching.</p></li><li><p>pairwise feature matching</p></li></ul><h6 id="network-design"><a href="#network-design" class="headerlink" title="network design"></a>network design</h6><p>classification both used in the generator and the discriminator</p><p>KL loss is used in the generator</p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> combination </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper Reading Related GAN</title>
      <link href="/2019/06/19/Simialr-to-DAGAN/"/>
      <url>/2019/06/19/Simialr-to-DAGAN/</url>
      
        <content type="html"><![CDATA[<h3 id="Similar-to-DAGAN"><a href="#Similar-to-DAGAN" class="headerlink" title="Similar to DAGAN"></a>Similar to DAGAN</h3><h4 id="DAGAN"><a href="#DAGAN" class="headerlink" title="DAGAN"></a><a href="https://arxiv.org/pdf/1711.04340.pdf" target="_blank" rel="noopener">DAGAN</a></h4><p>detailed understanding <a href="https://pan.baidu.com/s/1jwQoUsxwjv6Mv7axS5bRRw" target="_blank" rel="noopener">ppt</a></p><h4 id="Class-Distinct-and-Class-Mutual-Image-Generation-with-GANs"><a href="#Class-Distinct-and-Class-Mutual-Image-Generation-with-GANs" class="headerlink" title="Class-Distinct and Class-Mutual Image Generation with GANs"></a><a href="https://arxiv.org/pdf/1811.11163.pdf" target="_blank" rel="noopener">Class-Distinct and Class-Mutual Image Generation with GANs</a></h4><ul><li><p>problem definition: typically in class-conditional image generation, it is assume that there are no intersections between classes, gan model are optimized to fit discrete class labels. Actually, there are data with ambiguous bundaries in real world in class-overlapping settings.</p></li><li><p>aims: class-distinct and class-mutual image generation is desigend to selectively generate class-distinct and class-mutual images in a controllable manner.</p></li><li><p>effects: propose novel families of GANs called class-mixture GAN and class-posterior GAN, mainly redisgining the generator prior and the objective function with auxiliary classifier.</p></li><li><p>destail: weak supervision(binary class labels), focusing on class specificity, different from typical class-wise interpolation.</p><ol><li>based on AC-GAN(class-conditional GAN): loss_gan + loss_classification</li><li>class label $y_g = r_0y_0 + r_1y_1 + … + r_iy_i$, interpolation in label space ($r_i$ is from Dirichlet distribution)</li><li>posterior: r_i = C(y|x_r)</li><li>visualization: vary y continuously between classes to generate images</li></ol></li><li><p>conclusion: latent space is effective, simple version in y space, we selected in image space</p></li></ul><h4 id="Attention-based-Fusion-for-Multi-source-Human-Image-Generation"><a href="#Attention-based-Fusion-for-Multi-source-Human-Image-Generation" class="headerlink" title="Attention-based Fusion for Multi-source Human Image Generation"></a><a href="https://arxiv.org/pdf/1905.02655v1.pdf" target="_blank" rel="noopener">Attention-based Fusion for Multi-source Human Image Generation</a></h4><ul><li><p>problem: conditioned on a target pose and a set X of source appearance images</p></li><li><p>effect: complementary images of the same person which usually available at training and at testing time. (similar idea)</p></li><li><p>novelty: attention-mechanism which selects relevant information from different source image regions. (similar)</p></li><li><p>U-Net: attention are integrated into U-Net, attention module which is spatial-attention nodule(channel independent), using Enconder-Decoder construction.</p></li><li><p>dataset: market1501(source appearance) + DeepFashion(pose)</p></li></ul><h3 id="GAN-applied-in-other-leanring-tasks"><a href="#GAN-applied-in-other-leanring-tasks" class="headerlink" title="GAN applied in other leanring tasks"></a>GAN applied in other leanring tasks</h3><h4 id="Progressive-Pose-Attention-Transfer-for-Person-Image-Generation"><a href="#Progressive-Pose-Attention-Transfer-for-Person-Image-Generation" class="headerlink" title="Progressive Pose Attention Transfer for Person Image Generation"></a><a href="https://arxiv.org/pdf/1904.03349.pdf" target="_blank" rel="noopener">Progressive Pose Attention Transfer for Person Image Generation</a></h4><h4 id="Attention-based-Fusion-for-Multi-source-Human-Image-Generation-1"><a href="#Attention-based-Fusion-for-Multi-source-Human-Image-Generation-1" class="headerlink" title="[Attention-based Fusion for Multi-source Human Image Generation"></a>[Attention-based Fusion for Multi-source Human Image Generation</h4><p>](<a href="https://arxiv.org/pdf/1905.02655v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.02655v1.pdf</a>)</p><h4 id="Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification"><a href="#Joint-Discriminative-and-Generative-Learning-for-Person-Re-identification" class="headerlink" title="Joint Discriminative and Generative Learning for Person Re-identification"></a><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Joint_Discriminative_and_Generative_Learning_for_Person_Re-Identification_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Joint Discriminative and Generative Learning for Person Re-identification</a></h4><ul><li><p>problem: significant intra-class variations across different cameras.</p></li><li><p>motivation: augamenting data to enhance the invariance to input changes, improving learned re-id embeddings by better leveraging the generated data, which achieved by jointly coupling re-id learning and data augmentation end-to-end.</p></li><li><p>important methods: generative module including appearance code and structure code + discriminative module sharing appearance encoder with generative module. switching the appearance codes or appearance codes to generate high-quality cross-id images which are on line fed back to the appeance encoder and used to improve the discriminative module.</p></li></ul><p>— my understanding: analyzing the essence of problem (for example, intra-class invariance in reid, maybe intra-class invariance is important for all classification tasks), generating corresponding images to improve the performance (generate cross-id images by appearance code and structure code, maybe using two code focused on different content(appeance and structure two codes for reid problem),also it can be beneficial to generate cross-class images).</p><ul><li>method: </li></ul><p>for generative module: leveraging latent code (appeance and structure)</p><p> $ self-identity: encouraging the appeance encoder to pull appeance code by generating images with seeing cross-images in the same identity, so that intra-class feature variance are reduced, similar to 1-way-1-shot</p><p> $ cross-identity: reconstruct appeance and structure codes, soft label, adversarial training, similar to 2-way-1-shot</p><p> for discriminative module: (images and generated images are fed into discriminative part)</p><p> $ primary feature learning: teacher-student type supervision based on soft label</p><p> $ fine-grained feature mining: multi-task learning style</p><h4 id="RelGAN-Multi-Domain-Image-to-Image-Translation-via-Relative-Attributes"><a href="#RelGAN-Multi-Domain-Image-to-Image-Translation-via-Relative-Attributes" class="headerlink" title="RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes"></a><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_RelGAN_Multi-Domain_Image-to-Image_Translation_via_Relative_Attributes_ICCV_2019_paper.pdf" target="_blank" rel="noopener">RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes</a></h4><ul><li>matching: discriminator</li><li>interpolation: discriminator</li></ul><h4 id="Guided-Image-to-Image-Translation-with-Bi-Directional-Feature-Transformation"><a href="#Guided-Image-to-Image-Translation-with-Bi-Directional-Feature-Transformation" class="headerlink" title="[Guided Image-to-Image Translation with Bi-Directional Feature Transformation"></a>[Guided Image-to-Image Translation with Bi-Directional Feature Transformation</h4><p>](<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/AlBahar_Guided_Image-to-Image_Translation_With_Bi-Directional_Feature_Transformation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/papers/AlBahar_Guided_Image-to-Image_Translation_With_Bi-Directional_Feature_Transformation_ICCV_2019_paper.pdf</a>)</p><ul><li>bidirectional feature extraction</li><li>pixel-to-pixel adain</li></ul><h4 id="AttentionGAN-Unpaired-Image-to-Image-Translation-using-Attention-Guided-Generative-Adversarial-Networks"><a href="#AttentionGAN-Unpaired-Image-to-Image-Translation-using-Attention-Guided-Generative-Adversarial-Networks" class="headerlink" title="[AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks"></a>[AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks</h4><p>](<a href="https://arxiv.org/pdf/1911.11897.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.11897.pdf</a>)</p><ul><li><p>abstract: problem(limited in transforming high-level semantics of input images) -&gt; reson(perceive the most discriminative semantic parts between the source and target domains) -&gt; method(identify the most discriminative semantic objects and minimize changes of unwanted parts for semantic manipulation problems without using extra data and models)</p></li><li><p>introduction: problem(change unwanted parts in the translation, and can also be easily affected by background changes) -&gt; other solutions(ContrastGAN uses object-mask annotations, mask-guided generation train an extra model to detect the object masks and then employ them for image generation) -&gt; Attention GAN (only focus on the foreground of the target do- main and preserve the background of the source domain effectively. the proposed generator learns both foreground and background attentions. It uses the foreground attention to select from the generated out- put for the foreground regions, while uses the background attention to maintain the background informa- tion from the input image)</p></li><li><p>attention: detail</p></li></ul><h4 id="ELEGANT-Exchanging-Latent-Encodings-with-GAN-for-Transferring-Multiple-Face-Attributes"><a href="#ELEGANT-Exchanging-Latent-Encodings-with-GAN-for-Transferring-Multiple-Face-Attributes" class="headerlink" title="ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes"></a><a href="https://arxiv.org/pdf/1803.10562.pdf" target="_blank" rel="noopener">ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes</a></h4><ul><li>abstract: problem(transfer multiple face attributes simultaneously, low quality of generated images) -&gt; method(receives two images of opposite attributes as inputs, All the attributes are encoded in a disentangled manner in the latent space, which enables us to manipulate several attributes simultaneously, multi-scale discrimina- tors for adversarial training, it can even generate high-quality images with finer details and less artifacts)</li></ul><h4 id="StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation"><a href="#StarGAN-Unified-Generative-Adversarial-Networks-for-Multi-Domain-Image-to-Image-Translation" class="headerlink" title="StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"></a><a href="https://arxiv.org/pdf/1711.09020.pdf" target="_blank" rel="noopener">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</a></h4><ul><li>abstract: problem(limited scalability and robustness in handling more than two domains) -&gt; reson -&gt; method(simultaneous training of multiple datasets with different domains within a single network)</li></ul><h4 id="StarGAN-v2-Diverse-Image-Synthesis-for-Multiple-Domains"><a href="#StarGAN-v2-Diverse-Image-Synthesis-for-Multiple-Domains" class="headerlink" title="StarGAN v2: Diverse Image Synthesis for Multiple Domains"></a><a href="https://arxiv.org/pdf/1912.01865.pdf" target="_blank" rel="noopener">StarGAN v2: Diverse Image Synthesis for Multiple Domains</a></h4><ul><li>abstract: problem(diversity of generated images, scalability over multiple domains) -&gt; method(we start from StarGAN and replace its domain label with our proposed domain- specific style code that can represent diverse styles of a specific domain. we introduce two mod- ules, a mapping network and a style encoder. The mapping network learns to transform random Gaussian noise into a style code, while the encoder learns to extract the style code from a given reference image. Considering multiple domains, both modules have multiple output branches, each of which provides style codes for a specific domain.)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pedestrain Datasets</title>
      <link href="/2019/06/14/Pedestrain-Datasets/"/>
      <url>/2019/06/14/Pedestrain-Datasets/</url>
      
        <content type="html"><![CDATA[<h4 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h4><p><a href="https://craigie1996.github.io/2018/05/11/Pedestrian-Attribute-Recognition-调研笔记/" target="_blank" rel="noopener">RAP, PETA, PA-100K</a></p><p><a href="https://www.cnblogs.com/wangxiaocvpr/p/10162679.html" target="_blank" rel="noopener">Most dataset and papers links</a></p><p><a href="https://blog.csdn.net/Code_Mart/article/details/87721803" target="_blank" rel="noopener">Most dataset</a></p><h4 id="Simple-introduction"><a href="#Simple-introduction" class="headerlink" title="Simple introduction"></a>Simple introduction</h4><ul><li><p><a href="https://exhibits.stanford.edu/data/catalog/tb980qz1002" target="_blank" rel="noopener">Clothing Attributes Dataset</a>: cloth style(7 categories), ensurely used. accuracy (about 55%) </p></li><li><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/PETA.html" target="_blank" rel="noopener">PETA Dataset</a>: low-resolution, multi-class(age 4 categories), download. accuracy(16,31,46,&gt;61: 83.8, 78.8,76.4,89.0),19,000 images, about 0.82</p></li><li><p><a href="https://jurie.users.greyc.fr/datasets/hat.html" target="_blank" rel="noopener">Database of Human Attributes (HAT)</a>: high-resolution, age(multi-classes), having sent email</p></li><li><p><a href="https://arxiv.org/pdf/1603.07054.pdf" target="_blank" rel="noopener">RAP Dataset</a>: not high-resolution, 3-multi-classes(age 3 categories), waiting to send email.</p></li><li><p>PA-100K Dataset: binary</p></li><li><p>WIDER Attribute Dataset: multi-people</p></li><li><p>Parse27k Dataset: squence, oritation(multi-class), not so much related</p></li><li><p>CRP Dataset: squence, not human centre</p></li><li><p>Berkeley-Attributes of People: binary</p></li><li><p>Deepfashion dataset: cloth related, 289,222 - 50 classes cloth categories: 82%</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Human Attributes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> datasets </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Variations of U-Net</title>
      <link href="/2019/06/10/U-Net/"/>
      <url>/2019/06/10/U-Net/</url>
      
        <content type="html"><![CDATA[<p>refer to <a href="https://zhuanlan.zhihu.com/p/57530767" target="_blank" rel="noopener">Unet blog</a></p>]]></content>
      
      
      <categories>
          
          <category> Generative Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> variation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Interpolation in Latent Space</title>
      <link href="/2019/06/05/Interpolation-in-Latent-Space/"/>
      <url>/2019/06/05/Interpolation-in-Latent-Space/</url>
      
        <content type="html"><![CDATA[<h3 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h3><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Hsin-Ying_Lee_Diverse_Image-to-Image_Translation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Diverse Image-to-Image Translation via Disentangled Representations</a></p><ul><li>novalty</li></ul><p>diverse translation between two collections of images without aligned training pairs</p><p><a href="https://arxiv.org/pdf/1511.06434.pdf" target="_blank" rel="noopener">UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL<br>GENERATIVE ADVERSARIAL NETWORKS</a></p><p><a href="https://arxiv.org/pdf/1711.05415.pdf" target="_blank" rel="noopener">DNA-GAN: LEARNING DISENTANGLED REPRESEN- TATIONS FROM MULTI-ATTRIBUTE IMAGES</a></p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> interpolation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Normalization_for_Input_of_GAN</title>
      <link href="/2019/05/27/Normalization-for-Input-of-GAN/"/>
      <url>/2019/05/27/Normalization-for-Input-of-GAN/</url>
      
        <content type="html"><![CDATA[<h3 id="Normalization-methods-for-the-input-of-GAN-and-visualization"><a href="#Normalization-methods-for-the-input-of-GAN-and-visualization" class="headerlink" title="Normalization methods for the input of GAN and visualization"></a>Normalization methods for the input of GAN and visualization</h3><h4 id="simple"><a href="#simple" class="headerlink" title="simple"></a>simple</h4><p>input<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def normalization(data):</span><br><span class="line">    data = data - 127.5</span><br><span class="line">    data = data / 127.5</span><br><span class="line">    <span class="built_in">return</span> data</span><br></pre></td></tr></table></figure></p><p>visualization<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fake = G(z)</span><br><span class="line">def unnormalization(data):</span><br><span class="line">    data = data * 127.5</span><br><span class="line">    data = data + 127.5</span><br><span class="line">    <span class="built_in">return</span> data</span><br><span class="line">visual_image = unnormalization(fake)</span><br></pre></td></tr></table></figure></p><h4 id="mean-and-std"><a href="#mean-and-std" class="headerlink" title="mean and std"></a>mean and std</h4><p>input (using the mean and std of training set)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def normalization(self):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Normalizes our data, to have a mean of 0 and sdt of 1</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        self.mean = np.mean(self.x_train)</span><br><span class="line">        self.std = np.std(self.x_train)</span><br><span class="line">        self.x_train = (self.x_train - self.mean) / self.std</span><br><span class="line">        self.x_val = (self.x_val - self.mean) / self.std</span><br><span class="line">        self.x_test = (self.x_test - self.mean) / self.std</span><br></pre></td></tr></table></figure></p><p>visualization(using the mean and std of training set)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def unnormalization(self,fake):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Normalizes our data, to have a mean of 0 and sdt of 1</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        self.mean = np.mean(self.x_train)</span><br><span class="line">        self.std = np.std(self.x_train)</span><br><span class="line">        visual_image = fake*self.std + self.mean</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> GAN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> data preprocess </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Related Work about Few-shot Image Generation</title>
      <link href="/2019/05/21/Related-Work-about-Few-shot-Image-Generation/"/>
      <url>/2019/05/21/Related-Work-about-Few-shot-Image-Generation/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GAN Metrics</title>
      <link href="/2019/05/07/GAN-Metrics/"/>
      <url>/2019/05/07/GAN-Metrics/</url>
      
        <content type="html"><![CDATA[<p>Paper <a href="https://arxiv.org/pdf/1806.07755.pdf" target="_blank" rel="noopener">An empirical study on evaluation metrics of generative adversarial networks</a><br>Notes<a href="https://www.jianshu.com/p/fc5526b1fe3b" target="_blank" rel="noopener">gan metrics notes</a></p><p>There are two categories evaluation metric:</p><ol><li>sample-based</li><li>other</li></ol><h3 id="Sample-based-metrics"><a href="#Sample-based-metrics" class="headerlink" title="Sample-based metrics"></a>Sample-based metrics</h3><h4 id="The-Inception-Score"><a href="#The-Inception-Score" class="headerlink" title="The Inception Score"></a>The Inception Score</h4><ul><li>how: inception-v3 trained on Imagenet, the output of softmax, the input is the generated images.</li><li>evaluation:<ul><li>quality: the conditional possibility distriution $p(y | \mathbf{x})$, which is the probability of the vector of softmax layer (1000) and explicitly belong to which category.</li><li>diversity: the marginal possibility distriution $p(y)$, close to uniform distribution, which represents that the number of generated samples in each categoty is comparable.</li></ul></li><li>calculation:<br>\begin{equation}<br>\mathbf{I S}(G)=\exp \left(\mathbb{E}_{\mathbf{x} \sim p_{g}} D_{K L}(p(y | \mathbf{x}) | p(y))\right)<br>\end{equation}</li><li>the larger, the better. quality and diversity.</li></ul><h4 id="The-Mode-Score"><a href="#The-Mode-Score" class="headerlink" title="The Mode Score"></a>The Mode Score</h4><ul><li>how: improvement of The Inception Score, considering the distribution of real images</li><li>evaluation:<ul><li>quality:</li><li>diversity:</li><li>dissimilarity between the real distriution and fake distribution</li></ul></li><li>calcualtion</li><li>the larger, the better. quality and diversity.</li></ul><h4 id="Kernel-MMD"><a href="#Kernel-MMD" class="headerlink" title="Kernel MMD"></a>Kernel MMD</h4><ul><li>how: improved measurement to dissimilarity between the real distriution and fake distribution for some fixed kernel function.</li><li>evaluation: real samples and fake samples, lower MMD means close</li><li>the smaller, the better. quality.</li></ul><h4 id="The-Wasserstein-distance"><a href="#The-Wasserstein-distance" class="headerlink" title="The Wasserstein distance"></a>The Wasserstein distance</h4><ul><li>how: further improved measurement to dissimilarity between the real distriution and fake distribution</li><li>evaluation: real samples and fake samples, lower Wasserstein distance means close</li><li>the smaller, the better. depending on the selection of feature space.</li></ul><h4 id="The-Fréchet-Inception-Distance-FID"><a href="#The-Fréchet-Inception-Distance-FID" class="headerlink" title="The Fréchet Inception Distance (FID)"></a>The Fréchet Inception Distance (FID)</h4><ul><li>how: convulutional network is used to extract feature, modeling the Gaussian random variables with mean value and covariance from real and fake feature space.</li><li>evaluation: measuring the two Gaussian distribution</li><li>the smaller, the better. efficiency.</li></ul><h4 id="The-1-Nearest-Neighbor-classifier"><a href="#The-1-Nearest-Neighbor-classifier" class="headerlink" title="The 1-Nearest Neighbor classifier"></a>The 1-Nearest Neighbor classifier</h4><ul><li>how: assessing the two distriution are identical.</li><li>evaluation: about 50% leave-one-out, the accuracy of generated images</li><li>several measurement index: accuracy, recall ….. ideal evaluation metric</li></ul><h3 id="other-metrics"><a href="#other-metrics" class="headerlink" title="other metrics"></a>other metrics</h3>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> evaluation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper Writting</title>
      <link href="/2019/05/04/Paper-Writting/"/>
      <url>/2019/05/04/Paper-Writting/</url>
      
        <content type="html"><![CDATA[<h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><p>well-organized: storyline -&gt; abstract and introduction -&gt; methods -&gt; related work -&gt; experiment</p><ol><li><p>Storyline</p><ul><li>Make sure that the storyline is reasonable and fluent before your writting.</li></ul></li><li><p>Abstract &amp; introduction: </p><ul><li>Based on the refined storyline, to form abstract and introduction, a extension of abstract.</li></ul></li><li><p>Related work</p><ul><li>It related to our selected baselines, so I think it belong to the survey work before this research project, and to update it when you read related papers. </li><li>Another important point is to divide the sub-fields of your current research topic, you can find related works accurately and comprehensively.</li></ul></li><li><p>Methods</p><ul><li>To fully understand the proposed method, generally the method is inspired by other papers or books. </li><li>It’s important to know about what the method is and how it can deal with your problem. </li><li>More importantly, forming core combined method based on those refered algorithm can help us to explain our story line in therotical analysis.</li></ul></li><li><p>Experiment</p><ul><li>Comparion is important and classify those baselines into serveral group based on its relateness. </li><li>To ablate the role of parameters in experiments and introduce ‘special case’ to outline the effect of the proposed method.</li></ul></li></ol><h3 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h3><p>well-written: abstract -&gt; introduction -&gt; method -&gt; experiment -&gt; conclusion</p><ol><li><p>Abstract </p><ul><li>Describing the scenario of your reserch problem.</li><li>The shortcoming of the existing methods.</li><li>Our inspiration, which means what sub-field we selected as research point for this prolem.</li><li>What problem we will encounter in using this sub-field method, eg.. 1 …, 2… </li><li>To highly summarize how our approach to solve thos problems.</li><li>To brief describe the experimental results.</li></ul></li><li><p>Introduction</p><ul><li>From large research point (MTL) to sub-field research point (MTL with disjoint datasets).</li><li>Describing this scenario and the disadvantage of existing methods.</li><li>Our method, explain it.</li><li>Concluding contributios.</li></ul></li><li><p>Related work</p><ul><li>Directly related work.</li><li>Methods related works.</li></ul></li><li><p>Method</p><ul><li>To describe the problem in mathmatical method (inputs [images and labels], trainable parameters).</li><li>Inspiration work</li><li><font color="red">Hierarchical method description</font>, for example, data selection(confident [confidence score, local density], distribution), label vector interpolation.</li></ul></li></ol><ol><li><p>Experiment</p><ul><li>Datasets: split rule (make sure before experiment).</li><li>Comparison: directly related to the part of related work, brief how you reproduce those baseline in your prpblem scenario. and then to analyze the result to outligh the advantage of the proposed method.</li><li>Ablation: explaining the introduced hyper-parameter, surely, it’s necessary to classify those ‘special case’ to facilate the description.</li><li>Considering the qualitative analysis if possible.</li></ul></li><li><p>Conclusion</p><ul><li>Highly conclude what the proposed method and how it deal with problem.</li><li>The demonstration of the experimental results.</li></ul></li></ol><h3 id="Accumulated-core-experience"><a href="#Accumulated-core-experience" class="headerlink" title="Accumulated core experience"></a>Accumulated core experience</h3><ol><li><p><font color="red">Paper organization</font>: overall organization and the organization of each part.</p></li><li><p><font color="red">Hierarchical description in each part</font>: logical description in each paragragh.</p></li><li><p><font color="red">Expressions of mathematical formulas</font>: unification of superscript and subscript in formula, and refining it during writting.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Writting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> experience </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few-shot Settings</title>
      <link href="/2019/04/29/Few-shot-Setings/"/>
      <url>/2019/04/29/Few-shot-Setings/</url>
      
        <content type="html"><![CDATA[<p>Taking Omniglot dataset as an example.</p><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p><a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf" target="_blank" rel="noopener">Matching networks for one shot learning</a></p><ul><li><p>datasets split </p><ol><li>classes split: total 1623 classes, 1200 classes in training set, 300 classes in validation set, the remaining classes in testing set.</li><li>episode: support images[batch_size, 5way * 5shot, image_size, image_size,1], target images[batch_size, 5shot, image_size, image_size,1]), batch_size is adjustable.</li></ol></li><li><p>training:</p><ol><li>considering matching between support images and target images</li><li>training and validation</li><li>training set for parameter update</li><li>validation set is used for evaluation the trained model, if accuracy<br>parameter update: parameters update based on matching classification accuracy</li></ol></li><li><p>testing:</p><ol><li>without parameter update</li><li>evaluating the classification of target images in testing set.</li></ol></li></ul><h2 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h2><p><a href="https://arxiv.org/pdf/1901.02199.pdf" target="_blank" rel="noopener">FIGR: Few-shot Image Generation with Reptile</a></p><ul><li><p>datasets split</p><ol><li>classes split: total 1623 classes, 20 classes in testing set and the remaining classes in training set.</li><li>episode: 1way * 4shot [4,image_size, image_size,1], batch_size(default=1) is unadjustable</li></ol></li><li><p>training:</p><ol><li>considering the optimization procedure from meta training to inner training</li><li>training and validation</li><li>training procedure: optimization</li></ol></li><li><p>testing:</p><ol><li>without parameter update</li><li>evaliating generator loss, discriminator loss, generated images in testing set.</li></ol></li></ul><p><a href="https://openreview.net/pdf?id=HyCRyS9gx" target="_blank" rel="noopener">FAST ADAPTATION IN GENERATIVE MODELS WITH GENERATIVE MATCHING NETWORKS</a></p><ul><li><p>dataset split:</p><ol><li>total 50 alphabets, 30 alphabets for training and the remaining for testing.</li><li>episode: up to 2way * sampled instance (10), batch size is adjustable.</li></ol></li><li><p>training:</p><ol><li>considering the total loss from generation model (likelihood of the prior and generated images) and recognition model (likelihood of the generated prior).</li><li>training and testing</li><li>minimizing the loss (ll_gen_prior+ll_gen_img-ll_rec_prior), the total loss of testing is regarded as evaluation measurement.</li></ol></li><li><p>testing:</p><ol><li>testing set: calculating the total loss (different calculation method)</li><li>recondtruction: recontructed images based on trained samples (evaluaing the training intermediate results)</li><li>generation: fedding test set, generated new image without training</li><li>classification: two different methods. </li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> Setting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few-shot Datasets</title>
      <link href="/2019/04/28/Few-shot-Datasets/"/>
      <url>/2019/04/28/Few-shot-Datasets/</url>
      
        <content type="html"><![CDATA[<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><h3 id="FIGR-8"><a href="#FIGR-8" class="headerlink" title="FIGR-8"></a>FIGR-8</h3><ul><li>grey</li><li>18,409 classes</li><li>total 1,548,944 images</li><li>each class contains at least 8 images</li><li>unbalance</li></ul><h3 id="Omniglot"><a href="#Omniglot" class="headerlink" title="Omniglot"></a>Omniglot</h3><ul><li>binary</li><li>50 alphabets</li><li>1623 unique type of characters</li><li>20 samples of each characters</li></ul><p>It contains 1623 unique type of characters originating from 50 alphabets, each of which has been handwritten 1 time by 20 different individuals</p><h2 id="Datasets-setting-in-different-paper"><a href="#Datasets-setting-in-different-paper" class="headerlink" title="Datasets setting in different paper"></a>Datasets setting in different paper</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p><a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf" target="_blank" rel="noopener">Matching networks for one shot learning</a></p><ul><li>5-way-5-shot </li><li>support images (batchsize, 5 way * 5 shot, 1, 32, 32) + target images(batchsize, 5 shot from different 5 ways)</li></ul><h3 id="Generation"><a href="#Generation" class="headerlink" title="Generation"></a>Generation</h3><p><a href="https://arxiv.org/pdf/1901.02199.pdf" target="_blank" rel="noopener">FIGR: Few-shot Image Generation with Reptile</a></p><ul><li><p>FIGR-8: The training classes where all 18, 409 classes minus 50 ran- domly sampled classes for the test set; (batchsize=4,1,32,32) from same class.</p></li><li><p>Omniglot: The training classes where all 1623 characters in the dataset minus 20 randomly sampled character classes for the test set; (batchsize=4,1,32,32) from same class.</p></li></ul><p><a href="https://openreview.net/pdf?id=HyCRyS9gx" target="_blank" rel="noopener">FAST ADAPTATION IN GENERATIVE MODELS WITH GENERATIVE MATCHING NETWORKS</a></p><ul><li>Omniglot: (batchsize, 20 samples from up to 2 classes, 28, 28)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Datasets </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VAE-GAN</title>
      <link href="/2019/04/24/VAE-GAN/"/>
      <url>/2019/04/24/VAE-GAN/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Comparion between VAE and GAN</title>
      <link href="/2019/04/23/Comparion-between-VAE-and-GAN/"/>
      <url>/2019/04/23/Comparion-between-VAE-and-GAN/</url>
      
        <content type="html"><![CDATA[<h3 id="Similarity"><a href="#Similarity" class="headerlink" title="Similarity"></a>Similarity</h3><ul><li><p>Both aim at constructing a model that can map latent variabels $z$ to object data $p_{data}$. specifically, the trained model $\mathbf{X} = g(\mathbf{Z})$ where $Z$ is general distribution such as normal distribution and gaussian distribution, $\mathbf{X}$ represents the probability distribution of training data. So they both aim at <font color="red">distribution transformation</font>.</p></li><li><p>Both have generative problem that it’s difficult to obtain distribution expression of generated distribution and true distribution, only samples from two distribution are available.</p></li></ul><ul><li>$KL$ divergence only be applied to calculte distribution differene on the condition that complete distribution expressions are provided, so it is unapplicable in this scenario.</li></ul><h3 id="Difference"><a href="#Difference" class="headerlink" title="Difference"></a>Difference</h3><ul><li><p><font color="red">Measurement method</font>: manmade measurement rule for VAE while this measurement rule of GAN is trained by neural network.</p></li><li><p>GAN: proposed to leverage deep neural networks to measure distribution difference because there isn’t suitable measurement method.</p></li><li><p>VAE: adopt a roundabout skill to leverage $KL$ divergence.</p></li></ul><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><h4 id="Important-points"><a href="#Important-points" class="headerlink" title="Important points"></a>Important points</h4><ul><li>The detail is here <a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">VAE’s blog</a> </li></ul><p><img src="/imgs/vae.jpg" alt="&#39;VAE&#39;"></p><ul><li><p><font color="red">Notes</font>: each sample has a gaussian distribution constructed by multivariate Gaussion and then obtain $z$ by sampling it from this distribution:<br>\begin{equation}<br>\log q_{\phi}\left(\mathbf{z} | \mathbf{x}^{(i)}\right)=\log \mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)} \mathbf{I}\right)<br>\end{equation}</p></li><li><p><font color="red">Notes</font>: $log \boldsymbol{\mu_k} = f_1(x_k), \boldsymbol{\sigma}^{2(k)} = f_2(x_k)$ which are both fitted by neural network.</p></li><li><p><font color="red">Noise</font> from constructed $z$ can be calculated from $\sigma$ which can be controlled to zero, so noise takes no effect.</p></li><li><p><font color="red">Generative ability</font> is based on the condition that all $p(Z\X)$ close to gaussion distribution.</p></li></ul><p>\begin{equation}<br>p(Z)=\sum_{X} p(Z | X) p(X)=\sum_{X} \mathcal{N}(0, I) p(X)=\mathcal{N}(0, I) \sum_{X} p(X)=\mathcal{N}(0, I)<br>\end{equation}</p><p>so $p(Z)$ subjects to normal distribution, which satisfy the prior.</p><ul><li><p>how: intruducing reconstruction loss:<br>direct method:<br>\begin{equation}<br>\mathcal{L}_{\mu}=\left|f_{1}\left(X_{k}\right)\right|^{2}<br>\end{equation}</p><p>\begin{equation}<br>\mathcal{L}_{\sigma^{2}}=\left|f_{2}\left(X_{k}\right)\right|^{2}<br>\end{equation}</p><p>it’s difficult to measure those loss, so it’s reasonable to introduce $KL$ divegence between standard gaussion distribution and independent gaussion distribution $K L\left(N\left(\mu, \sigma^{\wedge}\right) | N(0, l)\right)$:<br>\begin{equation}<br>\mathcal{L}_{\mu, \sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\mu_{(i)}^{2}+\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)<br>\end{equation}<br>where $d$ is the dimension of $z$ </p></li></ul><h4 id="Essence-of-VAE"><a href="#Essence-of-VAE" class="headerlink" title="Essence of VAE"></a>Essence of VAE</h4><ul><li><p>Two encoder: $f_1$ for $\mu$ while $f_2$ for $\sigma$</p></li><li><p>Reconstruction process: the loss of decoder assume that there is no noise; Sample $z$ process: the loss of encoder assume that there is gaussion noise.</p></li></ul><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><h4 id="Important-points-1"><a href="#Important-points-1" class="headerlink" title="Important points"></a>Important points</h4><ul><li>GAN is used to map normal distribution $p(z)$ into specfic distribution $p(x)$.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> comparison </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generative Matching Networks</title>
      <link href="/2019/04/22/Generative-Matching-Networks/"/>
      <url>/2019/04/22/Generative-Matching-Networks/</url>
      
        <content type="html"><![CDATA[<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><h4 id="Related-links"><a href="#Related-links" class="headerlink" title="Related links"></a>Related links</h4><ul><li>This paper in here <a href="https://openreview.net/pdf?id=HyCRyS9gx" target="_blank" rel="noopener">FAST ADAPTATION IN GENERATIVE MODELS WITH GENERATIVE MATCHING NETWORKS</a></li><li>Another version is here <a href="http://proceedings.mlr.press/v84/bartunov18a/bartunov18a.pdf" target="_blank" rel="noopener">Few-shot Generative Modelling with Generative Matching Networks</a>.</li><li>Both of them inspired from <a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf" target="_blank" rel="noopener">Matching networks for one shot learning</a></li></ul><h4 id="Remaining-bottlenecks-in-deep-generative-models"><a href="#Remaining-bottlenecks-in-deep-generative-models" class="headerlink" title="Remaining bottlenecks in deep generative models"></a>Remaining bottlenecks in deep generative models</h4><ul><li>extensive training leading to slow training (currently experience in DCGAN, FIGR).</li><li>difficulities with generalization from small number of training examples.</li></ul><h4 id="Solution-to-those-bottlenecks"><a href="#Solution-to-those-bottlenecks" class="headerlink" title="Solution to those bottlenecks"></a>Solution to those bottlenecks</h4><ul><li>conditional generative models: adapting generative distribution to additional inputs.</li><li>the restriction of previous research: the input data to be a single object or multiple objects represents the same concepts.</li></ul><h4 id="Novelty-of-generative-matching-networks"><a href="#Novelty-of-generative-matching-networks" class="headerlink" title="Novelty of generative matching networks"></a>Novelty of generative matching networks</h4><ul><li>VAE + matching network</li><li>without explicit limitations on the numbles of additional inputs objects or the number of concepts.(experiment not demonstrate this, conditional GAN may achieve it.)</li><li>learn new concepts.</li></ul><h3 id="Implementing"><a href="#Implementing" class="headerlink" title="Implementing"></a>Implementing</h3><h4 id="Specific-problem"><a href="#Specific-problem" class="headerlink" title="Specific problem"></a>Specific problem</h4><ul><li>VAE has been transfered intractable optimization into tractable optmization problem, refer to <a href="https://www.yanhong.me/2019/04/09/Overview-of-GAN/" target="_blank" rel="noopener">Overview of Generative Models</a>. However, update of parameters computed from a small portion of training data have a immediate effect for the whole dataset.</li><li>cann’t quickly adaption because of not incremental learning.</li><li>original optimization objective<br>\begin{aligned}<br>\log p(\mathbf{x} | \boldsymbol{\theta}) \geq \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi})=\mathbb{E}_{q}[\log p(\mathbf{x}, \mathbf{z} | \boldsymbol{\theta})-\log q(\mathbf{z} | \mathbf{x}, \boldsymbol{\phi})]=\log p(\mathbf{x} | \boldsymbol{\theta})-\operatorname{KL}(q | p(\cdot | \mathbf{x}, \boldsymbol{\theta}))<br>\end{aligned}</li></ul><h4 id="Generative-matching-networks"><a href="#Generative-matching-networks" class="headerlink" title="Generative matching networks"></a>Generative matching networks</h4><ul><li>different VAE which is x -&gt; z -&gt; x, the framework of this model is from z -&gt;x -&gt; z.</li><li><p>incorporating knowledge from newly available data is conditioning, so design model as $p(\mathbf{x} | \mathbf{X}, \boldsymbol{\theta})$ conditioning on additional input data $\mathbf{X}$.<br>\begin{aligned}<br>p(\mathbf{x}, \mathbf{z} | \mathbf{X}, \boldsymbol{\theta})=p(\mathbf{z} | \mathbf{X}, \boldsymbol{\theta}) p(\mathbf{x} | \mathbf{z}, \mathbf{X}, \boldsymbol{\theta})<br>\end{aligned}</p></li><li><p>followed VAE, maintaining a recognition model approximating the posterior over latent variable $z$ to achieve lower bound optimization objective, so,<br>\begin{equation}<br>q(\mathbf{z} | \mathbf{x}, \mathbf{X}, \boldsymbol{\phi}) \approx p(\mathbf{z} | \mathbf{x}, \mathbf{X}, \boldsymbol{\theta})<br>\end{equation}</p></li></ul><h5 id="Basic-model"><a href="#Basic-model" class="headerlink" title="Basic model"></a>Basic model</h5><h6 id="Flow-chat-of-generative-matching-networks"><a href="#Flow-chat-of-generative-matching-networks" class="headerlink" title="Flow chat of generative matching networks"></a>Flow chat of generative matching networks</h6><p><img src="/imgs/matching_networks.jpg" alt="&#39;flow chart of generative matching networks&#39;"></p><ol><li><p>the <font color="red">objective</font> is:<br>\begin{equation}<br>p(\mathbf{x}, \mathbf{z} | \mathbf{X}, \boldsymbol{\theta})=p(\mathbf{z} | \mathbf{X}, \boldsymbol{\theta}) p(\mathbf{x} | \mathbf{z}, \mathbf{X}, \boldsymbol{\theta})<br>\end{equation}</p></li><li><p>In <font color="red">conditional likelihood</font> (decoder) $p(\mathbf{x} | \mathbf{z}, \mathbf{X}, \boldsymbol{\theta})$:</p><ul><li>$\mathbf{x}$ is additional input data which is conditional objects.</li><li>$\mathbf{z}$ is the prior.</li><li>$f_L$ and $g_L$ are used to map latent variabels and conditioning objects into feature space $\phi$. $f_L$ is simple affine transformation while $g_L$ is a convolutional network.</li><li>$\phi_L$ is feature extractor different from $g_L$ but sharing some parameters between them.</li><li>mapping $r_L$ to $x$ by a deconvulutional network.</li></ul></li><li><p>In <font color="red">recognition model</font> (encoder) $q(\mathbf{z} | \mathbf{X}, \mathbf{x})$:</p><ul><li>$g_R = f_R = g_L$</li><li>$\phi_L$ = $\phi_R$</li><li>$r_R$ is used to compute parameters of approximate posterior which is a normal distribution.</li></ul></li></ol><h6 id="Core-for-conditional-generative-matching-network"><a href="#Core-for-conditional-generative-matching-network" class="headerlink" title="Core for conditional generative matching network"></a>Core for conditional generative matching network</h6><ul><li>Conditioning on additional inputs $\mathbf{X}$ by matching networks, <font color="red">attention kernel</font> can be calculated as,</li></ul><p>\begin{equation}<br>a\left(\mathbf{z}, \mathbf{x}_{t}\right)=\frac{\exp \left(\operatorname{sim}\left(f_{L}(\mathbf{z}), g_{L}\left(\mathbf{x}_{t}\right)\right)\right)}{\sum_{t^{\prime}=1}^{T} \exp \left(\operatorname{sim}\left(f_{L}(\mathbf{z}), g_{L}\left(\mathbf{x}_{t^{\prime}}\right)\right)\right)}<br>\end{equation}</p><ul><li><font color="red">Aggregating additional inputs</font> for test input,<br>\begin{equation}<br>\mathbf{r}=\sum_{t=1}^{T} a\left(\mathbf{z}, \mathbf{x}_{t}\right) \psi_{L}\left(\mathbf{x}_{t}\right)<br>\end{equation}<br>where $\mathbf{r}$ is input of decoder which is denconvolutional network.</li></ul><!-- ##### Recognition model --><h5 id="FCE"><a href="#FCE" class="headerlink" title="FCE"></a>FCE</h5><p>Obtaining a joint embedding of conditional input dataset with LSTM.<br>\begin{equation}<br>a\left(\mathbf{z}, \mathbf{x}_{t}\right)=\frac{\exp \left(\operatorname{sim}\left(f\left(\mathbf{z}, \mathbf{h}_{k}\right), g\left(\mathbf{x}_{t}, \mathbf{h}_{k}\right)\right)\right)}{\sum_{t^{\prime}=1}^{T} \exp \left(\operatorname{sim}\left(f\left(\mathbf{z}, \mathbf{h}_{k}\right), g\left(\mathbf{x}_{t^{\prime}}, \mathbf{h}_{k}\right)\right)\right)}<br>\end{equation}</p><p>\begin{equation}<br>\mathbf{r}_{k}=\sum_{t=1}^{T} a\left(\mathbf{z}, \mathbf{x}_{t}\right) \psi\left(\mathbf{x}_{t}\right)<br>\end{equation}</p><p>\begin{equation}<br>\mathbf{h}_{k+1}=R\left(\mathbf{h}_{k}, \mathbf{r}_{k}\right)<br>\end{equation}</p><h4 id="Training-detail"><a href="#Training-detail" class="headerlink" title="Training detail"></a>Training detail</h4><p>The final objective with lower bound can be respresented as,</p><p>\begin{equation}<br>\mathcal{L}(\mathbf{X}, \boldsymbol{\theta}, \boldsymbol{\phi})=\sum_{t=1}^{T} \mathbb{E}_{q\left(\mathbf{z}_{t} | \mathbf{x}_{t}, \mathbf{X}<br>_{&lt; t}, \boldsymbol{\phi}\right)}\left[\log p\left(\mathbf{x}_{t}, \mathbf{z}_{t} | \mathbf{X}_{&lt; t}, \boldsymbol{\theta}\right)- log q\left(\mathbf{z}_{t} | \mathbf{x}_{t}, \mathbf{X}_{&lt; t}, \boldsymbol{\phi}\right)\right]<br>\end{equation}</p><ul><li><p>the first item is used to for reconstruction loss.<br><!-- $\mathbb{E}\_{q\left(\mathbf{z}\_{t} | \mathbf{x}\_{t}, \mathbf{X}\_{< t}, \boldsymbol{\phi}\right)}\left[\log p\left(\mathbf{x}\_{t}, \mathbf{z}\_{t} | \mathbf{X}\_{<  -->t}, \boldsymbol{\theta}\right)$ </p></li><li><p>the second item is used to calculate KL divergence which can measure the distribution difference between latent variables and normal distribution.</p><!-- $log q\left(\mathbf{z}\_{t} | \mathbf{x}\_{t}, \mathbf{X}\_{< t}, \boldsymbol{\phi}\right)\right$--><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4></li><li>visualization of generated images.</li><li>conditional negative log-likelihoods for the test.</li></ul><h3 id="Possible-ideas"><a href="#Possible-ideas" class="headerlink" title="Possible ideas"></a>Possible ideas</h3><ul><li>VAE generated images is vague, may it can be combined with GAN <a href="https://arxiv.org/pdf/1512.09300.pdf" target="_blank" rel="noopener">Autoencoding beyond pixels using a learned similarity metric</a></li></ul><h3 id="My-high-level-understanding-of-GMN"><a href="#My-high-level-understanding-of-GMN" class="headerlink" title="My high-level understanding of GMN"></a>My high-level understanding of GMN</h3><ul><li><p>VAE: reconstruction (likelihood) + KL divergence(the distribution of latent varables[z_posterior] and normal distribution[z_prior])</p></li><li><p>GMN: reconstruction (likelihood) + KL divergence(the distribution of latent varables[z_posterior] and modified distribution with mean, variance values from observations[z_prior])<br><img src="/imgs/vae_gmn.jpg" alt="&#39;comparison between VAE and GMN&#39;"><br><img src="/imgs/GMN_VAE.jpg" alt="&#39;comparison between VAE and GMN&#39;"></p></li><li><p>GMN: flowchart<br><img src="/imgs/gmn_flowchat.jpg" alt="&#39;comparison between VAE and GMN&#39;"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Meta Learning</title>
      <link href="/2019/04/18/meta-learning/"/>
      <url>/2019/04/18/meta-learning/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Overview of Generative Models</title>
      <link href="/2019/04/09/Overview-of-GAN/"/>
      <url>/2019/04/09/Overview-of-GAN/</url>
      
        <content type="html"><![CDATA[<!-- <img src="https://www.yanhong.me/images/gm.jpg" width="50%" height="50%"> --><!-- <img src="⁨hy-zpg.github.io⁩/source⁩/imgs⁩/gm.jpg" class="full-image" width="180" height="180" title="hello"> --><!-- ![avatar](http://baidu.com/pic/doge.png)--><h2 id="The-detail-of-generative-model-can-refer-to-generative-models"><a href="#The-detail-of-generative-model-can-refer-to-generative-models" class="headerlink" title="The detail of generative model can refer to generative models"></a>The detail of generative model can refer to <a href="https://github.com/w-ww/cs231n/blob/master/slides/cs231n_2018_lecture12.pdf" target="_blank" rel="noopener">generative models</a></h2><h2 id="Definition-of-generative-models"><a href="#Definition-of-generative-models" class="headerlink" title="Definition of generative models"></a>Definition of generative models</h2><p>given training data, generate new samples from same distribution<br>\begin{aligned}<br>\mathrm{p}_{model}(\mathrm{x}) { similar to } \mathrm{p}_(\mathrm{x})<br>\end{aligned}</p><h2 id="Taxonomy-of-Generative-models"><a href="#Taxonomy-of-Generative-models" class="headerlink" title="Taxonomy of Generative models"></a>Taxonomy of Generative models</h2><p><img src="/imgs/gm.jpg" alt="&#39;taxonomy of generative models&#39;"></p><h3 id="PixelsRNN-and-PixelsCNN"><a href="#PixelsRNN-and-PixelsCNN" class="headerlink" title="PixelsRNN and PixelsCNN"></a>PixelsRNN and PixelsCNN</h3><ul><li><p>this method belongs to <strong><em>explicit density model</em></strong>.</p></li><li><p>Using <strong><em>chain rule</em></strong> to decompose likelihood of an image x into product of 1-d distributions:</p></li></ul><p>\begin{aligned}<br> p_{\theta}(x)=\prod_{i=1}^{n} p_{\theta}\left(x_{i} | x_{1}, \ldots, x_{i-1}\right)<br>\end{aligned}</p><p>where $x_i$ represents pixel, $p(x)$ denoets the likelihood of image $x$</p><ul><li><p>it’s important to define <strong><em>ordering</em></strong> of those pixels</p></li><li><p>complex distribution over pixels can be sloved by neural networks</p></li><li><p>pros: <strong><em>explicitly</em></strong> compute likelihood $p(x)$.</p></li><li><p>cons: sequential generation is slow.</p></li></ul><h4 id="PixelsRNN"><a href="#PixelsRNN" class="headerlink" title="PixelsRNN"></a>PixelsRNN</h4><ul><li><p>generating image pixels from corner, and then using RNN(LSTM) to model dependency on previous pixels.</p></li><li><p>drawback is that the process of sequential generation if slow.</p></li></ul><h4 id="PixelsCNN"><a href="#PixelsCNN" class="headerlink" title="PixelsCNN"></a>PixelsCNN</h4><ul><li><p>generating image pixels from corner, and then using CNN to cover context region.</p></li><li><p>generation proceed sequentially is still slow.</p></li></ul><h3 id="Variational-Autoenconders"><a href="#Variational-Autoenconders" class="headerlink" title="Variational Autoenconders"></a>Variational Autoenconders</h3><ul><li><p>general explanation is here <a href="https://zhuanlan.zhihu.com/p/27549418" target="_blank" rel="noopener">VAE</a></p></li><li><p>this method defines <strong><em>intractable density function</em></strong> with latent $z$:</p></li></ul><p>\begin{aligned}<br>p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x | z) d z<br>\end{aligned}</p><h4 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h4><ul><li>mapping image $x$ to features $z$ with deep neural networks, $z$ is regarded to capture meaningful factors of variation in data.</li><li>to learn feature $z$ by reconstruction error: $x$ -&gt; $z$ -&gt; $\hat_{x}$ with encoder and decoder.</li></ul><h4 id="Variational-Autoenconders-for-generation-problem"><a href="#Variational-Autoenconders-for-generation-problem" class="headerlink" title="Variational Autoenconders for generation problem"></a>Variational Autoenconders for generation problem</h4><ul><li><p>Assuming training data $x(i)$ representation $z$ is generated from latent representation $z$. Specifically, first sampling $z$ from true prior $p_{\theta}(z)$, then sampling $x$ from true conditional $p_{\theta}(x|z^(i))$</p></li><li><p>the <strong><em>aim</em></strong> is to estimate the true parameter $\theta$ by <font color="red">maximuming likelihood of training data</font> :</p></li></ul><p>\begin{equation}<br>p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x | z) d z<br>\end{equation}</p><p>where $p_{\theta}(z)$ is gaussion prior, and $p_{\theta}(x | z) $ is decoder.</p><ul><li><p>intractable optimization problem, because it’s impossible to compute $p(x | z)$ for every $z$ which means the integral operation is fail in this condition. </p></li><li><p>posterior density $p_{\theta}(z | x)=p_{\theta}(x | z) p_{\theta}(z) / p_{\theta}(x)$ is also intractable, since $p_{\theta}(x)$ is intractable data likelihood. </p></li></ul><h4 id="lower-bound-of-VAE"><a href="#lower-bound-of-VAE" class="headerlink" title="lower bound of VAE"></a>lower bound of VAE</h4><ul><li>why: intractable, transfer optimization </li><li>how: In addition to decoder network modeling $p_\theta(x|z)$, define additional encoder network $q_{\phi}(z|x)$ that approximates $p_\theta(z|x)$</li><li><p>final optimization objective:<br>\begin{aligned}<br>\log p_{\theta}\left(x^{(i)}\right)\\<br>&amp;=\mathbf{E}_{z \sim q_{\phi}\left(z | x^{(i)}\right)}\left[\log p_{\theta}\left(x^{(i)}\right)\right]\\<br>&amp;=\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} | z\right) p_{\theta}(z)}{p_{\theta}\left(z | x^{(i)}\right)}\right]\\<br>&amp;=\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} | z\right) p_{\theta}(z)}{p_{\theta}\left(z | x^{(i)}\right)} \frac{q_{\phi}\left(z | x^{(i)}\right)}{q_{\phi}\left(z | x^{(i)}\right)}\right]\\<br>&amp;=\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} | z\right)\right]-\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z | x^{(i)}\right)}{p_{\theta}(z)}\right]+\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z | x^{(i)}\right)}{p_{\theta}\left(z | x^{(i)}\right)}\right]\\<br>&amp;=\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} | z\right)\right]-D_{K L}\left(q_{\phi}\left(z | x^{(i)}\right) | p_{\theta}(z)\right)+D_{K L}\left(q_{\phi}\left(z | x^{(i)}\right) | p_{\theta}\left(z | x^{(i)}\right)\right)<br>\end{aligned}<br>where $\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} | z\right)\right]$ is used to reconstruct the input data, $q_{\phi}\left(z | x^{(i)}\right) | p_{\theta}(z)$ is used to make approximate posterior distribution close to prior.</p></li><li><p>lower bound: <font color="red">transfering intractable problem into tractale problem with lower bounds</font>, introducing encoder $_{\phi}$ to transfer two KL divergence, in which one is tractable while another one is untractabel. Thus, lower bound aims at optimizing the tractable KL divergence.<br><img src="/imgs/lower_bound.jpg" alt="&#39;lower bound&#39;"></p></li><li><p>the detail of optimization procedure as follows:<br><img src="/imgs/procedure.jpg" alt="&#39;procedure of optimization&#39;"></p></li></ul><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><p>\begin{equation}<br>{\min <em>{G} \max </em>{D} V(D, G)=}<br>{\mathbb{E}<em>{\boldsymbol{x} \sim p</em>{\text {data}}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}<em>{\boldsymbol{z} \sim p</em>{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z}))]}<br>\end{equation}</p><p>where $D(x)$ represents the probability of $x$ sourced from real data.</p>]]></content>
      
      
      <categories>
          
          <category> Generative Model </category>
          
      </categories>
      
      
        <tags>
            
            <tag> overview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few-shot Image generation</title>
      <link href="/2019/04/04/Few-shot-Image-generation/"/>
      <url>/2019/04/04/Few-shot-Image-generation/</url>
      
        <content type="html"><![CDATA[<h2 id="Meta-learning"><a href="#Meta-learning" class="headerlink" title="Meta learning"></a>Meta learning</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><ul><li><p>“learn to learn”, intends to design models that can learn <em><strong>new</strong></em> skills or adapt to <em><strong>new</strong></em> environment <em><strong>rapidly</strong></em> with <em><strong>a few</strong></em> traing samples, like <em><strong>human learning way</strong></em>. The detail can be posted in <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank" rel="noopener">Meta-Learning: Learning to Learn Fast</a></p></li><li><p>Optimization aims:<br>\begin{aligned}<br>\theta^* = \arg\min_\theta \mathbb{E}_{\mathcal{D}\sim p(\mathcal{D})} [\mathcal{L}_\theta(\mathcal{D})]<br>\end{aligned}</p></li></ul><p>where $\mathcal{D}=\langle S, B\rangle$, Support set and Batch set.</p><ul><li>Training steps</li></ul><ol><li>sample a subset of labels $L\subset\mathcal{L}$.</li><li>samples a support set $S^L \subset \mathcal{D}$ and a training batch $B^L \subset \mathcal{D}$. Both of them belong to the sampled label set $L$, $y \in L, \forall (x, y) \in S^L, B^L$.</li><li>support set is the input of model.</li><li>the update of model parameters is based on the loss in backpropagation calculated from the mini-batch $B^L$.</li></ol><p>each pair of sampled dataset $(S^L, B^L)$ is regarded as one data point, such that trained models can generalize to other datasets. Symbols in red are added for meta-learning in addition to the general supervised learning objective.<br>\begin{aligned}<br>\theta = \arg\max_\theta \color{red}{E_{L\subset\mathcal{L}}[} E_{\color{red}{S^L \subset\mathcal{D}, }B^L \subset\mathcal{D}} [\sum_{(x, y)\in B^L} P_\theta(x, y\color{red}{, S^L})] \color{red}{]}<br>\end{aligned}</p><ul><li>Traning stages</li></ul><ol><li>meta-learner: a optimizer $g_\phi$ learns how to update the learner model’s parameters via the support set $S$, $\theta’ = g_\phi(\theta, S)$</li><li>learner: A classifier $f_\theta$ is the “learner” model, trained for operating a given task.</li><li>final learning objective is:<br>\begin{aligned}<br>\mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]]<br>\end{aligned}</li></ol><h3 id="Common-methods"><a href="#Common-methods" class="headerlink" title="Common methods"></a>Common methods</h3><h4 id="model-based-f-theta-mathbf-x-S"><a href="#model-based-f-theta-mathbf-x-S" class="headerlink" title="model-based: $f_\theta(\mathbf{x}, S)$"></a>model-based: $f_\theta(\mathbf{x}, S)$</h4><ul><li>use <em><strong>recurrent</strong></em> network with <em><strong>internal (or external)</strong></em> memory.</li><li>rapid parameter update achieved by <em><strong>meta-learner model</strong></em> or <em><strong>internal architecture</strong></em>.</li><li>memory-augmented neural network for meta-learning<ul><li>using <em>external memory storage</em> to facilate learning process <em>without forgetting</em> new information in future.</li><li>encoding new information <em>quicly</em>, so adapt to new tasks after only <em>a few samples</em>.</li><li>memory-augmented neural networkc: how to assign weights to attention vector. memory serves as knowledge repository, the controller learns to read and write memory rows. ttention weights generation by addressing mechanism: control-based + location-based.</li><li>MANN for meta-learning: the update of memory for efficient information retrievel and storage, how to read from memory and how to write into memory.</li></ul></li><li>meta network<ul><li>architecture</li><li>loss gradients are used as meta information to populate models to learn fast weights.</li></ul></li></ul><h4 id="metric-based-sum-mathbf-x-i-y-i-in-S-k-theta-mathbf-x-mathbf-x-i-y-i"><a href="#metric-based-sum-mathbf-x-i-y-i-in-S-k-theta-mathbf-x-mathbf-x-i-y-i" class="headerlink" title="metric-based: $\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i$"></a>metric-based: $\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i$</h4><ul><li>learn <em><strong>efficient</strong></em> distance metric.</li><li>similar to <em><strong>nearnest neighbors algorithm</strong></em> (KNN,k-means) and <em><strong>kernel density estimation</strong></em>.</li><li><p>the predicted possibility of labeled samples is from is a <em><strong>weighted sum of support set samples</strong></em>, and the weight is generated by a kernel function $k<em>\theta$, which can measure the <em><strong>similarity</strong></em> of twp data samples:<br>\begin{aligned}<br>P\</em>\theta(y \vert \mathbf{x}, S) = \sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_{i}<br>\end{aligned}</p></li><li><p>crucial points: <em><strong>good kernel</strong></em></p></li><li>solution: learning <em><strong>embedding vector</strong></em> of input data explicitly and use them to design <em><strong>proper kernel</strong></em> functions.</li><li>Siamese networks: <ul><li>assumption: the <em>learned embedding</em> can be <em>generalized</em> to be useful for measuring the distance between images of unknown categories.</li><li><em>verification</em>, images pairs.</li><li>final prediction is the class of the support image with highest probability.</li></ul></li><li>Matching networks:<ul><li>$g_{\theta}$ with $k$ classifiers for k classes, while $f_{\theta}$ for testing images.</li><li><em>attention kernel</em> depends on two embedding functions $g$ and $f$, in simple version where $f=g$. in complex version where integrating full contextual embedding (FCE), it achieves improvement on hard tasks, not for simple tasks.</li></ul></li><li>Relation networks<ul><li>image pairs, <em>feature concatenation</em>.</li><li>relation modeled: <em>mse</em> loss function.</li></ul></li><li>Prototypical networks<ul><li>images of <em>each class</em> are embedded into $M$ dimensional feature vector, each class has <em>prototype</em> feature vector from the mean vector of the embedded support data samples in each class.</li><li><em>squared euclidean distance</em> loss.</li></ul></li></ul><h4 id="optimization-based-P-g-phi-theta-S-L-y-vert-mathbf-x"><a href="#optimization-based-P-g-phi-theta-S-L-y-vert-mathbf-x" class="headerlink" title="optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$"></a>optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$</h4><ul><li>optimize the <em><strong>model parameter</strong></em> explicitly for <em><strong>fast</strong></em> learning.</li><li>modeling <em><strong>optimization algorithm</strong></em> exploitly.</li><li>LSTM meta-learner</li><li>MAML</li><li><p>reptile</p></li><li><p>crucial keys: <em><strong>good kernel</strong></em></p></li><li>solution: learning <em><strong>embedding vector</strong></em> of input data explicitly and use them to design <em><strong>proper kernel</strong></em> functions.</li><li>Siamese networks: <ul><li>assumption: the <em><strong>learned embedding</strong></em> can be <em><strong>generalized</strong></em> to be useful for measuring the distance between images of unknown categories.</li><li><em><strong>verification</strong></em>, images pairs.</li><li>final prediction is the class of the support image with highest probability.</li></ul></li><li>Matching networks:<ul><li>$g_{\theta}$ with $k$ classifiers for k classes, while $f_{\theta}$ for testing images.</li><li>attention kernel depends on two embedding functions $g$ and $f$, in simple version where $f=g$. in complex version where integrating full contextual embedding (FCE), it achieves improvement on hard tasks, not for simple tasks.</li></ul></li><li>Relation network<ul><li>image pairs, feature concatenation.</li><li>relation modeled: mse loss function.</li></ul></li><li>Prototypical<ul><li>images of each class are embedded into $M$ dimensional feature vector, each class has <em>prototype</em> feature vector from the mean vector of the embedded support data samples in each class.</li><li>squared euclidean distance.</li></ul></li></ul><h4 id="optimization-based-P-g-phi-theta-S-L-y-vert-mathbf-x-1"><a href="#optimization-based-P-g-phi-theta-S-L-y-vert-mathbf-x-1" class="headerlink" title="optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$"></a>optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$</h4><ul><li>optimize the <em><strong>model parameter</strong></em> explicitly for <em><strong>fast</strong></em> learning</li></ul>]]></content>
      
      
      <categories>
          
          <category> Meta-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> few-shot learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Manifold Learning</title>
      <link href="/2019/03/18/Manifold-Learning/"/>
      <url>/2019/03/18/Manifold-Learning/</url>
      
        <content type="html"><![CDATA[<h2 id="Manifold-Leanring-for-Semi-supervised-Learning"><a href="#Manifold-Leanring-for-Semi-supervised-Learning" class="headerlink" title="Manifold Leanring for Semi-supervised Learning"></a>Manifold Leanring for Semi-supervised Learning</h2><ul><li>the model trained with labeled samples and unlabeled samples</li><li>introducing manifold learning to learn the geometry of marginal distribution &gt;reference first paper<a href="http://www.jmlr.org/papers/volume7/belkin06a/belkin06a.pdf" target="_blank" rel="noopener">Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</a> <ul><li>explanation in Chinese(<a href="https://zhuanlan.zhihu.com/p/33006509" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33006509</a>)</li><li>aim: data-depend regularization to exploit geometry of the marginal distribution</li></ul></li></ul><h3 id="Manifold-learning"><a href="#Manifold-learning" class="headerlink" title="Manifold learning"></a>Manifold learning</h3><ul><li>non-linear dimensionality reduction method to obtain intrinsic feature &gt;reference PAMI</li></ul><h3 id="Semi-supervised-Learning-with-Manifold-Learning"><a href="#Semi-supervised-Learning-with-Manifold-Learning" class="headerlink" title="Semi-supervised Learning with Manifold Learning"></a>Semi-supervised Learning with Manifold Learning</h3><ul><li>reason: leveraging a few of training samples to train model always fail to reflect the dataset distribution, which means that the trained model only adapt to the supervised label without learning the intrinsic feature. Introducing manifold learning to combine labeled samples and unlabeled samples can exploit the geometr of marginal distribution.</li></ul><h3 id="Semi-supervised-Multi-task-Learning-with-Manifold-Learning"><a href="#Semi-supervised-Multi-task-Learning-with-Manifold-Learning" class="headerlink" title="Semi-supervised Multi-task Learning with Manifold Learning"></a>Semi-supervised Multi-task Learning with Manifold Learning</h3><blockquote><p>reference [Semisupervised feature analysis by mining correlations among multiple tasks]{<a href="https://arxiv.org/pdf/1411.6232.pdf}" target="_blank" rel="noopener">https://arxiv.org/pdf/1411.6232.pdf}</a></p><ul><li>aim: feature selection in semi-supervised MTL</li><li>method: sparce coefficients learnt<br>\begin{aligned}<br>p(x) &amp; = \min_{w<em>t} \sum\</em>(l=1}^t (loss(w_l) + \alpha ||w_l||_{1,2} + \gamma ||w||_{*})<br>\end{aligned}</li></ul></blockquote><p>including $l_{1,2}$ norm and Laplacian norm:</p><p>\begin{aligned}<br>\gamma ||w|| = \min_{w,b}\sum_{l=1}{t}Tr(w^Tx_lL_lx_l^Tw)<br>\end{aligned}</p><blockquote><p>reference [Semi-supervised multitask learning]{<a href="http://papers.nips.cc/paper/3198-semi-supervised-multitask-learning.pdf}" target="_blank" rel="noopener">http://papers.nips.cc/paper/3198-semi-supervised-multitask-learning.pdf}</a></p><p>reference [Semi-supervised multi-task learning with task regularizations]{<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.194.1854&amp;rep=rep1&amp;type=pdf}" target="_blank" rel="noopener">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.194.1854&amp;rep=rep1&amp;type=pdf}</a></p><p>reference [Semi-supervised multitask learning for scene recognition]{<a href="https://www.researchgate.net/profile/Lichao_Mou/publication/268880603_Semi-Supervised_Multitask_Learning_for_Scene_Recognition/links/567a67f608ae7fea2e9a08f1.pdf}" target="_blank" rel="noopener">https://www.researchgate.net/profile/Lichao_Mou/publication/268880603_Semi-Supervised_Multitask_Learning_for_Scene_Recognition/links/567a67f608ae7fea2e9a08f1.pdf}</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> feature regularization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semi-MTL Related</title>
      <link href="/2019/03/09/Semi-MTL-Related/"/>
      <url>/2019/03/09/Semi-MTL-Related/</url>
      
        <content type="html"><![CDATA[<h2 id="MTL"><a href="#MTL" class="headerlink" title="MTL"></a>MTL</h2><h3 id="Learning-Methods"><a href="#Learning-Methods" class="headerlink" title="Learning Methods"></a>Learning Methods</h3><ul><li>Supervised</li><li>Semi-supervised</li></ul><h3 id="Scenarios"><a href="#Scenarios" class="headerlink" title="Scenarios"></a>Scenarios</h3><ul><li>Homogeneous</li><li>Heterogeneous<ul><li>Heterogeneous Tasks (Document &amp; Images &amp; Audio)</li><li>Hterogeneous Datsets (Each dataset with a set of labels)</li></ul></li></ul><h3 id="Improvement-Ideas"><a href="#Improvement-Ideas" class="headerlink" title="Improvement Ideas"></a>Improvement Ideas</h3><ul><li>Network Structure</li><li>Feature Selection</li></ul><h3 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h3><ul><li>CV</li><li>NLP</li></ul><p>ALL MTL is the combination of above mentioned.<br>Our method is semi-supervised MTL with heterogeneous datasets, comparison including supervised or semi-supervised MTL with heterogeneous dataset.</p><blockquote><p>reference <a href="https://arxiv.org/pdf/1411.6232.pdf" target="_blank" rel="noopener">Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks</a></p></blockquote><ul><li>semi-supervised, heterogeneou datasets, feature selection.</li><li>scenario: CV, dataset A for tasks A[labeled and unlabeled], dataset B for tasks B[labeled and unlabeled]</li><li>method: manifold learning, mining feature correlation by sparse coefficients.</li><li>No</li></ul><blockquote><p>reference <a href="https://arxiv.org/pdf/1604.01335.pdf" target="_blank" rel="noopener">Deep Cross Residual Learning for Multitask Visual Recognition</a></p></blockquote><ul><li>supervised, heterogeneou datasets, network structure.</li><li>scenario: CV, dataset A for tasks A[labeled], dataset B for tasks B[labeled]</li><li>method: enables intuitive learning across multiple related tasks using cross-connections called cross-residuals</li><li>Yes? Netowrk realization?</li></ul><blockquote><p>reference <a href="http://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf" target="_blank" rel="noopener">A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></p></blockquote><ul><li>semi-supervised, heterogeneou datasets, network structure.</li><li>scenario: </li><li>method: </li><li>No</li></ul><blockquote><p>reference <a href="https://arxiv.org/pdf/1802.09913.pdf" target="_blank" rel="noopener">Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces</a></p></blockquote><ul><li>supervised, heterogeneou datasets, network structure.</li><li>scenario: text classification, dataset A for tasks A[labeled], dataset B for tasks B[labeled]</li><li>method: combining multi-task learning and semi- supervised learning by inducing a joint embed- ding space between disparate label spaces and learning transfer functions between label embeddings. LTN can be used to label unlabelled and auxiliary task data by utilising the ‘distilling knowledge’ contained in auxiliary model predictions. not only model their relationship, but also to directly estimate the cor- responding label of the target task based on auxil- iary predictions</li><li>Yes: embedding, prediction, output, temperature=1, embedding can be realized with cross-stich network</li></ul><blockquote><p>reference <a href="http://www.aclweb.org/anthology/D16-1070" target="_blank" rel="noopener">Neural Network for Heterogeneous Annotations</a></p></blockquote><ul><li>supervised, heterogeneou datasets, multi-view &amp; stacking setting</li><li>scenario: NLP, dataset A for tasks A[labeled], dataset B for tasks B[labeled]</li><li>method: multi-view, stacking, neurual network</li><li>Yes: not understand</li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multitask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semi-supervised Heteroneous Multitask Learning Baselines</title>
      <link href="/2019/02/28/Semi-Heteroneous-Multitask-Baseline/"/>
      <url>/2019/02/28/Semi-Heteroneous-Multitask-Baseline/</url>
      
        <content type="html"><![CDATA[<h2 id="high-related"><a href="#high-related" class="headerlink" title="high-related"></a>high-related</h2><blockquote><p>reference <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8107520" target="_blank" rel="noopener">Learning without Forgetting</a></p><ul><li>distilled knowledge</li></ul><p>reference <a href="https://arxiv.org/pdf/1611.00851.pdf" target="_blank" rel="noopener">An All-In-One Convolutional Neural Network for Face Analysis</a></p><ul><li>naive joint</li></ul><p>reference <a href="https://arxiv.org/pdf/1609.06426.pdf" target="_blank" rel="noopener">From Facial Expression Recognition to Interpersonal Relation Prediction<br>Zhanpeng</a></p><ul><li>pseudo label</li></ul><p>reference NLP <a href="https://arxiv.org/pdf/1802.09913.pdf" target="_blank" rel="noopener">Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces</a><br>‘we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions.’</p><ul><li>probelm: jointly using unlabelled data and auxiliary, annotated datasets; domain gap; missing labels</li><li>method: training the Label Transfer Network, minimise the squared error between the model predictions and the pseudo label</li><li>baseline: yes (how to calculate loss funtion [squared error] based on pseudo label)</li></ul><p>reference <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Misra_Seeing_Through_the_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels Ishan</a></p><ul><li>confidence score</li><li>same apply scene</li><li>refer to paper writting</li></ul><p>reference non-DL <a href="http://de.arxiv.org/pdf/1411.6232" target="_blank" rel="noopener">Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks</a></p><ul><li>problem: ‘Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence.’ ‘These previous works, however, independently select features for each task, which fails to consider correlations among multiple related tasks.’ ‘Despite of their good performances, these classical algorithms are all implemented only with labeled training data.’</li><li>method: ‘ignoring the correlations among different features -&gt;apply the sparse coefficients to the feature vectors -&gt; proposing multiple feature selection’</li><li>baseline: undetermined (hard to complete)</li></ul><p><a href="https://arxiv.org/pdf/1803.04062.pdf" target="_blank" rel="noopener">Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing—and Back</a></p><ul><li>related work conclude papers of joint training of models for multiple tasks.<ul><li>‘how learned structure is shared across tasks’:<ol><li>supervise different tasks at different depths of the shared structure <a href=""></a></li><li>duplicate the shared structure into columns and define mechanisms for sharing information across columns <a href="https://arxiv.org/pdf/1704.01631.pdf" target="_blank" rel="noopener">Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition</a>, using intermediate representations as auxiliary supervision for low-level task recognition to improve final task performance.</li></ol></li></ul></li><li>baseline: no( pose-emotion both are high-level task, not similar to keypoint detection)</li></ul></blockquote><h2 id="pseudo-method-related"><a href="#pseudo-method-related" class="headerlink" title="pseudo method related"></a>pseudo method related</h2><blockquote><p>reference CVPR 2018<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Pseudo_Mask_Augmented_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Pseudo Mask Augmented Object Detection</a><br> ‘proposing an effective learning approach that progressively improves the quality pseudo from a coarse initialization,the detection network parameters Θ and pseudo masks Mpseudo are alternatively optimized following a EM-like way’</p><ul><li>probelm: object detection without mask annotation</li><li>method: pseudo mask (alternatively,initialization,pseudo mask with refinement algorithm[graphical model, because the pixel charaterization])</li><li>baseline: yes(uing graphical model with learned information as input to refine pseudo label)</li></ul><p>reference <a href="https://arxiv.org/pdf/1703.02391.pdf" target="_blank" rel="noopener">Learning from Noisy Labels with Distillation</a></p><ul><li>interpolation between noisy label and distilled knowledge</li><li>baseline: yes</li></ul><p>reference <a href="http://papers.nips.cc/paper/4292-clustered-multi-task-learning-via-alternating-structure-optimization.pdf" target="_blank" rel="noopener">Clustered Multi-Task Learning Via Alternating Structure Optimization</a></p><ul><li>‘ASO which aims to identify a shared low-dimensional predictive structure for all tasks’,’based on the standard assump- tion that each task can learn equally well from any other task’,’the clustering view of ASO has not been explored before’</li><li>hard to understand, pure theories.</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multitask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Critical Points in Multitask Learning</title>
      <link href="/2019/02/28/Critical-Points-in-Multitask-0/"/>
      <url>/2019/02/28/Critical-Points-in-Multitask-0/</url>
      
        <content type="html"><![CDATA[<h2 id="Several-tips-for-heterogeneous-multitask-learning"><a href="#Several-tips-for-heterogeneous-multitask-learning" class="headerlink" title="Several tips for heterogeneous multitask learning"></a>Several tips for heterogeneous multitask learning</h2><ul><li>core problem - heterogeneity structrues in tasks[age and gender], source[documents and images], feature[generated from model for classification or regression tasks], samples[one labeled with age while another annotated with gender]</li></ul><ol><li>network with suitabel initialization and learning rate</li><li>construct feature relationship in network, offering trainable parameters, refer to [cross-stitch network]</li><li>feature selection, sparsity and factorization, which means that the specific branch layers to ontain task-specific fearures in a common feature space.</li></ol><h2 id="Research-directions"><a href="#Research-directions" class="headerlink" title="Research directions"></a>Research directions</h2><ul><li>feature relationship from the level of network.</li><li>feature processing from the level fo feature space, such as selection, sparsity and facorization.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multitask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Parameters Selection for Pseudo</title>
      <link href="/2019/02/27/Parameters-Selection-for-Pseudo/"/>
      <url>/2019/02/27/Parameters-Selection-for-Pseudo/</url>
      
        <content type="html"><![CDATA[<h2 id="temperature-in-distilled-knowledge"><a href="#temperature-in-distilled-knowledge" class="headerlink" title="temperature in distilled knowledge"></a>temperature in distilled knowledge</h2><blockquote><p>reference-1 <a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></p></blockquote><p>aim: distilling the knowledge in an ensemble of models into a single model<br>temperature: the higher the temperature T, the softer the probability distribution over classes. ‘For the distillation we tried temperatures of [1, 2, 5, 10] and used a relative weight of 0.5 on the cross-entropy for the hard targets, where bold font indicates the best value’</p><blockquote><p>reference-2 from reference-1 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8107520" target="_blank" rel="noopener">Learning without Forgetting</a></p></blockquote><p>aim: useing only new task data to train the network while preserving the original capabilities<br>temperature: $T=2$, grid search method</p><h2 id="density-threshold-in-local-feature-density"><a href="#density-threshold-in-local-feature-density" class="headerlink" title="density threshold in local feature density"></a>density threshold in local feature density</h2><h2 id="distribution-distance-is-transfered-into-weights"><a href="#distribution-distance-is-transfered-into-weights" class="headerlink" title="distribution distance is transfered into weights"></a>distribution distance is transfered into weights</h2>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pesudo labels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross Stitch Network</title>
      <link href="/2019/02/27/Cross-stitch-network/"/>
      <url>/2019/02/27/Cross-stitch-network/</url>
      
        <content type="html"><![CDATA[<h2 id="cross-stitch-network"><a href="#cross-stitch-network" class="headerlink" title="cross-stitch-network"></a>cross-stitch-network</h2><blockquote><p>reference CVPR 2016<a href="https://arxiv.org/pdf/1604.03539.pdf" target="_blank" rel="noopener">Cross-stitch Networks for Multi-task Learning</a> <a href="https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning/blob/master/gender_age_multi_task_learning.py" target="_blank" rel="noopener">tensorflow-code</a></p></blockquote><ul><li>problem: existing multi-task approaches rely on enumerating multiple net- work architectures specific to the tasks at hand, that do not generalize.</li><li>method: proposing a new sharing unit: “cross-stitch” unit. These units combine the activations from multiple networks.</li></ul><h2 id="self-defined-cross-stitch-layer-in-keras-with-tensorflow-as-backbone"><a href="#self-defined-cross-stitch-layer-in-keras-with-tensorflow-as-backbone" class="headerlink" title="self-defined cross-stitch layer in keras with tensorflow as backbone"></a>self-defined cross-stitch layer in keras with tensorflow as backbone</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class Cross_stitch(Layer):</span><br><span class="line"><span class="comment"># basic parameter setting</span></span><br><span class="line">    def __init__(self,input_shape_1,input_shape_2, **kwargs):</span><br><span class="line">        super(Cross_stitch, self).__init__(**kwargs)</span><br><span class="line">        self.input_shape_1 = input_shape_1</span><br><span class="line">        self.input_shape_2 = input_shape_2</span><br><span class="line">    <span class="comment"># apply trainable parameters in network, similar to convolutional layer </span></span><br><span class="line">    <span class="comment"># shape is important, you must to calculate specific size based on the shape of input and output</span></span><br><span class="line">    <span class="comment"># in cross-stitch network: [xa,xb]*[papameter]=[xa',xb'], the detail refer to the paper</span></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        shape = self.input_shape_1 + self.input_shape_2</span><br><span class="line">        self.cross_stitch = self.add_weight(</span><br><span class="line">            shape=(shape,shape),</span><br><span class="line">            initializer=tf.initializers.identity(),</span><br><span class="line">            name=<span class="string">'cross_stitch'</span>)</span><br><span class="line">        self.built = True</span><br><span class="line">    <span class="comment"># conduct implement of the detailed algorithm calculation</span></span><br><span class="line">    <span class="comment"># inputs represent the output of upper layer, such as x=Dense(parameter)(inputs)</span></span><br><span class="line">    def call(self,inputs):</span><br><span class="line">        inputss = tf.concat((inputs[0], inputs[1]), axis=1)</span><br><span class="line">        output = tf.matmul(inputss, self.cross_stitch)</span><br><span class="line">        output1 = tf.reshape(output[:,:self.input_shape_1],shape=[-1,self.input_shape_1])</span><br><span class="line">        output2 = tf.reshape(output[:,self.input_shape_2:],shape=[-1,self.input_shape_2])</span><br><span class="line">        <span class="built_in">return</span> [output1, output2]</span><br><span class="line">    def get_config(self):</span><br><span class="line">        config = &#123;<span class="string">'input_shape_1'</span>: self.input_shape_1,<span class="string">'input_shape_2'</span>: self.input_shape_2&#125;</span><br><span class="line">        base_config = super(Cross_stitch, self).get_config()</span><br><span class="line">        <span class="built_in">return</span> dict(list(base_config.items()) + list(config.items()))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multitask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Heterogeneous Multitask</title>
      <link href="/2019/02/26/Heterogeneous-Multitask/"/>
      <url>/2019/02/26/Heterogeneous-Multitask/</url>
      
        <content type="html"><![CDATA[<h1 id="Large-scale-Heterogeneous-Learning-in-Big-Data-Analytics"><a href="#Large-scale-Heterogeneous-Learning-in-Big-Data-Analytics" class="headerlink" title="Large-scale Heterogeneous Learning in Big Data Analytics"></a>Large-scale Heterogeneous Learning in Big Data Analytics</h1><blockquote><p>reference slide <a href="http://cci.drexel.edu/bigdata/bigdata2014/MTL_BigData_14.pdf" target="_blank" rel="noopener">Large-scale Heterogeneous Learning in Big Data Analytics</a></p></blockquote><ul><li>multi-task learning<br>‘Multitask learning is an inductive transfer mechanism for improving generalization performance [Caruana, Machine Learning’97]’</li><li>multi-label learning:</li><li>multi-view learning:</li></ul><h2 id="Heterogeneous-outputs-related-to-each-other-with-the-same-set-of-inputs"><a href="#Heterogeneous-outputs-related-to-each-other-with-the-same-set-of-inputs" class="headerlink" title="Heterogeneous outputs related to each other with the same set of inputs"></a>Heterogeneous outputs related to each other with the same set of inputs</h2><blockquote><p>reference-1 <a href="http://www.cs.cmu.edu/~sssykim/papers/yang_kim_xing_nips09.pdf" target="_blank" rel="noopener">Heterogeneous Multitask Learning with Joint Sparsity Constraints</a></p></blockquote><ol><li>probelm: dealing with homogeneous tasks, such as purely regression (continue) or classification (discrete) task, from a common set of high-dimensional feature space. </li><li>method: modeling the joint sparsity as L1/L∞ or L1/L2 norm of the model parameters, achieving joint sparse feature selection.</li><li>datasets: discovering genetic markers that influence multiple correlated traits jointly, datasets with different clinical traits including continuous and discrete labels.</li></ol><h2 id="Heterogeneous-inputs-from-multiple-domains"><a href="#Heterogeneous-inputs-from-multiple-domains" class="headerlink" title="Heterogeneous inputs from multiple domains"></a>Heterogeneous inputs from multiple domains</h2><blockquote><p>reference-2 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8058002&amp;tag=1" target="_blank" rel="noopener">Heterogeneous Multitask Metric Learning<br>Across Multiple Domains</a> <a href="https://www.semanticscholar.org/paper/Exploiting-High-Order-Information-in-Heterogeneous-Luo-Tao/1bf6d4b31fa26aa9e0c2b759e6bee9ec718dcae7" target="_blank" rel="noopener">Exploiting High-Order Information in Heterogeneous Multi-Task Feature Learning</a></p></blockquote><ul><li>‘Multitask metric learning (MTML), which can be regarded as a special case of transfer metric learning (TML) by performing transfer across all related domains.’</li><li>‘Heterogeneous transfer learning approaches can be adopted to remedy this drawback by deriving a metric from the learned transformation across different domains’<ol><li>probelm: inputs from heterogeneous domain can be solved by deriving a metric from the learned transformation from two domains, but pratical aims is to deal with multiple domain by learning the metric from all domains.</li><li>method: metrics -&gt; transformation -&gt; subspace -&gt; maximize high-order ovariance among the predictive structures of these domains, because high-order statistics (correlation information), which can only be exploited by simultaneously examining all domains, thus obtaining more reliable feature transformations and metrics.</li><li>datasets: Document Categorization, Scene Classification and Image Annotation</li></ol></li></ul><h2 id="Heterogeneous-features-for-different-task"><a href="#Heterogeneous-features-for-different-task" class="headerlink" title="Heterogeneous features for different task"></a>Heterogeneous features for different task</h2><blockquote><p>reference-3 <a href="http://www.intsci.ac.cn/users/fzzhuang/papers/TOC2017.pdf" target="_blank" rel="noopener">Semantic Feature Learning for Heterogeneous<br>Multitask Classification via Non-Negative<br>Matrix Factorization</a></p></blockquote><ol><li>problem: different tasks have heterogenous feature in real world.</li><li>method: leveraging a non-negative matrix factorization-based multitask method (MTNMF) to learn a common semantic feature space underlying different heterogeneous feature spaces of each task. it is similar to reference-1, feature selection vs feature factorization.</li><li>datasets: 20Newsgroups&amp;ImageNet (image and document), Email Spam Detection (15 persons), Sentiment Classification (books;  dvd; electronics; kitchen)</li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multitask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Why and How to Use Pseudo</title>
      <link href="/2019/02/22/Why-How-Pseudo/"/>
      <url>/2019/02/22/Why-How-Pseudo/</url>
      
        <content type="html"><![CDATA[<ul><li>Pseudo labels in multi-task learning</li><li>Pseudo data selection with density and distribution distance</li></ul><h2 id="Pseudo-labels"><a href="#Pseudo-labels" class="headerlink" title="Pseudo labels"></a>Pseudo labels</h2><p>The reason why need to apply pseudo data into model training </p><ul><li><blockquote><p>reference <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper.pdf" target="_blank" rel="noopener">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a>, demonstrating that training a CNN from scratch using both clean and noisy data is better than just using the clean one, on the condition that the amount of pseudo data is limited.</p></blockquote></li><li><blockquote><p>reference <a href="https://arxiv.org/pdf/1609.06426.pdf" target="_blank" rel="noopener">From Facial Expression Recognition to Interpersonal Relation Prediction Zhanpeng</a>, stating explaination that pseudo data can bridge the gap between heterogeneous data.</p></blockquote></li><li><blockquote><p>reference <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Misra_Seeing_Through_the_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels</a>, ‘what’s the all attributs of images’ versus ‘what’s the labeled attributes’, labeling missing attributes. The novalty may be regarded as a baseline (waiting to understand the detail).</p></blockquote></li><li>data imbalance in joint training strategy, such as 20k~ pose dataset AFLW and 90k~ emotion dataset ExpW.</li></ul><h2 id="Pseudo-data-selection"><a href="#Pseudo-data-selection" class="headerlink" title="Pseudo data selection"></a>Pseudo data selection</h2><p>Because much noisy data can effect the model performance and even disturb the model training process, leading to poor model performance, pseudo data selection can control the ratio of pseudo data in the whole datasets.</p><h3 id="Density"><a href="#Density" class="headerlink" title="Density"></a>Density</h3><ul><li>reason: because clusters are easily detected by the local density of data points, in the pseudo data belong to same categoty have similar feature. Appling a density based clustering algorithm that measures the complexity of psuedo data using data distribution density in. each category.</li><li>implementation detail: measuring the purity of data with pseudo label based on its distribution density in a feature space, and rank the purity to generate pseudo weights, pseudo data with higher density is assigned larger weights, while smaller weights are assigned to low-density pseudo data.</li></ul><h3 id="Distribution-distance"><a href="#Distribution-distance" class="headerlink" title="Distribution distance"></a>Distribution distance</h3><ul><li>reason: <ul><li>‘transfer learning is one important method in machine learning, it can relax the condition of the independent identical distribution in train dataset and test dataset, so that knowledge can be transfered from source domain to target domain’,’its main application includes domain adaptation and multi-domain tranferation’.</li><li>instance-based domain adaption: calculating the distance between source domain data and target domain data, then adjusting the weights of target domain instance so that the target data can be matched with source domain data. In detail, the smaller distance, the higher similarity.</li></ul></li><li><p>implementation detail: </p><ol><li>GMM<ul><li>reason:</li><li>implementation: generating Gaussian Mixture models (GMM) based on A1 (data with pseuodo label) and B (data with ground truth) in each categoty. Assuming GMM has K Gaussian models, we obtain G(A1_1), …, G(A1_K), G(B1_1), …, G(B1_K); </li></ul></li><li><p>EMD</p><ul><li><p>reason: &gt;reference <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning</a>,  which takes domain scale into account by adding a scale factor. In this paper, transfer learning can be viewed as moving a set of images from the source domain S to the target domain T. The work needed to be done by moving an image to another can be defined as their feature Euaullean distance, so the distance between two domains can be defined as the least amount of total work needed. This definition of domain similarity can be calculated by the Earth Mover’s Distance (EMD). </p><ul><li><p>original EMD equation</p><p>\begin{aligned}<br>\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}<br>\end{aligned}</p><p>\begin{aligned}<br>EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}<br>\end{aligned}</p><p>\begin{aligned}<br>P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}<br>\end{aligned}</p><p>\begin{aligned}<br>Q={(q_{1},w_{q1}),(q_{2},w_{q2}),…,(q_{n},w_{qn})}<br>\end{aligned}</p></li><li><p>reference paper EMD application<br>g(s<em>{i}) from the mean value of image features in category i from source domain<br>g(t</em>{i}) from the mean value of image features in category i from target domain<br>\begin{aligned}<br>\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}<br>\end{aligned}</p><p>\begin{aligned}<br>EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}<br>\end{aligned}</p><p>\begin{aligned}<br>S={(s_{1},w_{s1}),(s_{2},w_{s2}),…,(s_{m},w_{sm})}<br>\end{aligned}</p><p>\begin{aligned}<br>T={(t_{1},w_{t1}),(t_{2},w_{t2}),…,(t_{n},w_{tn})}<br>\end{aligned}</p><p>\begin{aligned}<br>D=[d_{i,j}] = || g(s_{i}) − g(t_{j})||<br>\end{aligned}</p><p>\begin{aligned}<br>sim(S, T) = e−γd(S,T )<br>\end{aligned}</p></li><li><p>EMD application for pseudo data selection<br>  $g(p)$ represents mean value of image features in specific cluster from the same category in source domain<br>  $g(g_{i})$ is mean value of image features in cluster i from the same category in target domain<br>\begin{aligned}<br>\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}<br>\end{aligned}</p><p>\begin{aligned}<br>EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}<br>\end{aligned}</p><p>\begin{aligned}<br>P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}<br>\end{aligned}</p><p>\begin{aligned}<br>G={(g_{1},w_{g1}),(g_{2},w_{g2}),…,(g_{n},w_{gn})}<br>\end{aligned}</p><p>\begin{aligned}<br>D=[d_{i,j}] = || g(p_{i}) − g(g_{j})||<br>\end{aligned}</p><p>\begin{aligned}<br>sim(S, T) = e−γd(S,T )<br>\end{aligned}</p></li></ul></li><li>implementation: calculating the distance between $G(A_1^i)$ and the whole cluster in G(B1). As for distance calculation with EMD method, we obtain $P={G(A_1^i)_{mean},G(A_1^i)_{probs}}$ and $Q={[G(B_1^i)_{mean}, …, G(B_1^K)_{mean}],[G(B_1^i)_{probs}, …, G(B_1^K)_{probs}]}$, so we can calculate distance vector $D=(emd_1,emd_k)$, which is used for obtaining distribution weights $weights_g$.</li></ul></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pesudo labels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Density and Distribution</title>
      <link href="/2019/02/21/Density-Distribution/"/>
      <url>/2019/02/21/Density-Distribution/</url>
      
        <content type="html"><![CDATA[<h2 id="Purity-based-density-amp-distribution-distance-based-GMM"><a href="#Purity-based-density-amp-distribution-distance-based-GMM" class="headerlink" title="Purity based density &amp; distribution distance based GMM"></a>Purity based density &amp; distribution distance based GMM</h2><h3 id="Purity-based-density"><a href="#Purity-based-density" class="headerlink" title="Purity based density"></a>Purity based density</h3><blockquote><p>reference：ECCV 2018 <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper.pdf" target="_blank" rel="noopener">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a></p><p>original reference： Science 2004 <a href="http://sites.psu.edu/mcnl/files/2017/03/9-2dhti48.pdf" target="_blank" rel="noopener">Clustering by fast search and find of density peaks</a></p></blockquote><h4 id="Creativity"><a href="#Creativity" class="headerlink" title="Creativity"></a>Creativity</h4><ul><li>leveraging data distribution density in feature space to evaluate the complexity of the data<br>ours — evaluating the purity of pseudo label data by data density in feature space</li><li>noisy data can be regared as regularied method to improve the model generalization<br>ours — pesudo labels can improve the model generalization </li></ul><h4 id="Technical-detail"><a href="#Technical-detail" class="headerlink" title="Technical detail"></a>Technical detail</h4><p>Density based clustering algorithm</p><ul><li>generating features</li><li>calculating Euclidean distance D_ij</li><li>calculating local density of each image</li><li>calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence </li></ul><h3 id="Distribution-distance-based-GMM"><a href="#Distribution-distance-based-GMM" class="headerlink" title="Distribution distance based GMM"></a>Distribution distance based GMM</h3><h4 id="Clustring-based-GMM"><a href="#Clustring-based-GMM" class="headerlink" title="Clustring based GMM"></a>Clustring based GMM</h4><h5 id="GMM-with-EM"><a href="#GMM-with-EM" class="headerlink" title="GMM with EM"></a>GMM with EM</h5><ul><li><p>original definition<br>\begin{aligned}<br>p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)<br>\end{aligned}<br>$\pi_k$  represent the possibility of sample belong to kth category</p></li><li><p>new definition for coding<br>\begin{aligned}<br>\sum_{k} z_k = 1<br>\end{aligned}</p></li></ul><h5 id="Related-code"><a href="#Related-code" class="headerlink" title="Related code"></a>Related code</h5><ul><li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html" target="_blank" rel="noopener">sklearn.mixture.GaussianMixture</a></li></ul><h4 id="Data-distribution-distance-EMD"><a href="#Data-distribution-distance-EMD" class="headerlink" title="Data distribution distance EMD"></a>Data distribution distance EMD</h4><blockquote><p>reference： <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.pdf" target="_blank" rel="noopener"> Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning</a><br>original reference： <a href="http://robotics.stanford.edu/~rubner/papers/rubnerIjcv00.pdf" target="_blank" rel="noopener">The Earth Mover’s Distance as a Metric for Image Retrieval</a></p></blockquote><ul><li><p>EDM definition: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second</p></li><li><p>EMD data signature:<br>\begin{aligned}<br>s= (feature,weights)<br>\end{aligned}</p></li><li><p>related code</p></li><li><a href="https://github.com/chalmersgit/EMD" target="_blank" rel="noopener">general EMD</a> <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/RUBNER/emd.htm" target="_blank" rel="noopener">expaination</a></li><li><a href="https://pypi.org/project/pyemd/" target="_blank" rel="noopener">pyemd-1D data</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wasserstein_distance.html" target="_blank" rel="noopener">scipy.atats.wasserstein_distance</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pesudo labels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pseudo Purity and Dataset Distribution Distance</title>
      <link href="/2019/02/17/Pseudo-purity&amp;distribution-distance/"/>
      <url>/2019/02/17/Pseudo-purity&amp;distribution-distance/</url>
      
        <content type="html"><![CDATA[<h2 id="Purity-and-distribution-distance-based-density"><a href="#Purity-and-distribution-distance-based-density" class="headerlink" title="Purity and distribution distance based density"></a>Purity and distribution distance based density</h2><h3 id="Data-density"><a href="#Data-density" class="headerlink" title="Data density"></a>Data density</h3><blockquote><p>reference  ECCV 2018 <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng_Guo_CurriculumNet_Learning_from_ECCV_2018_paper.pdf" target="_blank" rel="noopener"> CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a></p></blockquote><h4 id="creativity"><a href="#creativity" class="headerlink" title="creativity"></a>creativity</h4><ol><li>leveraging data distribution density in feature space to evaluate the complexity of the data<br> ours — evaluating the purity of pseudo label data by data density in feature space</li><li>noisy data can be regared as regularied method to improve the model generalization<br> ours — pesudo labels can improve the model generalization </li></ol><h4 id="technical-detail"><a href="#technical-detail" class="headerlink" title="technical detail"></a>technical detail</h4><ol><li>density based clustering algorithm<ul><li>generating features</li><li>calculating Euclidean distance D_ij<ul><li>calculating local density of each image</li><li>calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence </li></ul></li></ul></li></ol><h3 id="Data-distribution-distance-EMD"><a href="#Data-distribution-distance-EMD" class="headerlink" title="Data distribution distance EMD"></a>Data distribution distance EMD</h3><blockquote><p>reference IJCV 2000 <a href="http://robotics.stanford.edu/~rubner/papers/rubnerIjcv00.pdf" target="_blank" rel="noopener">The Earth Mover’s Distance as a Metric for Image Retrieval</a></p></blockquote><h4 id="definition"><a href="#definition" class="headerlink" title="definition"></a>definition</h4><ol><li>EDM: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second</li><li>signature:<br>\begin{aligned}<br>s= (feature,weights)<br>\end{aligned}</li></ol><h2 id="Purity-and-distribution-distance-based-gmm"><a href="#Purity-and-distribution-distance-based-gmm" class="headerlink" title="Purity and distribution distance based gmm"></a>Purity and distribution distance based gmm</h2><h3 id="GMM-with-EM"><a href="#GMM-with-EM" class="headerlink" title="GMM with EM"></a>GMM with EM</h3><ul><li>original definition<br>\begin{aligned}<br>p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)<br>\end{aligned}</li></ul><p>$\pi_k$  represent the possibility of sample belong to kth category</p><ul><li>new definition for coding<br>\begin{aligned}<br>\sum_{k} z_k = 1<br>\end{aligned}</li></ul>]]></content>
      
      
      <categories>
          
          <category> Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pesudo labels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross Entropy Loss</title>
      <link href="/2019/01/16/Cross_entropy_loss/"/>
      <url>/2019/01/16/Cross_entropy_loss/</url>
      
        <content type="html"><![CDATA[<h2 id="Cross-entropy-loss-calculation-in-different-circumstances"><a href="#Cross-entropy-loss-calculation-in-different-circumstances" class="headerlink" title="Cross entropy loss calculation in different circumstances"></a>Cross entropy loss calculation in different circumstances</h2><ul><li><p>Cross_entropy_loss in tensorflow</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def softmax_cross_entropy_with_logits(</span><br><span class="line">    _sentinel=None,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=None,</span><br><span class="line">    logits=None,</span><br><span class="line">    dim=-1,</span><br><span class="line">    name=None):</span><br><span class="line">  _ensure_xent_args(<span class="string">"softmax_cross_entropy_with_logits"</span>, _sentinel, labels,</span><br><span class="line">                    logits)</span><br><span class="line">  with ops.name_scope(name, <span class="string">"softmax_cross_entropy_with_logits_sg"</span>,</span><br><span class="line">                      [logits, labels]) as name:</span><br><span class="line">    labels = array_ops.stop_gradient(labels, name=<span class="string">"labels_stop_gradient"</span>)</span><br><span class="line">  <span class="built_in">return</span> softmax_cross_entropy_with_logits_v2(</span><br><span class="line">      labels=labels, logits=logits, dim=dim, name=name)</span><br><span class="line">def softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    _sentinel=None,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=None,</span><br><span class="line">    logits=None,</span><br><span class="line">    dim=-1,</span><br><span class="line">    name=None):</span><br><span class="line">  _ensure_xent_args(<span class="string">"softmax_cross_entropy_with_logits"</span>, _sentinel, labels,</span><br><span class="line">                    logits)</span><br><span class="line">  with ops.name_scope(name, <span class="string">"softmax_cross_entropy_with_logits"</span>,</span><br><span class="line">                      [logits, labels]) as name:</span><br><span class="line">    logits = ops.convert_to_tensor(logits, name=<span class="string">"logits"</span>)</span><br><span class="line">    labels = ops.convert_to_tensor(labels, name=<span class="string">"labels"</span>)</span><br><span class="line">    convert_to_float32 = (</span><br><span class="line">        logits.dtype == dtypes.float16 or logits.dtype == dtypes.bfloat16)</span><br><span class="line">    precise_logits = math_ops.cast(</span><br><span class="line">        logits, dtypes.float32) <span class="keyword">if</span> convert_to_float32 <span class="keyword">else</span> logits</span><br><span class="line">    <span class="comment"># labels and logits must be of the same type</span></span><br><span class="line">    labels = math_ops.cast(labels, precise_logits.dtype)</span><br><span class="line">    input_rank = array_ops.rank(precise_logits)</span><br><span class="line">    shape = logits.get_shape()</span><br><span class="line">    <span class="keyword">if</span> dim is not -1:</span><br><span class="line">      def _move_dim_to_end(tensor, dim_index, rank):</span><br><span class="line">        <span class="built_in">return</span> array_ops.transpose(</span><br><span class="line">            tensor,</span><br><span class="line">            array_ops.concat([</span><br><span class="line">                math_ops.range(dim_index),</span><br><span class="line">                math_ops.range(dim_index + 1, rank), [dim_index]</span><br><span class="line">            ], 0))</span><br><span class="line">      precise_logits = _move_dim_to_end(precise_logits, dim, input_rank)</span><br><span class="line">      labels = _move_dim_to_end(labels, dim, input_rank)</span><br><span class="line">    input_shape = array_ops.shape(precise_logits)</span><br><span class="line">    precise_logits = _flatten_outer_dims(precise_logits)</span><br><span class="line">    labels = _flatten_outer_dims(labels)</span><br><span class="line">    cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits(</span><br><span class="line">        precise_logits, labels, name=name)</span><br><span class="line">    output_shape = array_ops.slice(input_shape, [0],</span><br><span class="line">                                   [math_ops.subtract(input_rank, 1)])</span><br><span class="line">    cost = array_ops.reshape(cost, output_shape)</span><br><span class="line">    <span class="keyword">if</span> not context.executing_eagerly(</span><br><span class="line">    ) and shape is not None and shape.dims is not None:</span><br><span class="line">      shape = shape.as_list()</span><br><span class="line">      del shape[dim]</span><br><span class="line">      cost.set_shape(shape)</span><br><span class="line">    <span class="keyword">if</span> convert_to_float32:</span><br><span class="line">      <span class="built_in">return</span> math_ops.cast(cost, logits.dtype)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">return</span> cost</span><br></pre></td></tr></table></figure></li><li><p>Cross_entropy_loss in Keras with backbone of tensorflow</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output)</span><br></pre></td></tr></table></figure></li></ul><h2 id="The-reason-why-the-value-of-loss-become-inf"><a href="#The-reason-why-the-value-of-loss-become-inf" class="headerlink" title="The reason why the value of loss become inf"></a>The reason why the value of loss become inf</h2><ul><li>log(x) when x -&gt; 0</li><li>learning rate is too high</li><li>some parameters of Nel appear inf</li><li>input data appear inf</li><li>the parameters of model no longer updated </li></ul><h2 id="Cross-entropy-loss-with-missing-labels"><a href="#Cross-entropy-loss-with-missing-labels" class="headerlink" title="Cross_entropy_loss with missing labels"></a>Cross_entropy_loss with missing labels</h2><ul><li><p>leveraging fixed zero array or ones array as ground truth and generating mask in loss function</p><pre><code class="lang-bash">def mask_cross_entropy_loss(y_true,y_pred):  mask=K.all(K.equal(y_true,0),axis=-1)  mask=1-K.cast(mask,K.floatx())  loss = K.categorical_crossentropy(y_true,y_pred)*mask  return (K.sum(loss)/K.sum(mask)</code></pre></li><li><p>corresponding accuracy with mask</p><pre><code class="lang-bash">def mask_cross_entropy_acc(y_true,y_pred):  mask=K.all(K.equal(y_true,0),axis=-1)  mask=1-K.cast(mask,K.floatx())  acc = K.cast(K.equal(K.argmax(y_true,axis=-1),K.argmax(y_pred,axis=-1)),K.floatx()))*mask  return (K.sum(acc)/K.sum(mask)</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> Deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MMD and Density</title>
      <link href="/2019/01/15/GMM/"/>
      <url>/2019/01/15/GMM/</url>
      
        <content type="html"><![CDATA[<h2 id="Relation-between-MMD-and-density-for-pseudo-data-selection"><a href="#Relation-between-MMD-and-density-for-pseudo-data-selection" class="headerlink" title="Relation between MMD and density for pseudo data selection"></a>Relation between MMD and density for pseudo data selection</h2><p>combination of MMD and density: my understanding is that we can obtain MMD, density, prior weights from GMM, when GMM can be calculated from ground-trurh label of </p><ul><li><p>MMD: first-order information</p></li><li><p>Density: second-order information, the larger variance, the sparser, the small variance, the denser </p></li></ul><h3 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h3><ul><li>Gaussian Mixed Model: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.</li></ul><ul><li><p>Differentce between k-means and GMM: k-means notes that each point is assigned to different clusters, while GMM can calculate the probabiliy of each point belong to each clusters.</p></li><li><p>Import parameters of GMM: K Gaussion models, also K clusters, $\pi$, $\mu$, $\Sigma$<br>\begin{aligned}<br>p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)<br>\end{aligned}</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Theory </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pseudo labels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Numpy Learning</title>
      <link href="/2019/01/14/Numpy%20learning/"/>
      <url>/2019/01/14/Numpy%20learning/</url>
      
        <content type="html"><![CDATA[<p>One-hot labels preprocessing</p><ul><li><p>Weighting samples with confidence score</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = np.ragmax(predicted_result,axis=1)</span><br><span class="line">arg = np_utils.to_categorical(index,classes)</span><br><span class="line">weighted_one_hot=predicted_result*arg</span><br></pre></td></tr></table></figure></li><li><p>Selection &amp;&amp; weighting samples with confidence score</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weighted_selected_one_hot=np.where(predicted_result&gt;k,predicted_result,0)</span><br></pre></td></tr></table></figure></li><li><p>Extending vector into matrix</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xx = [np.full(classes,value) <span class="keyword">for</span> value <span class="keyword">in</span> x]</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Codes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
