<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CVAE-GAN]]></title>
    <url>%2F2019%2F07%2F01%2FCVAE-GAN%2F</url>
    <content type="text"><![CDATA[Paper Understanding paper links problem,method,effect problem: synthesizing images in fine-grained categories. method: gan+vaegan: asymmetric loss(G_loss[mean discrepancy] + D_loss[cross entropy])vae: encoder(relatio between latent space and real images) -&gt; pairwise feature matching -&gt; keep structure of real images effect: generating realistic and diverse samples with fine-grained category labels. detail explanation the loss of generator (mean feature matching): minimizing the distance of mean feature to the real images, reduce mode collapse. using an intermediate layer of the discriminator to extract feature for real images and generated fake images. before the last fc layer of the discriminator. conditional image generation: using mean feature matching. pairwise feature matching network designclassification both used in the generator and the discriminator KL loss is used in the generator]]></content>
      <categories>
        <category>Generative Model</category>
      </categories>
      <tags>
        <tag>combination</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper Reading Related GAN]]></title>
    <url>%2F2019%2F06%2F19%2FSimialr-to-DAGAN%2F</url>
    <content type="text"><![CDATA[Similar to DAGANDAGANdetailed understanding ppt Class-Distinct and Class-Mutual Image Generation with GANs problem definition: typically in class-conditional image generation, it is assume that there are no intersections between classes, gan model are optimized to fit discrete class labels. Actually, there are data with ambiguous bundaries in real world in class-overlapping settings. aims: class-distinct and class-mutual image generation is desigend to selectively generate class-distinct and class-mutual images in a controllable manner. effects: propose novel families of GANs called class-mixture GAN and class-posterior GAN, mainly redisgining the generator prior and the objective function with auxiliary classifier. destail: weak supervision(binary class labels), focusing on class specificity, different from typical class-wise interpolation. based on AC-GAN(class-conditional GAN): loss_gan + loss_classification class label $y_g = r_0y_0 + r_1y_1 + … + r_iy_i$, interpolation in label space ($r_i$ is from Dirichlet distribution) posterior: r_i = C(y|x_r) visualization: vary y continuously between classes to generate images conclusion: latent space is effective, simple version in y space, we selected in image space Attention-based Fusion for Multi-source Human Image Generation problem: conditioned on a target pose and a set X of source appearance images effect: complementary images of the same person which usually available at training and at testing time. (similar idea) novelty: attention-mechanism which selects relevant information from different source image regions. (similar) U-Net: attention are integrated into U-Net, attention module which is spatial-attention nodule(channel independent), using Enconder-Decoder construction. dataset: market1501(source appearance) + DeepFashion(pose) GAN applied in other leanring tasksProgressive Pose Attention Transfer for Person Image Generation[Attention-based Fusion for Multi-source Human Image Generation](https://arxiv.org/pdf/1905.02655v1.pdf) Joint Discriminative and Generative Learning for Person Re-identification problem: significant intra-class variations across different cameras. motivation: augamenting data to enhance the invariance to input changes, improving learned re-id embeddings by better leveraging the generated data, which achieved by jointly coupling re-id learning and data augmentation end-to-end. important methods: generative module including appearance code and structure code + discriminative module sharing appearance encoder with generative module. switching the appearance codes or appearance codes to generate high-quality cross-id images which are on line fed back to the appeance encoder and used to improve the discriminative module. — my understanding: analyzing the essence of problem (for example, intra-class invariance in reid, maybe intra-class invariance is important for all classification tasks), generating corresponding images to improve the performance (generate cross-id images by appearance code and structure code, maybe using two code focused on different content(appeance and structure two codes for reid problem),also it can be beneficial to generate cross-class images). method: for generative module: leveraging latent code (appeance and structure) $ self-identity: encouraging the appeance encoder to pull appeance code by generating images with seeing cross-images in the same identity, so that intra-class feature variance are reduced, similar to 1-way-1-shot $ cross-identity: reconstruct appeance and structure codes, soft label, adversarial training, similar to 2-way-1-shot for discriminative module: (images and generated images are fed into discriminative part) $ primary feature learning: teacher-student type supervision based on soft label $ fine-grained feature mining: multi-task learning style]]></content>
      <categories>
        <category>Generative Model</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pedestrain Datasets]]></title>
    <url>%2F2019%2F06%2F14%2FPedestrain-Datasets%2F</url>
    <content type="text"><![CDATA[LinksRAP, PETA, PA-100K Most dataset and papers links Most dataset Simple introduction Clothing Attributes Dataset: cloth style(7 categories), ensurely used. accuracy (about 55%) PETA Dataset: low-resolution, multi-class(age 4 categories), download. accuracy(16,31,46,&gt;61: 83.8, 78.8,76.4,89.0),19,000 images, about 0.82 Database of Human Attributes (HAT): high-resolution, age(multi-classes), having sent email RAP Dataset: not high-resolution, 3-multi-classes(age 3 categories), waiting to send email. PA-100K Dataset: binary WIDER Attribute Dataset: multi-people Parse27k Dataset: squence, oritation(multi-class), not so much related CRP Dataset: squence, not human centre Berkeley-Attributes of People: binary Deepfashion dataset: cloth related, 289,222 - 50 classes cloth categories: 82%]]></content>
      <categories>
        <category>Human Attributes</category>
      </categories>
      <tags>
        <tag>datasets</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[U-Net]]></title>
    <url>%2F2019%2F06%2F10%2FU-Net%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Interpolation in Latent Space]]></title>
    <url>%2F2019%2F06%2F05%2FInterpolation-in-Latent-Space%2F</url>
    <content type="text"><![CDATA[Paper ReadingDiverse Image-to-Image Translation via Disentangled Representations novalty diverse translation between two collections of images without aligned training pairs UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONALGENERATIVE ADVERSARIAL NETWORKS DNA-GAN: LEARNING DISENTANGLED REPRESEN- TATIONS FROM MULTI-ATTRIBUTE IMAGES]]></content>
      <categories>
        <category>Generative Model</category>
      </categories>
      <tags>
        <tag>interpolation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Normalization_for_Input_of_GAN]]></title>
    <url>%2F2019%2F05%2F27%2FNormalization-for-Input-of-GAN%2F</url>
    <content type="text"><![CDATA[Normalization methods for the input of GAN and visualizationsimpleinput1234def normalization(data): data = data - 127.5 data = data / 127.5 return data visualization123456fake = G(z)def unnormalization(data): data = data * 127.5 data = data + 127.5 return datavisual_image = unnormalization(fake) mean and stdinput (using the mean and std of training set)123456789def normalization(self): """ Normalizes our data, to have a mean of 0 and sdt of 1 """ self.mean = np.mean(self.x_train) self.std = np.std(self.x_train) self.x_train = (self.x_train - self.mean) / self.std self.x_val = (self.x_val - self.mean) / self.std self.x_test = (self.x_test - self.mean) / self.std visualization(using the mean and std of training set)1234567def unnormalization(self,fake): """ Normalizes our data, to have a mean of 0 and sdt of 1 """ self.mean = np.mean(self.x_train) self.std = np.std(self.x_train) visual_image = fake*self.std + self.mean]]></content>
      <categories>
        <category>GAN</category>
      </categories>
      <tags>
        <tag>data preprocess</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Related Work about Few-shot Image Generation]]></title>
    <url>%2F2019%2F05%2F21%2FRelated-Work-about-Few-shot-Image-Generation%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[GAN Metrics]]></title>
    <url>%2F2019%2F05%2F07%2FGAN-Metrics%2F</url>
    <content type="text"><![CDATA[Paper An empirical study on evaluation metrics of generative adversarial networksNotesgan metrics notes There are two categories evaluation metric: sample-based other Sample-based metricsThe Inception Score how: inception-v3 trained on Imagenet, the output of softmax, the input is the generated images. evaluation: quality: the conditional possibility distriution $p(y | \mathbf{x})$, which is the probability of the vector of softmax layer (1000) and explicitly belong to which category. diversity: the marginal possibility distriution $p(y)$, close to uniform distribution, which represents that the number of generated samples in each categoty is comparable. calculation:\begin{equation}\mathbf{I S}(G)=\exp \left(\mathbb{E}_{\mathbf{x} \sim p_{g}} D_{K L}(p(y | \mathbf{x}) | p(y))\right)\end{equation} the larger, the better. quality and diversity. The Mode Score how: improvement of The Inception Score, considering the distribution of real images evaluation: quality: diversity: dissimilarity between the real distriution and fake distribution calcualtion the larger, the better. quality and diversity. Kernel MMD how: improved measurement to dissimilarity between the real distriution and fake distribution for some fixed kernel function. evaluation: real samples and fake samples, lower MMD means close the smaller, the better. quality. The Wasserstein distance how: further improved measurement to dissimilarity between the real distriution and fake distribution evaluation: real samples and fake samples, lower Wasserstein distance means close the smaller, the better. depending on the selection of feature space. The Fréchet Inception Distance (FID) how: convulutional network is used to extract feature, modeling the Gaussian random variables with mean value and covariance from real and fake feature space. evaluation: measuring the two Gaussian distribution the smaller, the better. efficiency. The 1-Nearest Neighbor classifier how: assessing the two distriution are identical. evaluation: about 50% leave-one-out, the accuracy of generated images several measurement index: accuracy, recall ….. ideal evaluation metric other metrics]]></content>
      <categories>
        <category>Generative Model</category>
      </categories>
      <tags>
        <tag>evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper Writting]]></title>
    <url>%2F2019%2F05%2F04%2FPaper-Writting%2F</url>
    <content type="text"><![CDATA[Ruleswell-organized: storyline -&gt; abstract and introduction -&gt; methods -&gt; related work -&gt; experiment Storyline Make sure that the storyline is reasonable and fluent before your writting. Abstract &amp; introduction: Based on the refined storyline, to form abstract and introduction, a extension of abstract. Related work It related to our selected baselines, so I think it belong to the survey work before this research project, and to update it when you read related papers. Another important point is to divide the sub-fields of your current research topic, you can find related works accurately and comprehensively. Methods To fully understand the proposed method, generally the method is inspired by other papers or books. It’s important to know about what the method is and how it can deal with your problem. More importantly, forming core combined method based on those refered algorithm can help us to explain our story line in therotical analysis. Experiment Comparion is important and classify those baselines into serveral group based on its relateness. To ablate the role of parameters in experiments and introduce ‘special case’ to outline the effect of the proposed method. Detailswell-written: abstract -&gt; introduction -&gt; method -&gt; experiment -&gt; conclusion Abstract Describing the scenario of your reserch problem. The shortcoming of the existing methods. Our inspiration, which means what sub-field we selected as research point for this prolem. What problem we will encounter in using this sub-field method, eg.. 1 …, 2… To highly summarize how our approach to solve thos problems. To brief describe the experimental results. Introduction From large research point (MTL) to sub-field research point (MTL with disjoint datasets). Describing this scenario and the disadvantage of existing methods. Our method, explain it. Concluding contributios. Related work Directly related work. Methods related works. Method To describe the problem in mathmatical method (inputs [images and labels], trainable parameters). Inspiration work Hierarchical method description, for example, data selection(confident [confidence score, local density], distribution), label vector interpolation. Experiment Datasets: split rule (make sure before experiment). Comparison: directly related to the part of related work, brief how you reproduce those baseline in your prpblem scenario. and then to analyze the result to outligh the advantage of the proposed method. Ablation: explaining the introduced hyper-parameter, surely, it’s necessary to classify those ‘special case’ to facilate the description. Considering the qualitative analysis if possible. Conclusion Highly conclude what the proposed method and how it deal with problem. The demonstration of the experimental results. Accumulated core experience Paper organization: overall organization and the organization of each part. Hierarchical description in each part: logical description in each paragragh. Expressions of mathematical formulas: unification of superscript and subscript in formula, and refining it during writting.]]></content>
      <categories>
        <category>Writting</category>
      </categories>
      <tags>
        <tag>experience</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Few-shot Settings]]></title>
    <url>%2F2019%2F04%2F29%2FFew-shot-Setings%2F</url>
    <content type="text"><![CDATA[Taking Omniglot dataset as an example. ClassificationMatching networks for one shot learning datasets split classes split: total 1623 classes, 1200 classes in training set, 300 classes in validation set, the remaining classes in testing set. episode: support images[batch_size, 5way * 5shot, image_size, image_size,1], target images[batch_size, 5shot, image_size, image_size,1]), batch_size is adjustable. training: considering matching between support images and target images training and validation training set for parameter update validation set is used for evaluation the trained model, if accuracyparameter update: parameters update based on matching classification accuracy testing: without parameter update evaluating the classification of target images in testing set. GenerationFIGR: Few-shot Image Generation with Reptile datasets split classes split: total 1623 classes, 20 classes in testing set and the remaining classes in training set. episode: 1way * 4shot [4,image_size, image_size,1], batch_size(default=1) is unadjustable training: considering the optimization procedure from meta training to inner training training and validation training procedure: optimization testing: without parameter update evaliating generator loss, discriminator loss, generated images in testing set. FAST ADAPTATION IN GENERATIVE MODELS WITH GENERATIVE MATCHING NETWORKS dataset split: total 50 alphabets, 30 alphabets for training and the remaining for testing. episode: up to 2way * sampled instance (10), batch size is adjustable. training: considering the total loss from generation model (likelihood of the prior and generated images) and recognition model (likelihood of the generated prior). training and testing minimizing the loss (ll_gen_prior+ll_gen_img-ll_rec_prior), the total loss of testing is regarded as evaluation measurement. testing: testing set: calculating the total loss (different calculation method) recondtruction: recontructed images based on trained samples (evaluaing the training intermediate results) generation: fedding test set, generated new image without training classification: two different methods.]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>few-shot generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Few-shot Datasets]]></title>
    <url>%2F2019%2F04%2F28%2FFew-shot-Datasets%2F</url>
    <content type="text"><![CDATA[DatasetsFIGR-8 grey 18,409 classes total 1,548,944 images each class contains at least 8 images unbalance Omniglot binary 50 alphabets 1623 unique type of characters 20 samples of each characters It contains 1623 unique type of characters originating from 50 alphabets, each of which has been handwritten 1 time by 20 different individuals Datasets setting in different paperClassificationMatching networks for one shot learning 5-way-5-shot support images (batchsize, 5 way * 5 shot, 1, 32, 32) + target images(batchsize, 5 shot from different 5 ways) GenerationFIGR: Few-shot Image Generation with Reptile FIGR-8: The training classes where all 18, 409 classes minus 50 ran- domly sampled classes for the test set; (batchsize=4,1,32,32) from same class. Omniglot: The training classes where all 1623 characters in the dataset minus 20 randomly sampled character classes for the test set; (batchsize=4,1,32,32) from same class. FAST ADAPTATION IN GENERATIVE MODELS WITH GENERATIVE MATCHING NETWORKS Omniglot: (batchsize, 20 samples from up to 2 classes, 28, 28)]]></content>
      <categories>
        <category>Datasets</category>
      </categories>
      <tags>
        <tag>few-shot learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VAE-GAN]]></title>
    <url>%2F2019%2F04%2F24%2FVAE-GAN%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Comparion between VAE and GAN]]></title>
    <url>%2F2019%2F04%2F23%2FComparion-between-VAE-and-GAN%2F</url>
    <content type="text"><![CDATA[Similarity Both aim at constructing a model that can map latent variabels $z$ to object data $p_{data}$. specifically, the trained model $\mathbf{X} = g(\mathbf{Z})$ where $Z$ is general distribution such as normal distribution and gaussian distribution, $\mathbf{X}$ represents the probability distribution of training data. So they both aim at distribution transformation. Both have generative problem that it’s difficult to obtain distribution expression of generated distribution and true distribution, only samples from two distribution are available. $KL$ divergence only be applied to calculte distribution differene on the condition that complete distribution expressions are provided, so it is unapplicable in this scenario. Difference Measurement method: manmade measurement rule for VAE while this measurement rule of GAN is trained by neural network. GAN: proposed to leverage deep neural networks to measure distribution difference because there isn’t suitable measurement method. VAE: adopt a roundabout skill to leverage $KL$ divergence. VAEImportant points The detail is here VAE’s blog Notes: each sample has a gaussian distribution constructed by multivariate Gaussion and then obtain $z$ by sampling it from this distribution:\begin{equation}\log q_{\phi}\left(\mathbf{z} | \mathbf{x}^{(i)}\right)=\log \mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)} \mathbf{I}\right)\end{equation} Notes: $log \boldsymbol{\mu_k} = f_1(x_k), \boldsymbol{\sigma}^{2(k)} = f_2(x_k)$ which are both fitted by neural network. Noise from constructed $z$ can be calculated from $\sigma$ which can be controlled to zero, so noise takes no effect. Generative ability is based on the condition that all $p(Z\X)$ close to gaussion distribution. \begin{equation}p(Z)=\sum_{X} p(Z | X) p(X)=\sum_{X} \mathcal{N}(0, I) p(X)=\mathcal{N}(0, I) \sum_{X} p(X)=\mathcal{N}(0, I)\end{equation} so $p(Z)$ subjects to normal distribution, which satisfy the prior. how: intruducing reconstruction loss:direct method:\begin{equation}\mathcal{L}_{\mu}=\left|f_{1}\left(X_{k}\right)\right|^{2}\end{equation} \begin{equation}\mathcal{L}_{\sigma^{2}}=\left|f_{2}\left(X_{k}\right)\right|^{2}\end{equation} it’s difficult to measure those loss, so it’s reasonable to introduce $KL$ divegence between standard gaussion distribution and independent gaussion distribution $K L\left(N\left(\mu, \sigma^{\wedge}\right) | N(0, l)\right)$:\begin{equation}\mathcal{L}_{\mu, \sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\mu_{(i)}^{2}+\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)\end{equation}where $d$ is the dimension of $z$ Essence of VAE Two encoder: $f_1$ for $\mu$ while $f_2$ for $\sigma$ Reconstruction process: the loss of decoder assume that there is no noise; Sample $z$ process: the loss of encoder assume that there is gaussion noise. GANImportant points GAN is used to map normal distribution $p(z)$ into specfic distribution $p(x)$.]]></content>
      <categories>
        <category>Generative Model</category>
      </categories>
      <tags>
        <tag>comparison</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Matching Networks]]></title>
    <url>%2F2019%2F04%2F22%2FGenerative-Matching-Networks%2F</url>
    <content type="text"><![CDATA[OverviewRelated links This paper in here FAST ADAPTATION IN GENERATIVE MODELS WITH GENERATIVE MATCHING NETWORKS Another version is here Few-shot Generative Modelling with Generative Matching Networks. Both of them inspired from Matching networks for one shot learning Remaining bottlenecks in deep generative models extensive training leading to slow training (currently experience in DCGAN, FIGR). difficulities with generalization from small number of training examples. Solution to those bottlenecks conditional generative models: adapting generative distribution to additional inputs. the restriction of previous research: the input data to be a single object or multiple objects represents the same concepts. Novelty of generative matching networks VAE + matching network without explicit limitations on the numbles of additional inputs objects or the number of concepts.(experiment not demonstrate this, conditional GAN may achieve it.) learn new concepts. ImplementingSpecific problem VAE has been transfered intractable optimization into tractable optmization problem, refer to Overview of Generative Models. However, update of parameters computed from a small portion of training data have a immediate effect for the whole dataset. cann’t quickly adaption because of not incremental learning. original optimization objective\begin{aligned}\log p(\mathbf{x} | \boldsymbol{\theta}) \geq \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi})=\mathbb{E}_{q}[\log p(\mathbf{x}, \mathbf{z} | \boldsymbol{\theta})-\log q(\mathbf{z} | \mathbf{x}, \boldsymbol{\phi})]=\log p(\mathbf{x} | \boldsymbol{\theta})-\operatorname{KL}(q | p(\cdot | \mathbf{x}, \boldsymbol{\theta}))\end{aligned} Generative matching networks different VAE which is x -&gt; z -&gt; x, the framework of this model is from z -&gt;x -&gt; z. incorporating knowledge from newly available data is conditioning, so design model as $p(\mathbf{x} | \mathbf{X}, \boldsymbol{\theta})$ conditioning on additional input data $\mathbf{X}$.\begin{aligned}p(\mathbf{x}, \mathbf{z} | \mathbf{X}, \boldsymbol{\theta})=p(\mathbf{z} | \mathbf{X}, \boldsymbol{\theta}) p(\mathbf{x} | \mathbf{z}, \mathbf{X}, \boldsymbol{\theta})\end{aligned} followed VAE, maintaining a recognition model approximating the posterior over latent variable $z$ to achieve lower bound optimization objective, so,\begin{equation}q(\mathbf{z} | \mathbf{x}, \mathbf{X}, \boldsymbol{\phi}) \approx p(\mathbf{z} | \mathbf{x}, \mathbf{X}, \boldsymbol{\theta})\end{equation} Basic modelFlow chat of generative matching networks the objective is:\begin{equation}p(\mathbf{x}, \mathbf{z} | \mathbf{X}, \boldsymbol{\theta})=p(\mathbf{z} | \mathbf{X}, \boldsymbol{\theta}) p(\mathbf{x} | \mathbf{z}, \mathbf{X}, \boldsymbol{\theta})\end{equation} In conditional likelihood (decoder) $p(\mathbf{x} | \mathbf{z}, \mathbf{X}, \boldsymbol{\theta})$: $\mathbf{x}$ is additional input data which is conditional objects. $\mathbf{z}$ is the prior. $f_L$ and $g_L$ are used to map latent variabels and conditioning objects into feature space $\phi$. $f_L$ is simple affine transformation while $g_L$ is a convolutional network. $\phi_L$ is feature extractor different from $g_L$ but sharing some parameters between them. mapping $r_L$ to $x$ by a deconvulutional network. In recognition model (encoder) $q(\mathbf{z} | \mathbf{X}, \mathbf{x})$: $g_R = f_R = g_L$ $\phi_L$ = $\phi_R$ $r_R$ is used to compute parameters of approximate posterior which is a normal distribution. Core for conditional generative matching network Conditioning on additional inputs $\mathbf{X}$ by matching networks, attention kernel can be calculated as, \begin{equation}a\left(\mathbf{z}, \mathbf{x}_{t}\right)=\frac{\exp \left(\operatorname{sim}\left(f_{L}(\mathbf{z}), g_{L}\left(\mathbf{x}_{t}\right)\right)\right)}{\sum_{t^{\prime}=1}^{T} \exp \left(\operatorname{sim}\left(f_{L}(\mathbf{z}), g_{L}\left(\mathbf{x}_{t^{\prime}}\right)\right)\right)}\end{equation} Aggregating additional inputs for test input,\begin{equation}\mathbf{r}=\sum_{t=1}^{T} a\left(\mathbf{z}, \mathbf{x}_{t}\right) \psi_{L}\left(\mathbf{x}_{t}\right)\end{equation}where $\mathbf{r}$ is input of decoder which is denconvolutional network. FCEObtaining a joint embedding of conditional input dataset with LSTM.\begin{equation}a\left(\mathbf{z}, \mathbf{x}_{t}\right)=\frac{\exp \left(\operatorname{sim}\left(f\left(\mathbf{z}, \mathbf{h}_{k}\right), g\left(\mathbf{x}_{t}, \mathbf{h}_{k}\right)\right)\right)}{\sum_{t^{\prime}=1}^{T} \exp \left(\operatorname{sim}\left(f\left(\mathbf{z}, \mathbf{h}_{k}\right), g\left(\mathbf{x}_{t^{\prime}}, \mathbf{h}_{k}\right)\right)\right)}\end{equation} \begin{equation}\mathbf{r}_{k}=\sum_{t=1}^{T} a\left(\mathbf{z}, \mathbf{x}_{t}\right) \psi\left(\mathbf{x}_{t}\right)\end{equation} \begin{equation}\mathbf{h}_{k+1}=R\left(\mathbf{h}_{k}, \mathbf{r}_{k}\right)\end{equation} Training detailThe final objective with lower bound can be respresented as, \begin{equation}\mathcal{L}(\mathbf{X}, \boldsymbol{\theta}, \boldsymbol{\phi})=\sum_{t=1}^{T} \mathbb{E}_{q\left(\mathbf{z}_{t} | \mathbf{x}_{t}, \mathbf{X}_{&lt; t}, \boldsymbol{\phi}\right)}\left[\log p\left(\mathbf{x}_{t}, \mathbf{z}_{t} | \mathbf{X}_{&lt; t}, \boldsymbol{\theta}\right)- log q\left(\mathbf{z}_{t} | \mathbf{x}_{t}, \mathbf{X}_{&lt; t}, \boldsymbol{\phi}\right)\right]\end{equation} the first item is used to for reconstruction loss.t}, \boldsymbol{\theta}\right)$ the second item is used to calculate KL divergence which can measure the distribution difference between latent variables and normal distribution. Evaluation visualization of generated images. conditional negative log-likelihoods for the test. Possible ideas VAE generated images is vague, may it can be combined with GAN Autoencoding beyond pixels using a learned similarity metric My high-level understanding of GMN VAE: reconstruction (likelihood) + KL divergence(the distribution of latent varables[z_posterior] and normal distribution[z_prior]) GMN: reconstruction (likelihood) + KL divergence(the distribution of latent varables[z_posterior] and modified distribution with mean, variance values from observations[z_prior]) GMN: flowchart]]></content>
      <categories>
        <category>Generative Model</category>
      </categories>
      <tags>
        <tag>few-shot generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Meta Learning]]></title>
    <url>%2F2019%2F04%2F18%2Fmeta-learning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Overview of Generative Models]]></title>
    <url>%2F2019%2F04%2F09%2FOverview-of-GAN%2F</url>
    <content type="text"><![CDATA[The detail of generative model can refer to generative modelsDefinition of generative modelsgiven training data, generate new samples from same distribution\begin{aligned}\mathrm{p}_{model}(\mathrm{x}) { similar to } \mathrm{p}_(\mathrm{x})\end{aligned} Taxonomy of Generative models PixelsRNN and PixelsCNN this method belongs to explicit density model. Using chain rule to decompose likelihood of an image x into product of 1-d distributions: \begin{aligned} p_{\theta}(x)=\prod_{i=1}^{n} p_{\theta}\left(x_{i} | x_{1}, \ldots, x_{i-1}\right)\end{aligned} where $x_i$ represents pixel, $p(x)$ denoets the likelihood of image $x$ it’s important to define ordering of those pixels complex distribution over pixels can be sloved by neural networks pros: explicitly compute likelihood $p(x)$. cons: sequential generation is slow. PixelsRNN generating image pixels from corner, and then using RNN(LSTM) to model dependency on previous pixels. drawback is that the process of sequential generation if slow. PixelsCNN generating image pixels from corner, and then using CNN to cover context region. generation proceed sequentially is still slow. Variational Autoenconders general explanation is here VAE this method defines intractable density function with latent $z$: \begin{aligned}p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x | z) d z\end{aligned} Autoencoders mapping image $x$ to features $z$ with deep neural networks, $z$ is regarded to capture meaningful factors of variation in data. to learn feature $z$ by reconstruction error: $x$ -&gt; $z$ -&gt; $\hat_{x}$ with encoder and decoder. Variational Autoenconders for generation problem Assuming training data $x(i)$ representation $z$ is generated from latent representation $z$. Specifically, first sampling $z$ from true prior $p_{\theta}(z)$, then sampling $x$ from true conditional $p_{\theta}(x|z^(i))$ the aim is to estimate the true parameter $\theta$ by maximuming likelihood of training data : \begin{equation}p_{\theta}(x)=\int p_{\theta}(z) p_{\theta}(x | z) d z\end{equation} where $p_{\theta}(z)$ is gaussion prior, and $p_{\theta}(x | z) $ is decoder. intractable optimization problem, because it’s impossible to compute $p(x | z)$ for every $z$ which means the integral operation is fail in this condition. posterior density $p_{\theta}(z | x)=p_{\theta}(x | z) p_{\theta}(z) / p_{\theta}(x)$ is also intractable, since $p_{\theta}(x)$ is intractable data likelihood. lower bound of VAE why: intractable, transfer optimization how: In addition to decoder network modeling $p_\theta(x|z)$, define additional encoder network $q_{\phi}(z|x)$ that approximates $p_\theta(z|x)$ final optimization objective:\begin{aligned}\log p_{\theta}\left(x^{(i)}\right)\\&amp;=\mathbf{E}_{z \sim q_{\phi}\left(z | x^{(i)}\right)}\left[\log p_{\theta}\left(x^{(i)}\right)\right]\\&amp;=\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} | z\right) p_{\theta}(z)}{p_{\theta}\left(z | x^{(i)}\right)}\right]\\&amp;=\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} | z\right) p_{\theta}(z)}{p_{\theta}\left(z | x^{(i)}\right)} \frac{q_{\phi}\left(z | x^{(i)}\right)}{q_{\phi}\left(z | x^{(i)}\right)}\right]\\&amp;=\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} | z\right)\right]-\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z | x^{(i)}\right)}{p_{\theta}(z)}\right]+\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z | x^{(i)}\right)}{p_{\theta}\left(z | x^{(i)}\right)}\right]\\&amp;=\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} | z\right)\right]-D_{K L}\left(q_{\phi}\left(z | x^{(i)}\right) | p_{\theta}(z)\right)+D_{K L}\left(q_{\phi}\left(z | x^{(i)}\right) | p_{\theta}\left(z | x^{(i)}\right)\right)\end{aligned}where $\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} | z\right)\right]$ is used to reconstruct the input data, $q_{\phi}\left(z | x^{(i)}\right) | p_{\theta}(z)$ is used to make approximate posterior distribution close to prior. lower bound: transfering intractable problem into tractale problem with lower bounds, introducing encoder $_{\phi}$ to transfer two KL divergence, in which one is tractable while another one is untractabel. Thus, lower bound aims at optimizing the tractable KL divergence. the detail of optimization procedure as follows: GAN\begin{equation}{\min {G} \max {D} V(D, G)=}{\mathbb{E}{\boldsymbol{x} \sim p{\text {data}}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}{\boldsymbol{z} \sim p{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z}))]}\end{equation} where $D(x)$ represents the probability of $x$ sourced from real data.]]></content>
      <categories>
        <category>Generative Model</category>
      </categories>
      <tags>
        <tag>overview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Few-shot Image generation]]></title>
    <url>%2F2019%2F04%2F04%2FFew-shot-Image-generation%2F</url>
    <content type="text"><![CDATA[Meta learningDefinition “learn to learn”, intends to design models that can learn new skills or adapt to new environment rapidly with a few traing samples, like human learning way. The detail can be posted in Meta-Learning: Learning to Learn Fast Optimization aims:\begin{aligned}\theta^* = \arg\min_\theta \mathbb{E}_{\mathcal{D}\sim p(\mathcal{D})} [\mathcal{L}_\theta(\mathcal{D})]\end{aligned} where $\mathcal{D}=\langle S, B\rangle$, Support set and Batch set. Training steps sample a subset of labels $L\subset\mathcal{L}$. samples a support set $S^L \subset \mathcal{D}$ and a training batch $B^L \subset \mathcal{D}$. Both of them belong to the sampled label set $L$, $y \in L, \forall (x, y) \in S^L, B^L$. support set is the input of model. the update of model parameters is based on the loss in backpropagation calculated from the mini-batch $B^L$. each pair of sampled dataset $(S^L, B^L)$ is regarded as one data point, such that trained models can generalize to other datasets. Symbols in red are added for meta-learning in addition to the general supervised learning objective.\begin{aligned}\theta = \arg\max_\theta \color{red}{E_{L\subset\mathcal{L}}[} E_{\color{red}{S^L \subset\mathcal{D}, }B^L \subset\mathcal{D}} [\sum_{(x, y)\in B^L} P_\theta(x, y\color{red}{, S^L})] \color{red}{]}\end{aligned} Traning stages meta-learner: a optimizer $g_\phi$ learns how to update the learner model’s parameters via the support set $S$, $\theta’ = g_\phi(\theta, S)$ learner: A classifier $f_\theta$ is the “learner” model, trained for operating a given task. final learning objective is:\begin{aligned}\mathbb{E}_{L\subset\mathcal{L}}[ \mathbb{E}_{S^L \subset\mathcal{D}, B^L \subset\mathcal{D}} [\sum_{(\mathbf{x}, y)\in B^L} P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})]]\end{aligned} Common methodsmodel-based: $f_\theta(\mathbf{x}, S)$ use recurrent network with internal (or external) memory. rapid parameter update achieved by meta-learner model or internal architecture. memory-augmented neural network for meta-learning using external memory storage to facilate learning process without forgetting new information in future. encoding new information quicly, so adapt to new tasks after only a few samples. memory-augmented neural networkc: how to assign weights to attention vector. memory serves as knowledge repository, the controller learns to read and write memory rows. ttention weights generation by addressing mechanism: control-based + location-based. MANN for meta-learning: the update of memory for efficient information retrievel and storage, how to read from memory and how to write into memory. meta network architecture loss gradients are used as meta information to populate models to learn fast weights. metric-based: $\sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_i$ learn efficient distance metric. similar to nearnest neighbors algorithm (KNN,k-means) and kernel density estimation. the predicted possibility of labeled samples is from is a weighted sum of support set samples, and the weight is generated by a kernel function $k\theta$, which can measure the similarity of twp data samples:\begin{aligned}P\\theta(y \vert \mathbf{x}, S) = \sum_{(\mathbf{x}_i, y_i) \in S} k_\theta(\mathbf{x}, \mathbf{x}_i)y_{i}\end{aligned} crucial points: good kernel solution: learning embedding vector of input data explicitly and use them to design proper kernel functions. Siamese networks: assumption: the learned embedding can be generalized to be useful for measuring the distance between images of unknown categories. verification, images pairs. final prediction is the class of the support image with highest probability. Matching networks: $g_{\theta}$ with $k$ classifiers for k classes, while $f_{\theta}$ for testing images. attention kernel depends on two embedding functions $g$ and $f$, in simple version where $f=g$. in complex version where integrating full contextual embedding (FCE), it achieves improvement on hard tasks, not for simple tasks. Relation networks image pairs, feature concatenation. relation modeled: mse loss function. Prototypical networks images of each class are embedded into $M$ dimensional feature vector, each class has prototype feature vector from the mean vector of the embedded support data samples in each class. squared euclidean distance loss. optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$ optimize the model parameter explicitly for fast learning. modeling optimization algorithm exploitly. LSTM meta-learner MAML reptile crucial keys: good kernel solution: learning embedding vector of input data explicitly and use them to design proper kernel functions. Siamese networks: assumption: the learned embedding can be generalized to be useful for measuring the distance between images of unknown categories. verification, images pairs. final prediction is the class of the support image with highest probability. Matching networks: $g_{\theta}$ with $k$ classifiers for k classes, while $f_{\theta}$ for testing images. attention kernel depends on two embedding functions $g$ and $f$, in simple version where $f=g$. in complex version where integrating full contextual embedding (FCE), it achieves improvement on hard tasks, not for simple tasks. Relation network image pairs, feature concatenation. relation modeled: mse loss function. Prototypical images of each class are embedded into $M$ dimensional feature vector, each class has prototype feature vector from the mean vector of the embedded support data samples in each class. squared euclidean distance. optimization-based: $P_{g_\phi(\theta, S^L)}(y \vert \mathbf{x})$ optimize the model parameter explicitly for fast learning]]></content>
      <categories>
        <category>Meta-learning</category>
      </categories>
      <tags>
        <tag>few-shot learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Manifold Learning]]></title>
    <url>%2F2019%2F03%2F18%2FManifold-Learning%2F</url>
    <content type="text"><![CDATA[Manifold Leanring for Semi-supervised Learning the model trained with labeled samples and unlabeled samples introducing manifold learning to learn the geometry of marginal distribution &gt;reference first paperManifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples explanation in Chinese(https://zhuanlan.zhihu.com/p/33006509) aim: data-depend regularization to exploit geometry of the marginal distribution Manifold learning non-linear dimensionality reduction method to obtain intrinsic feature &gt;reference PAMI Semi-supervised Learning with Manifold Learning reason: leveraging a few of training samples to train model always fail to reflect the dataset distribution, which means that the trained model only adapt to the supervised label without learning the intrinsic feature. Introducing manifold learning to combine labeled samples and unlabeled samples can exploit the geometr of marginal distribution. Semi-supervised Multi-task Learning with Manifold Learning reference [Semisupervised feature analysis by mining correlations among multiple tasks]{https://arxiv.org/pdf/1411.6232.pdf} aim: feature selection in semi-supervised MTL method: sparce coefficients learnt\begin{aligned}p(x) &amp; = \min_{wt} \sum\(l=1}^t (loss(w_l) + \alpha ||w_l||_{1,2} + \gamma ||w||_{*})\end{aligned} including $l_{1,2}$ norm and Laplacian norm: \begin{aligned}\gamma ||w|| = \min_{w,b}\sum_{l=1}{t}Tr(w^Tx_lL_lx_l^Tw)\end{aligned} reference [Semi-supervised multitask learning]{http://papers.nips.cc/paper/3198-semi-supervised-multitask-learning.pdf} reference [Semi-supervised multi-task learning with task regularizations]{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.194.1854&amp;rep=rep1&amp;type=pdf} reference [Semi-supervised multitask learning for scene recognition]{https://www.researchgate.net/profile/Lichao_Mou/publication/268880603_Semi-Supervised_Multitask_Learning_for_Scene_Recognition/links/567a67f608ae7fea2e9a08f1.pdf}]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>feature regularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semi-MTL Related]]></title>
    <url>%2F2019%2F03%2F09%2FSemi-MTL-Related%2F</url>
    <content type="text"><![CDATA[MTLLearning Methods Supervised Semi-supervised Scenarios Homogeneous Heterogeneous Heterogeneous Tasks (Document &amp; Images &amp; Audio) Hterogeneous Datsets (Each dataset with a set of labels) Improvement Ideas Network Structure Feature Selection Application CV NLP ALL MTL is the combination of above mentioned.Our method is semi-supervised MTL with heterogeneous datasets, comparison including supervised or semi-supervised MTL with heterogeneous dataset. reference Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks semi-supervised, heterogeneou datasets, feature selection. scenario: CV, dataset A for tasks A[labeled and unlabeled], dataset B for tasks B[labeled and unlabeled] method: manifold learning, mining feature correlation by sparse coefficients. No reference Deep Cross Residual Learning for Multitask Visual Recognition supervised, heterogeneou datasets, network structure. scenario: CV, dataset A for tasks A[labeled], dataset B for tasks B[labeled] method: enables intuitive learning across multiple related tasks using cross-connections called cross-residuals Yes? Netowrk realization? reference A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data semi-supervised, heterogeneou datasets, network structure. scenario: method: No reference Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces supervised, heterogeneou datasets, network structure. scenario: text classification, dataset A for tasks A[labeled], dataset B for tasks B[labeled] method: combining multi-task learning and semi- supervised learning by inducing a joint embed- ding space between disparate label spaces and learning transfer functions between label embeddings. LTN can be used to label unlabelled and auxiliary task data by utilising the ‘distilling knowledge’ contained in auxiliary model predictions. not only model their relationship, but also to directly estimate the cor- responding label of the target task based on auxil- iary predictions Yes: embedding, prediction, output, temperature=1, embedding can be realized with cross-stich network reference Neural Network for Heterogeneous Annotations supervised, heterogeneou datasets, multi-view &amp; stacking setting scenario: NLP, dataset A for tasks A[labeled], dataset B for tasks B[labeled] method: multi-view, stacking, neurual network Yes: not understand]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semi-supervised Heteroneous Multitask Learning Baselines]]></title>
    <url>%2F2019%2F02%2F28%2FSemi-Heteroneous-Multitask-Baseline%2F</url>
    <content type="text"><![CDATA[high-related reference Learning without Forgetting distilled knowledge reference An All-In-One Convolutional Neural Network for Face Analysis naive joint reference From Facial Expression Recognition to Interpersonal Relation PredictionZhanpeng pseudo label reference NLP Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces‘we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions.’ probelm: jointly using unlabelled data and auxiliary, annotated datasets; domain gap; missing labels method: training the Label Transfer Network, minimise the squared error between the model predictions and the pseudo label baseline: yes (how to calculate loss funtion [squared error] based on pseudo label) reference Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels Ishan confidence score same apply scene refer to paper writting reference non-DL Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks problem: ‘Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence.’ ‘These previous works, however, independently select features for each task, which fails to consider correlations among multiple related tasks.’ ‘Despite of their good performances, these classical algorithms are all implemented only with labeled training data.’ method: ‘ignoring the correlations among different features -&gt;apply the sparse coefficients to the feature vectors -&gt; proposing multiple feature selection’ baseline: undetermined (hard to complete) Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing—and Back related work conclude papers of joint training of models for multiple tasks. ‘how learned structure is shared across tasks’: supervise different tasks at different depths of the shared structure duplicate the shared structure into columns and define mechanisms for sharing information across columns Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition , using intermediate representations as auxiliary supervision for low-level task recognition to improve final task performance. baseline: no( pose-emotion both are high-level task, not similar to keypoint detection) pseudo method related reference CVPR 2018Pseudo Mask Augmented Object Detection ‘proposing an effective learning approach that progressively improves the quality pseudo from a coarse initialization,the detection network parameters Θ and pseudo masks Mpseudo are alternatively optimized following a EM-like way’ probelm: object detection without mask annotation method: pseudo mask (alternatively,initialization,pseudo mask with refinement algorithm[graphical model, because the pixel charaterization]) baseline: yes(uing graphical model with learned information as input to refine pseudo label) reference Learning from Noisy Labels with Distillation interpolation between noisy label and distilled knowledge baseline: yes reference Clustered Multi-Task Learning Via Alternating Structure Optimization ‘ASO which aims to identify a shared low-dimensional predictive structure for all tasks’,’based on the standard assump- tion that each task can learn equally well from any other task’,’the clustering view of ASO has not been explored before’ hard to understand, pure theories.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Critical Points in Multitask Learning]]></title>
    <url>%2F2019%2F02%2F28%2FCritical-Points-in-Multitask-0%2F</url>
    <content type="text"><![CDATA[Several tips for heterogeneous multitask learning core problem - heterogeneity structrues in tasks[age and gender], source[documents and images], feature[generated from model for classification or regression tasks], samples[one labeled with age while another annotated with gender] network with suitabel initialization and learning rate construct feature relationship in network, offering trainable parameters, refer to [cross-stitch network] feature selection, sparsity and factorization, which means that the specific branch layers to ontain task-specific fearures in a common feature space. Research directions feature relationship from the level of network. feature processing from the level fo feature space, such as selection, sparsity and facorization.]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parameters Selection for Pseudo]]></title>
    <url>%2F2019%2F02%2F27%2FParameters-Selection-for-Pseudo%2F</url>
    <content type="text"><![CDATA[temperature in distilled knowledge reference-1 Distilling the Knowledge in a Neural Network aim: distilling the knowledge in an ensemble of models into a single modeltemperature: the higher the temperature T, the softer the probability distribution over classes. ‘For the distillation we tried temperatures of [1, 2, 5, 10] and used a relative weight of 0.5 on the cross-entropy for the hard targets, where bold font indicates the best value’ reference-2 from reference-1 Learning without Forgetting aim: useing only new task data to train the network while preserving the original capabilitiestemperature: $T=2$, grid search method density threshold in local feature densitydistribution distance is transfered into weights]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross Stitch Network]]></title>
    <url>%2F2019%2F02%2F27%2FCross-stitch-network%2F</url>
    <content type="text"><![CDATA[cross-stitch-network reference CVPR 2016Cross-stitch Networks for Multi-task Learning tensorflow-code problem: existing multi-task approaches rely on enumerating multiple net- work architectures specific to the tasks at hand, that do not generalize. method: proposing a new sharing unit: “cross-stitch” unit. These units combine the activations from multiple networks. self-defined cross-stitch layer in keras with tensorflow as backbone12345678910111213141516171819202122232425262728class Cross_stitch(Layer): # basic parameter setting def __init__(self,input_shape_1,input_shape_2, **kwargs): super(Cross_stitch, self).__init__(**kwargs) self.input_shape_1 = input_shape_1 self.input_shape_2 = input_shape_2 # apply trainable parameters in network, similar to convolutional layer # shape is important, you must to calculate specific size based on the shape of input and output # in cross-stitch network: [xa,xb]*[papameter]=[xa',xb'], the detail refer to the paper def build(self, input_shape): shape = self.input_shape_1 + self.input_shape_2 self.cross_stitch = self.add_weight( shape=(shape,shape), initializer=tf.initializers.identity(), name='cross_stitch') self.built = True # conduct implement of the detailed algorithm calculation # inputs represent the output of upper layer, such as x=Dense(parameter)(inputs) def call(self,inputs): inputss = tf.concat((inputs[0], inputs[1]), axis=1) output = tf.matmul(inputss, self.cross_stitch) output1 = tf.reshape(output[:,:self.input_shape_1],shape=[-1,self.input_shape_1]) output2 = tf.reshape(output[:,self.input_shape_2:],shape=[-1,self.input_shape_2]) return [output1, output2] def get_config(self): config = &#123;'input_shape_1': self.input_shape_1,'input_shape_2': self.input_shape_2&#125; base_config = super(Cross_stitch, self).get_config() return dict(list(base_config.items()) + list(config.items()))]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Heterogeneous Multitask]]></title>
    <url>%2F2019%2F02%2F26%2FHeterogeneous-Multitask%2F</url>
    <content type="text"><![CDATA[Large-scale Heterogeneous Learning in Big Data Analytics reference slide Large-scale Heterogeneous Learning in Big Data Analytics multi-task learning‘Multitask learning is an inductive transfer mechanism for improving generalization performance [Caruana, Machine Learning’97]’ multi-label learning: multi-view learning: Heterogeneous outputs related to each other with the same set of inputs reference-1 Heterogeneous Multitask Learning with Joint Sparsity Constraints probelm: dealing with homogeneous tasks, such as purely regression (continue) or classification (discrete) task, from a common set of high-dimensional feature space. method: modeling the joint sparsity as L1/L∞ or L1/L2 norm of the model parameters, achieving joint sparse feature selection. datasets: discovering genetic markers that influence multiple correlated traits jointly, datasets with different clinical traits including continuous and discrete labels. Heterogeneous inputs from multiple domains reference-2 Heterogeneous Multitask Metric LearningAcross Multiple Domains Exploiting High-Order Information in Heterogeneous Multi-Task Feature Learning ‘Multitask metric learning (MTML), which can be regarded as a special case of transfer metric learning (TML) by performing transfer across all related domains.’ ‘Heterogeneous transfer learning approaches can be adopted to remedy this drawback by deriving a metric from the learned transformation across different domains’ probelm: inputs from heterogeneous domain can be solved by deriving a metric from the learned transformation from two domains, but pratical aims is to deal with multiple domain by learning the metric from all domains. method: metrics -&gt; transformation -&gt; subspace -&gt; maximize high-order ovariance among the predictive structures of these domains, because high-order statistics (correlation information), which can only be exploited by simultaneously examining all domains, thus obtaining more reliable feature transformations and metrics. datasets: Document Categorization, Scene Classification and Image Annotation Heterogeneous features for different task reference-3 Semantic Feature Learning for HeterogeneousMultitask Classification via Non-NegativeMatrix Factorization problem: different tasks have heterogenous feature in real world. method: leveraging a non-negative matrix factorization-based multitask method (MTNMF) to learn a common semantic feature space underlying different heterogeneous feature spaces of each task. it is similar to reference-1, feature selection vs feature factorization. datasets: 20Newsgroups&amp;ImageNet (image and document), Email Spam Detection (15 persons), Sentiment Classification (books; dvd; electronics; kitchen)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>multitask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Why and How to Use Pseudo]]></title>
    <url>%2F2019%2F02%2F22%2FWhy-How-Pseudo%2F</url>
    <content type="text"><![CDATA[Pseudo labels in multi-task learning Pseudo data selection with density and distribution distance Pseudo labelsThe reason why need to apply pseudo data into model training reference CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images, demonstrating that training a CNN from scratch using both clean and noisy data is better than just using the clean one, on the condition that the amount of pseudo data is limited. reference From Facial Expression Recognition to Interpersonal Relation Prediction Zhanpeng, stating explaination that pseudo data can bridge the gap between heterogeneous data. reference Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels, ‘what’s the all attributs of images’ versus ‘what’s the labeled attributes’, labeling missing attributes. The novalty may be regarded as a baseline (waiting to understand the detail). data imbalance in joint training strategy, such as 20k~ pose dataset AFLW and 90k~ emotion dataset ExpW. Pseudo data selectionBecause much noisy data can effect the model performance and even disturb the model training process, leading to poor model performance, pseudo data selection can control the ratio of pseudo data in the whole datasets. Density reason: because clusters are easily detected by the local density of data points, in the pseudo data belong to same categoty have similar feature. Appling a density based clustering algorithm that measures the complexity of psuedo data using data distribution density in. each category. implementation detail: measuring the purity of data with pseudo label based on its distribution density in a feature space, and rank the purity to generate pseudo weights, pseudo data with higher density is assigned larger weights, while smaller weights are assigned to low-density pseudo data. Distribution distance reason: ‘transfer learning is one important method in machine learning, it can relax the condition of the independent identical distribution in train dataset and test dataset, so that knowledge can be transfered from source domain to target domain’,’its main application includes domain adaptation and multi-domain tranferation’. instance-based domain adaption: calculating the distance between source domain data and target domain data, then adjusting the weights of target domain instance so that the target data can be matched with source domain data. In detail, the smaller distance, the higher similarity. implementation detail: GMM reason: implementation: generating Gaussian Mixture models (GMM) based on A1 (data with pseuodo label) and B (data with ground truth) in each categoty. Assuming GMM has K Gaussian models, we obtain G(A1_1), …, G(A1_K), G(B1_1), …, G(B1_K); EMD reason: &gt;reference Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning, which takes domain scale into account by adding a scale factor. In this paper, transfer learning can be viewed as moving a set of images from the source domain S to the target domain T. The work needed to be done by moving an image to another can be defined as their feature Euaullean distance, so the distance between two domains can be defined as the least amount of total work needed. This definition of domain similarity can be calculated by the Earth Mover’s Distance (EMD). original EMD equation \begin{aligned}\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}\end{aligned} \begin{aligned}EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}\end{aligned} \begin{aligned}P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}\end{aligned} \begin{aligned}Q={(q_{1},w_{q1}),(q_{2},w_{q2}),…,(q_{n},w_{qn})}\end{aligned} reference paper EMD applicationg(s{i}) from the mean value of image features in category i from source domaing(t{i}) from the mean value of image features in category i from target domain\begin{aligned}\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}\end{aligned} \begin{aligned}EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}\end{aligned} \begin{aligned}S={(s_{1},w_{s1}),(s_{2},w_{s2}),…,(s_{m},w_{sm})}\end{aligned} \begin{aligned}T={(t_{1},w_{t1}),(t_{2},w_{t2}),…,(t_{n},w_{tn})}\end{aligned} \begin{aligned}D=[d_{i,j}] = || g(s_{i}) − g(t_{j})||\end{aligned} \begin{aligned}sim(S, T) = e−γd(S,T )\end{aligned} EMD application for pseudo data selection $g(p)$ represents mean value of image features in specific cluster from the same category in source domain $g(g_{i})$ is mean value of image features in cluster i from the same category in target domain\begin{aligned}\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}=\min {\sum_{i=1}^{m}w_{pi},\quad \sum_{j=1}^{n}w_{qj}}\end{aligned} \begin{aligned}EMD(P,Q)={\frac {\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}d_{i,j}}{\sum_{i=1}^{m}\sum_{j=1}^{n}f_{i,j}}}\end{aligned} \begin{aligned}P={(p_{1},w_{p1}),(p_{2},w_{p2}),…,(p_{m},w_{pm})}\end{aligned} \begin{aligned}G={(g_{1},w_{g1}),(g_{2},w_{g2}),…,(g_{n},w_{gn})}\end{aligned} \begin{aligned}D=[d_{i,j}] = || g(p_{i}) − g(g_{j})||\end{aligned} \begin{aligned}sim(S, T) = e−γd(S,T )\end{aligned} implementation: calculating the distance between $G(A_1^i)$ and the whole cluster in G(B1). As for distance calculation with EMD method, we obtain $P={G(A_1^i)_{mean},G(A_1^i)_{probs}}$ and $Q={[G(B_1^i)_{mean}, …, G(B_1^K)_{mean}],[G(B_1^i)_{probs}, …, G(B_1^K)_{probs}]}$, so we can calculate distance vector $D=(emd_1,emd_k)$, which is used for obtaining distribution weights $weights_g$.]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Density and Distribution]]></title>
    <url>%2F2019%2F02%2F21%2FDensity-Distribution%2F</url>
    <content type="text"><![CDATA[Purity based density &amp; distribution distance based GMMPurity based density reference：ECCV 2018 CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images original reference： Science 2004 Clustering by fast search and find of density peaks Creativity leveraging data distribution density in feature space to evaluate the complexity of the dataours — evaluating the purity of pseudo label data by data density in feature space noisy data can be regared as regularied method to improve the model generalizationours — pesudo labels can improve the model generalization Technical detailDensity based clustering algorithm generating features calculating Euclidean distance D_ij calculating local density of each image calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence Distribution distance based GMMClustring based GMMGMM with EM original definition\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}$\pi_k$ represent the possibility of sample belong to kth category new definition for coding\begin{aligned}\sum_{k} z_k = 1\end{aligned} Related code sklearn.mixture.GaussianMixture Data distribution distance EMD reference： Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learningoriginal reference： The Earth Mover’s Distance as a Metric for Image Retrieval EDM definition: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second EMD data signature:\begin{aligned}s= (feature,weights)\end{aligned} related code general EMD expaination pyemd-1D data scipy.atats.wasserstein_distance]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pseudo Purity and Dataset Distribution Distance]]></title>
    <url>%2F2019%2F02%2F17%2FPseudo-purity%26distribution-distance%2F</url>
    <content type="text"><![CDATA[Purity and distribution distance based densityData density reference ECCV 2018 CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images creativity leveraging data distribution density in feature space to evaluate the complexity of the data ours — evaluating the purity of pseudo label data by data density in feature space noisy data can be regared as regularied method to improve the model generalization ours — pesudo labels can improve the model generalization technical detail density based clustering algorithm generating features calculating Euclidean distance D_ij calculating local density of each image calculating distance of each image: maximun local distance is regarded as cluster centre, image with smaller distance between its distance and cluster centre represents that its label have high confidence Data distribution distance EMD reference IJCV 2000 The Earth Mover’s Distance as a Metric for Image Retrieval definition EDM: Signature matching can be naturally cast as a transportation problem by de fining one signature as the supplier and the other as the consumer, and by setting the cost for a supplier-consumer pair to equal the ground distance between an element in the fi rst signature and an element in the second signature:\begin{aligned}s= (feature,weights)\end{aligned} Purity and distribution distance based gmmGMM with EM original definition\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned} $\pi_k$ represent the possibility of sample belong to kth category new definition for coding\begin{aligned}\sum_{k} z_k = 1\end{aligned}]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pesudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross Entropy Loss]]></title>
    <url>%2F2019%2F01%2F16%2FCross_entropy_loss%2F</url>
    <content type="text"><![CDATA[Cross entropy loss calculation in different circumstances Cross_entropy_loss in tensorflow 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def softmax_cross_entropy_with_logits( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits_sg", [logits, labels]) as name: labels = array_ops.stop_gradient(labels, name="labels_stop_gradient") return softmax_cross_entropy_with_logits_v2( labels=labels, logits=logits, dim=dim, name=name)def softmax_cross_entropy_with_logits_v2( _sentinel=None, # pylint: disable=invalid-name labels=None, logits=None, dim=-1, name=None): _ensure_xent_args("softmax_cross_entropy_with_logits", _sentinel, labels, logits) with ops.name_scope(name, "softmax_cross_entropy_with_logits", [logits, labels]) as name: logits = ops.convert_to_tensor(logits, name="logits") labels = ops.convert_to_tensor(labels, name="labels") convert_to_float32 = ( logits.dtype == dtypes.float16 or logits.dtype == dtypes.bfloat16) precise_logits = math_ops.cast( logits, dtypes.float32) if convert_to_float32 else logits # labels and logits must be of the same type labels = math_ops.cast(labels, precise_logits.dtype) input_rank = array_ops.rank(precise_logits) shape = logits.get_shape() if dim is not -1: def _move_dim_to_end(tensor, dim_index, rank): return array_ops.transpose( tensor, array_ops.concat([ math_ops.range(dim_index), math_ops.range(dim_index + 1, rank), [dim_index] ], 0)) precise_logits = _move_dim_to_end(precise_logits, dim, input_rank) labels = _move_dim_to_end(labels, dim, input_rank) input_shape = array_ops.shape(precise_logits) precise_logits = _flatten_outer_dims(precise_logits) labels = _flatten_outer_dims(labels) cost, unused_backprop = gen_nn_ops.softmax_cross_entropy_with_logits( precise_logits, labels, name=name) output_shape = array_ops.slice(input_shape, [0], [math_ops.subtract(input_rank, 1)]) cost = array_ops.reshape(cost, output_shape) if not context.executing_eagerly( ) and shape is not None and shape.dims is not None: shape = shape.as_list() del shape[dim] cost.set_shape(shape) if convert_to_float32: return math_ops.cast(cost, logits.dtype) else: return cost Cross_entropy_loss in Keras with backbone of tensorflow 1tf.nn.softmax_cross_entropy_with_logits(labels=target,logits=output) The reason why the value of loss become inf log(x) when x -&gt; 0 learning rate is too high some parameters of Nel appear inf input data appear inf the parameters of model no longer updated Cross_entropy_loss with missing labels leveraging fixed zero array or ones array as ground truth and generating mask in loss function def mask_cross_entropy_loss(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) loss = K.categorical_crossentropy(y_true,y_pred)*mask return (K.sum(loss)/K.sum(mask) corresponding accuracy with mask def mask_cross_entropy_acc(y_true,y_pred): mask=K.all(K.equal(y_true,0),axis=-1) mask=1-K.cast(mask,K.floatx()) acc = K.cast(K.equal(K.argmax(y_true,axis=-1),K.argmax(y_pred,axis=-1)),K.floatx()))*mask return (K.sum(acc)/K.sum(mask)]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MMD and Density]]></title>
    <url>%2F2019%2F01%2F15%2FGMM%2F</url>
    <content type="text"><![CDATA[Relation between MMD and density for pseudo data selectioncombination of MMD and density: my understanding is that we can obtain MMD, density, prior weights from GMM, when GMM can be calculated from ground-trurh label of MMD: first-order information Density: second-order information, the larger variance, the sparser, the small variance, the denser GMM Gaussian Mixed Model: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. Differentce between k-means and GMM: k-means notes that each point is assigned to different clusters, while GMM can calculate the probabiliy of each point belong to each clusters. Import parameters of GMM: K Gaussion models, also K clusters, $\pi$, $\mu$, $\Sigma$\begin{aligned}p(x) &amp; = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)\end{aligned}]]></content>
      <categories>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>pseudo labels</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy Learning]]></title>
    <url>%2F2019%2F01%2F14%2FNumpy%20learning%2F</url>
    <content type="text"><![CDATA[One-hot labels preprocessing Weighting samples with confidence score 123index = np.ragmax(predicted_result,axis=1)arg = np_utils.to_categorical(index,classes)weighted_one_hot=predicted_result*arg Selection &amp;&amp; weighting samples with confidence score 1weighted_selected_one_hot=np.where(predicted_result&gt;k,predicted_result,0) Extending vector into matrix 1xx = [np.full(classes,value) for value in x]]]></content>
      <categories>
        <category>Codes</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
</search>
